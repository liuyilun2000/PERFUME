MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:04<01:29,  4.97s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:09<01:24,  4.97s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:14<01:19,  4.98s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:19<01:14,  4.94s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:24<01:09,  4.96s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:29<01:04,  4.96s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:34<00:59,  4.94s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:39<00:54,  4.94s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:44<00:49,  4.94s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:49<00:44,  4.92s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:54<00:39,  4.92s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:59<00:34,  4.92s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [01:04<00:29,  4.94s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [01:09<00:24,  4.91s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [01:14<00:19,  4.93s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [01:19<00:14,  4.93s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [01:23<00:09,  4.91s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [01:28<00:04,  4.92s/it]Loading checkpoint shards: 100%|██████████| 19/19 [01:32<00:00,  4.69s/it]Loading checkpoint shards: 100%|██████████| 19/19 [01:32<00:00,  4.89s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors #####
  0%|          | 0/103 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/103 | accuracy 21  0.65625:   0%|          | 0/103 [00:04<?, ?it/s]test:1/103 | accuracy 21  0.65625:   1%|          | 1/103 [00:04<06:59,  4.11s/it]test:2/103 | accuracy 44  0.6875:   1%|          | 1/103 [00:06<06:59,  4.11s/it] test:2/103 | accuracy 44  0.6875:   2%|▏         | 2/103 [00:06<05:22,  3.19s/it]test:3/103 | accuracy 66  0.6875:   2%|▏         | 2/103 [00:09<05:22,  3.19s/it]test:3/103 | accuracy 66  0.6875:   3%|▎         | 3/103 [00:09<04:50,  2.90s/it]test:4/103 | accuracy 89  0.6953125:   3%|▎         | 3/103 [00:11<04:50,  2.90s/it]test:4/103 | accuracy 89  0.6953125:   4%|▍         | 4/103 [00:11<04:33,  2.76s/it]test:5/103 | accuracy 111  0.69375:   4%|▍         | 4/103 [00:14<04:33,  2.76s/it] test:5/103 | accuracy 111  0.69375:   5%|▍         | 5/103 [00:14<04:21,  2.67s/it]test:6/103 | accuracy 138  0.71875:   5%|▍         | 5/103 [00:16<04:21,  2.67s/it]test:6/103 | accuracy 138  0.71875:   6%|▌         | 6/103 [00:16<04:15,  2.63s/it]test:7/103 | accuracy 160  0.7142857142857143:   6%|▌         | 6/103 [00:19<04:15,  2.63s/it]test:7/103 | accuracy 160  0.7142857142857143:   7%|▋         | 7/103 [00:19<04:14,  2.66s/it]test:8/103 | accuracy 185  0.72265625:   7%|▋         | 7/103 [00:22<04:14,  2.66s/it]        test:8/103 | accuracy 185  0.72265625:   8%|▊         | 8/103 [00:22<04:11,  2.64s/it]test:9/103 | accuracy 209  0.7256944444444444:   8%|▊         | 8/103 [00:24<04:11,  2.64s/it]test:9/103 | accuracy 209  0.7256944444444444:   9%|▊         | 9/103 [00:24<04:05,  2.61s/it]test:10/103 | accuracy 233  0.728125:   9%|▊         | 9/103 [00:27<04:05,  2.61s/it]         test:10/103 | accuracy 233  0.728125:  10%|▉         | 10/103 [00:27<04:00,  2.59s/it]test:11/103 | accuracy 258  0.7329545454545454:  10%|▉         | 10/103 [00:29<04:00,  2.59s/it]test:11/103 | accuracy 258  0.7329545454545454:  11%|█         | 11/103 [00:29<03:58,  2.59s/it]test:12/103 | accuracy 283  0.7369791666666666:  11%|█         | 11/103 [00:32<03:58,  2.59s/it]test:12/103 | accuracy 283  0.7369791666666666:  12%|█▏        | 12/103 [00:32<03:54,  2.58s/it]test:13/103 | accuracy 307  0.7379807692307693:  12%|█▏        | 12/103 [00:34<03:54,  2.58s/it]test:13/103 | accuracy 307  0.7379807692307693:  13%|█▎        | 13/103 [00:34<03:52,  2.58s/it]test:14/103 | accuracy 327  0.7299107142857143:  13%|█▎        | 13/103 [00:37<03:52,  2.58s/it]test:14/103 | accuracy 327  0.7299107142857143:  14%|█▎        | 14/103 [00:37<03:49,  2.58s/it]test:15/103 | accuracy 354  0.7375:  14%|█▎        | 14/103 [00:40<03:49,  2.58s/it]            test:15/103 | accuracy 354  0.7375:  15%|█▍        | 15/103 [00:40<03:45,  2.56s/it]test:16/103 | accuracy 378  0.73828125:  15%|█▍        | 15/103 [00:42<03:45,  2.56s/it]test:16/103 | accuracy 378  0.73828125:  16%|█▌        | 16/103 [00:42<03:41,  2.55s/it]test:17/103 | accuracy 402  0.7389705882352942:  16%|█▌        | 16/103 [00:45<03:41,  2.55s/it]test:17/103 | accuracy 402  0.7389705882352942:  17%|█▋        | 17/103 [00:45<03:40,  2.57s/it]test:18/103 | accuracy 425  0.7378472222222222:  17%|█▋        | 17/103 [00:47<03:40,  2.57s/it]test:18/103 | accuracy 425  0.7378472222222222:  17%|█▋        | 18/103 [00:47<03:37,  2.56s/it]test:19/103 | accuracy 445  0.7319078947368421:  17%|█▋        | 18/103 [00:50<03:37,  2.56s/it]test:19/103 | accuracy 445  0.7319078947368421:  18%|█▊        | 19/103 [00:50<03:35,  2.56s/it]test:20/103 | accuracy 468  0.73125:  18%|█▊        | 19/103 [00:52<03:35,  2.56s/it]           test:20/103 | accuracy 468  0.73125:  19%|█▉        | 20/103 [00:52<03:31,  2.55s/it]test:21/103 | accuracy 497  0.7395833333333334:  19%|█▉        | 20/103 [00:55<03:31,  2.55s/it]test:21/103 | accuracy 497  0.7395833333333334:  20%|██        | 21/103 [00:55<03:31,  2.58s/it]test:22/103 | accuracy 524  0.7443181818181818:  20%|██        | 21/103 [00:58<03:31,  2.58s/it]test:22/103 | accuracy 524  0.7443181818181818:  21%|██▏       | 22/103 [00:58<03:28,  2.58s/it]test:23/103 | accuracy 550  0.7472826086956522:  21%|██▏       | 22/103 [01:00<03:28,  2.58s/it]test:23/103 | accuracy 550  0.7472826086956522:  22%|██▏       | 23/103 [01:00<03:25,  2.56s/it]test:24/103 | accuracy 569  0.7408854166666666:  22%|██▏       | 23/103 [01:03<03:25,  2.56s/it]test:24/103 | accuracy 569  0.7408854166666666:  23%|██▎       | 24/103 [01:03<03:23,  2.57s/it]test:25/103 | accuracy 595  0.74375:  23%|██▎       | 24/103 [01:05<03:23,  2.57s/it]           test:25/103 | accuracy 595  0.74375:  24%|██▍       | 25/103 [01:05<03:21,  2.58s/it]test:26/103 | accuracy 619  0.7439903846153846:  24%|██▍       | 25/103 [01:08<03:21,  2.58s/it]test:26/103 | accuracy 619  0.7439903846153846:  25%|██▌       | 26/103 [01:08<03:20,  2.61s/it]test:27/103 | accuracy 642  0.7430555555555556:  25%|██▌       | 26/103 [01:11<03:20,  2.61s/it]test:27/103 | accuracy 642  0.7430555555555556:  26%|██▌       | 27/103 [01:11<03:18,  2.61s/it]test:28/103 | accuracy 667  0.7444196428571429:  26%|██▌       | 27/103 [01:13<03:18,  2.61s/it]test:28/103 | accuracy 667  0.7444196428571429:  27%|██▋       | 28/103 [01:13<03:13,  2.59s/it]test:29/103 | accuracy 689  0.7424568965517241:  27%|██▋       | 28/103 [01:16<03:13,  2.59s/it]test:29/103 | accuracy 689  0.7424568965517241:  28%|██▊       | 29/103 [01:16<03:12,  2.60s/it]test:30/103 | accuracy 714  0.74375:  28%|██▊       | 29/103 [01:18<03:12,  2.60s/it]           test:30/103 | accuracy 714  0.74375:  29%|██▉       | 30/103 [01:18<03:08,  2.59s/it]test:31/103 | accuracy 737  0.7429435483870968:  29%|██▉       | 30/103 [01:21<03:08,  2.59s/it]test:31/103 | accuracy 737  0.7429435483870968:  30%|███       | 31/103 [01:21<03:05,  2.58s/it]test:32/103 | accuracy 760  0.7421875:  30%|███       | 31/103 [01:23<03:05,  2.58s/it]         test:32/103 | accuracy 760  0.7421875:  31%|███       | 32/103 [01:23<03:02,  2.57s/it]test:33/103 | accuracy 783  0.7414772727272727:  31%|███       | 32/103 [01:26<03:02,  2.57s/it]test:33/103 | accuracy 783  0.7414772727272727:  32%|███▏      | 33/103 [01:26<02:59,  2.57s/it]test:34/103 | accuracy 811  0.7454044117647058:  32%|███▏      | 33/103 [01:29<02:59,  2.57s/it]test:34/103 | accuracy 811  0.7454044117647058:  33%|███▎      | 34/103 [01:29<02:57,  2.58s/it]test:35/103 | accuracy 835  0.7455357142857143:  33%|███▎      | 34/103 [01:31<02:57,  2.58s/it]test:35/103 | accuracy 835  0.7455357142857143:  34%|███▍      | 35/103 [01:31<02:55,  2.59s/it]test:36/103 | accuracy 859  0.7456597222222222:  34%|███▍      | 35/103 [01:34<02:55,  2.59s/it]test:36/103 | accuracy 859  0.7456597222222222:  35%|███▍      | 36/103 [01:34<02:52,  2.58s/it]test:37/103 | accuracy 887  0.7491554054054054:  35%|███▍      | 36/103 [01:36<02:52,  2.58s/it]test:37/103 | accuracy 887  0.7491554054054054:  36%|███▌      | 37/103 [01:36<02:49,  2.57s/it]test:38/103 | accuracy 912  0.75:  36%|███▌      | 37/103 [01:39<02:49,  2.57s/it]              test:38/103 | accuracy 912  0.75:  37%|███▋      | 38/103 [01:39<02:47,  2.58s/it]test:39/103 | accuracy 935  0.749198717948718:  37%|███▋      | 38/103 [01:41<02:47,  2.58s/it]test:39/103 | accuracy 935  0.749198717948718:  38%|███▊      | 39/103 [01:41<02:44,  2.57s/it]test:40/103 | accuracy 957  0.74765625:  38%|███▊      | 39/103 [01:44<02:44,  2.57s/it]       test:40/103 | accuracy 957  0.74765625:  39%|███▉      | 40/103 [01:44<02:42,  2.57s/it]test:41/103 | accuracy 982  0.7484756097560976:  39%|███▉      | 40/103 [01:47<02:42,  2.57s/it]test:41/103 | accuracy 982  0.7484756097560976:  40%|███▉      | 41/103 [01:47<02:39,  2.57s/it]test:42/103 | accuracy 1006  0.7485119047619048:  40%|███▉      | 41/103 [01:49<02:39,  2.57s/it]test:42/103 | accuracy 1006  0.7485119047619048:  41%|████      | 42/103 [01:49<02:36,  2.57s/it]test:43/103 | accuracy 1030  0.748546511627907:  41%|████      | 42/103 [01:52<02:36,  2.57s/it] test:43/103 | accuracy 1030  0.748546511627907:  42%|████▏     | 43/103 [01:52<02:34,  2.58s/it]test:44/103 | accuracy 1056  0.75:  42%|████▏     | 43/103 [01:54<02:34,  2.58s/it]             test:44/103 | accuracy 1056  0.75:  43%|████▎     | 44/103 [01:54<02:31,  2.57s/it]test:45/103 | accuracy 1084  0.7527777777777778:  43%|████▎     | 44/103 [01:57<02:31,  2.57s/it]test:45/103 | accuracy 1084  0.7527777777777778:  44%|████▎     | 45/103 [01:57<02:28,  2.57s/it]test:46/103 | accuracy 1110  0.7540760869565217:  44%|████▎     | 45/103 [01:59<02:28,  2.57s/it]test:46/103 | accuracy 1110  0.7540760869565217:  45%|████▍     | 46/103 [01:59<02:27,  2.59s/it]test:47/103 | accuracy 1136  0.7553191489361702:  45%|████▍     | 46/103 [02:02<02:27,  2.59s/it]test:47/103 | accuracy 1136  0.7553191489361702:  46%|████▌     | 47/103 [02:02<02:27,  2.63s/it]test:48/103 | accuracy 1158  0.75390625:  46%|████▌     | 47/103 [02:05<02:27,  2.63s/it]        test:48/103 | accuracy 1158  0.75390625:  47%|████▋     | 48/103 [02:05<02:22,  2.60s/it]test:49/103 | accuracy 1179  0.7519132653061225:  47%|████▋     | 48/103 [02:08<02:22,  2.60s/it]test:49/103 | accuracy 1179  0.7519132653061225:  48%|████▊     | 49/103 [02:08<02:30,  2.79s/it]test:50/103 | accuracy 1200  0.75:  48%|████▊     | 49/103 [02:11<02:30,  2.79s/it]              test:50/103 | accuracy 1200  0.75:  49%|████▊     | 50/103 [02:11<02:24,  2.73s/it]test:51/103 | accuracy 1225  0.7506127450980392:  49%|████▊     | 50/103 [02:13<02:24,  2.73s/it]test:51/103 | accuracy 1225  0.7506127450980392:  50%|████▉     | 51/103 [02:13<02:17,  2.65s/it]test:52/103 | accuracy 1251  0.7518028846153846:  50%|████▉     | 51/103 [02:16<02:17,  2.65s/it]test:52/103 | accuracy 1251  0.7518028846153846:  50%|█████     | 52/103 [02:16<02:14,  2.63s/it]test:53/103 | accuracy 1274  0.7511792452830188:  50%|█████     | 52/103 [02:18<02:14,  2.63s/it]test:53/103 | accuracy 1274  0.7511792452830188:  51%|█████▏    | 53/103 [02:18<02:12,  2.66s/it]test:54/103 | accuracy 1294  0.7488425925925926:  51%|█████▏    | 53/103 [02:21<02:12,  2.66s/it]test:54/103 | accuracy 1294  0.7488425925925926:  52%|█████▏    | 54/103 [02:21<02:09,  2.64s/it]test:55/103 | accuracy 1321  0.7505681818181819:  52%|█████▏    | 54/103 [02:24<02:09,  2.64s/it]test:55/103 | accuracy 1321  0.7505681818181819:  53%|█████▎    | 55/103 [02:24<02:15,  2.83s/it]test:56/103 | accuracy 1347  0.7516741071428571:  53%|█████▎    | 55/103 [02:27<02:15,  2.83s/it]test:56/103 | accuracy 1347  0.7516741071428571:  54%|█████▍    | 56/103 [02:27<02:09,  2.75s/it]test:57/103 | accuracy 1377  0.7549342105263158:  54%|█████▍    | 56/103 [02:29<02:09,  2.75s/it]test:57/103 | accuracy 1377  0.7549342105263158:  55%|█████▌    | 57/103 [02:29<02:04,  2.71s/it]test:58/103 | accuracy 1400  0.7543103448275862:  55%|█████▌    | 57/103 [02:32<02:04,  2.71s/it]test:58/103 | accuracy 1400  0.7543103448275862:  56%|█████▋    | 58/103 [02:32<02:01,  2.70s/it]test:59/103 | accuracy 1426  0.7552966101694916:  56%|█████▋    | 58/103 [02:35<02:01,  2.70s/it]test:59/103 | accuracy 1426  0.7552966101694916:  57%|█████▋    | 59/103 [02:35<01:57,  2.66s/it]test:60/103 | accuracy 1450  0.7552083333333334:  57%|█████▋    | 59/103 [02:37<01:57,  2.66s/it]test:60/103 | accuracy 1450  0.7552083333333334:  58%|█████▊    | 60/103 [02:37<01:52,  2.61s/it]test:61/103 | accuracy 1475  0.7556352459016393:  58%|█████▊    | 60/103 [02:40<01:52,  2.61s/it]test:61/103 | accuracy 1475  0.7556352459016393:  59%|█████▉    | 61/103 [02:40<01:49,  2.60s/it]test:62/103 | accuracy 1500  0.7560483870967742:  59%|█████▉    | 61/103 [02:42<01:49,  2.60s/it]test:62/103 | accuracy 1500  0.7560483870967742:  60%|██████    | 62/103 [02:42<01:45,  2.57s/it]test:63/103 | accuracy 1524  0.7559523809523809:  60%|██████    | 62/103 [02:45<01:45,  2.57s/it]test:63/103 | accuracy 1524  0.7559523809523809:  61%|██████    | 63/103 [02:45<01:42,  2.57s/it]test:64/103 | accuracy 1552  0.7578125:  61%|██████    | 63/103 [02:47<01:42,  2.57s/it]         test:64/103 | accuracy 1552  0.7578125:  62%|██████▏   | 64/103 [02:47<01:40,  2.58s/it]test:65/103 | accuracy 1575  0.7572115384615384:  62%|██████▏   | 64/103 [02:50<01:40,  2.58s/it]test:65/103 | accuracy 1575  0.7572115384615384:  63%|██████▎   | 65/103 [02:50<01:37,  2.57s/it]test:66/103 | accuracy 1595  0.7552083333333334:  63%|██████▎   | 65/103 [02:53<01:37,  2.57s/it]test:66/103 | accuracy 1595  0.7552083333333334:  64%|██████▍   | 66/103 [02:53<01:36,  2.61s/it]test:67/103 | accuracy 1615  0.7532649253731343:  64%|██████▍   | 66/103 [02:55<01:36,  2.61s/it]test:67/103 | accuracy 1615  0.7532649253731343:  65%|██████▌   | 67/103 [02:55<01:33,  2.59s/it]test:68/103 | accuracy 1636  0.7518382352941176:  65%|██████▌   | 67/103 [02:58<01:33,  2.59s/it]test:68/103 | accuracy 1636  0.7518382352941176:  66%|██████▌   | 68/103 [02:58<01:30,  2.58s/it]test:69/103 | accuracy 1664  0.7536231884057971:  66%|██████▌   | 68/103 [03:00<01:30,  2.58s/it]test:69/103 | accuracy 1664  0.7536231884057971:  67%|██████▋   | 69/103 [03:00<01:28,  2.60s/it]test:70/103 | accuracy 1687  0.753125:  67%|██████▋   | 69/103 [03:03<01:28,  2.60s/it]          test:70/103 | accuracy 1687  0.753125:  68%|██████▊   | 70/103 [03:03<01:25,  2.60s/it]test:71/103 | accuracy 1717  0.7557218309859155:  68%|██████▊   | 70/103 [03:06<01:25,  2.60s/it]test:71/103 | accuracy 1717  0.7557218309859155:  69%|██████▉   | 71/103 [03:06<01:23,  2.60s/it]test:72/103 | accuracy 1738  0.7543402777777778:  69%|██████▉   | 71/103 [03:08<01:23,  2.60s/it]test:72/103 | accuracy 1738  0.7543402777777778:  70%|██████▉   | 72/103 [03:08<01:20,  2.60s/it]test:73/103 | accuracy 1759  0.7529965753424658:  70%|██████▉   | 72/103 [03:11<01:20,  2.60s/it]test:73/103 | accuracy 1759  0.7529965753424658:  71%|███████   | 73/103 [03:11<01:17,  2.59s/it]test:74/103 | accuracy 1782  0.7525337837837838:  71%|███████   | 73/103 [03:13<01:17,  2.59s/it]test:74/103 | accuracy 1782  0.7525337837837838:  72%|███████▏  | 74/103 [03:13<01:15,  2.59s/it]test:75/103 | accuracy 1806  0.7525:  72%|███████▏  | 74/103 [03:16<01:15,  2.59s/it]            test:75/103 | accuracy 1806  0.7525:  73%|███████▎  | 75/103 [03:16<01:12,  2.58s/it]test:76/103 | accuracy 1828  0.7516447368421053:  73%|███████▎  | 75/103 [03:18<01:12,  2.58s/it]test:76/103 | accuracy 1828  0.7516447368421053:  74%|███████▍  | 76/103 [03:19<01:10,  2.60s/it]test:77/103 | accuracy 1848  0.75:  74%|███████▍  | 76/103 [03:21<01:10,  2.60s/it]              test:77/103 | accuracy 1848  0.75:  75%|███████▍  | 77/103 [03:21<01:07,  2.61s/it]test:78/103 | accuracy 1870  0.749198717948718:  75%|███████▍  | 77/103 [03:24<01:07,  2.61s/it]test:78/103 | accuracy 1870  0.749198717948718:  76%|███████▌  | 78/103 [03:24<01:06,  2.66s/it]test:79/103 | accuracy 1894  0.7492088607594937:  76%|███████▌  | 78/103 [03:26<01:06,  2.66s/it]test:79/103 | accuracy 1894  0.7492088607594937:  77%|███████▋  | 79/103 [03:27<01:03,  2.64s/it]test:80/103 | accuracy 1915  0.748046875:  77%|███████▋  | 79/103 [03:29<01:03,  2.64s/it]       test:80/103 | accuracy 1915  0.748046875:  78%|███████▊  | 80/103 [03:29<01:00,  2.61s/it]test:81/103 | accuracy 1939  0.748070987654321:  78%|███████▊  | 80/103 [03:32<01:00,  2.61s/it]test:81/103 | accuracy 1939  0.748070987654321:  79%|███████▊  | 81/103 [03:32<00:57,  2.61s/it]test:82/103 | accuracy 1965  0.7488567073170732:  79%|███████▊  | 81/103 [03:34<00:57,  2.61s/it]test:82/103 | accuracy 1965  0.7488567073170732:  80%|███████▉  | 82/103 [03:34<00:54,  2.61s/it]test:83/103 | accuracy 1991  0.7496234939759037:  80%|███████▉  | 82/103 [03:37<00:54,  2.61s/it]test:83/103 | accuracy 1991  0.7496234939759037:  81%|████████  | 83/103 [03:37<00:52,  2.61s/it]test:84/103 | accuracy 2018  0.7507440476190477:  81%|████████  | 83/103 [03:40<00:52,  2.61s/it]test:84/103 | accuracy 2018  0.7507440476190477:  82%|████████▏ | 84/103 [03:40<00:51,  2.70s/it]test:85/103 | accuracy 2042  0.7507352941176471:  82%|████████▏ | 84/103 [03:42<00:51,  2.70s/it]test:85/103 | accuracy 2042  0.7507352941176471:  83%|████████▎ | 85/103 [03:42<00:48,  2.67s/it]test:86/103 | accuracy 2068  0.751453488372093:  83%|████████▎ | 85/103 [03:45<00:48,  2.67s/it] test:86/103 | accuracy 2068  0.751453488372093:  83%|████████▎ | 86/103 [03:45<00:44,  2.64s/it]test:87/103 | accuracy 2091  0.7510775862068966:  83%|████████▎ | 86/103 [03:47<00:44,  2.64s/it]test:87/103 | accuracy 2091  0.7510775862068966:  84%|████████▍ | 87/103 [03:48<00:41,  2.62s/it]test:88/103 | accuracy 2115  0.7510653409090909:  84%|████████▍ | 87/103 [03:50<00:41,  2.62s/it]test:88/103 | accuracy 2115  0.7510653409090909:  85%|████████▌ | 88/103 [03:50<00:39,  2.62s/it]test:89/103 | accuracy 2139  0.7510533707865169:  85%|████████▌ | 88/103 [03:53<00:39,  2.62s/it]test:89/103 | accuracy 2139  0.7510533707865169:  86%|████████▋ | 89/103 [03:53<00:36,  2.61s/it]test:90/103 | accuracy 2159  0.7496527777777777:  86%|████████▋ | 89/103 [03:55<00:36,  2.61s/it]test:90/103 | accuracy 2159  0.7496527777777777:  87%|████████▋ | 90/103 [03:55<00:34,  2.62s/it]test:91/103 | accuracy 2178  0.7479395604395604:  87%|████████▋ | 90/103 [03:58<00:34,  2.62s/it]test:91/103 | accuracy 2178  0.7479395604395604:  88%|████████▊ | 91/103 [03:58<00:31,  2.61s/it]test:92/103 | accuracy 2202  0.7479619565217391:  88%|████████▊ | 91/103 [04:01<00:31,  2.61s/it]test:92/103 | accuracy 2202  0.7479619565217391:  89%|████████▉ | 92/103 [04:01<00:28,  2.63s/it]test:93/103 | accuracy 2226  0.7479838709677419:  89%|████████▉ | 92/103 [04:03<00:28,  2.63s/it]test:93/103 | accuracy 2226  0.7479838709677419:  90%|█████████ | 93/103 [04:03<00:26,  2.63s/it]test:94/103 | accuracy 2252  0.7486702127659575:  90%|█████████ | 93/103 [04:06<00:26,  2.63s/it]test:94/103 | accuracy 2252  0.7486702127659575:  91%|█████████▏| 94/103 [04:06<00:23,  2.61s/it]test:95/103 | accuracy 2276  0.7486842105263158:  91%|█████████▏| 94/103 [04:08<00:23,  2.61s/it]test:95/103 | accuracy 2276  0.7486842105263158:  92%|█████████▏| 95/103 [04:08<00:20,  2.61s/it]test:96/103 | accuracy 2304  0.75:  92%|█████████▏| 95/103 [04:11<00:20,  2.61s/it]              test:96/103 | accuracy 2304  0.75:  93%|█████████▎| 96/103 [04:11<00:18,  2.61s/it]test:97/103 | accuracy 2332  0.7512886597938144:  93%|█████████▎| 96/103 [04:14<00:18,  2.61s/it]test:97/103 | accuracy 2332  0.7512886597938144:  94%|█████████▍| 97/103 [04:14<00:15,  2.60s/it]test:98/103 | accuracy 2359  0.7522321428571429:  94%|█████████▍| 97/103 [04:16<00:15,  2.60s/it]test:98/103 | accuracy 2359  0.7522321428571429:  95%|█████████▌| 98/103 [04:16<00:12,  2.59s/it]test:99/103 | accuracy 2386  0.7531565656565656:  95%|█████████▌| 98/103 [04:19<00:12,  2.59s/it]test:99/103 | accuracy 2386  0.7531565656565656:  96%|█████████▌| 99/103 [04:19<00:10,  2.61s/it]test:100/103 | accuracy 2407  0.7521875:  96%|█████████▌| 99/103 [04:21<00:10,  2.61s/it]        test:100/103 | accuracy 2407  0.7521875:  97%|█████████▋| 100/103 [04:21<00:07,  2.59s/it]test:101/103 | accuracy 2429  0.7515470297029703:  97%|█████████▋| 100/103 [04:24<00:07,  2.59s/it]test:101/103 | accuracy 2429  0.7515470297029703:  98%|█████████▊| 101/103 [04:24<00:05,  2.62s/it]test:102/103 | accuracy 2451  0.7509191176470589:  98%|█████████▊| 101/103 [04:27<00:05,  2.62s/it]test:102/103 | accuracy 2451  0.7509191176470589:  99%|█████████▉| 102/103 [04:27<00:02,  2.60s/it]test:103/103 | accuracy 2456  0.7510703363914373:  99%|█████████▉| 102/103 [04:28<00:02,  2.60s/it]test:103/103 | accuracy 2456  0.7510703363914373: 100%|██████████| 103/103 [04:28<00:00,  2.13s/it]test:103/103 | accuracy 2456  0.7510703363914373: 100%|██████████| 103/103 [04:28<00:00,  2.60s/it]


test finished
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.91s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.92s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.92s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.90s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.91s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.91s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.90s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.91s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.92s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.91s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.90s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.90s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.91s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.90s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.95s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.94s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.92s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors #####
  0%|          | 0/58 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/58 | accuracy 28  0.875:   0%|          | 0/58 [00:09<?, ?it/s]test:1/58 | accuracy 28  0.875:   2%|▏         | 1/58 [00:09<09:02,  9.51s/it]test:2/58 | accuracy 57  0.890625:   2%|▏         | 1/58 [00:15<09:02,  9.51s/it]test:2/58 | accuracy 57  0.890625:   3%|▎         | 2/58 [00:15<06:52,  7.36s/it]test:3/58 | accuracy 83  0.8645833333333334:   3%|▎         | 2/58 [00:21<06:52,  7.36s/it]test:3/58 | accuracy 83  0.8645833333333334:   5%|▌         | 3/58 [00:21<06:13,  6.80s/it]test:4/58 | accuracy 113  0.8828125:   5%|▌         | 3/58 [00:27<06:13,  6.80s/it]        test:4/58 | accuracy 113  0.8828125:   7%|▋         | 4/58 [00:27<05:47,  6.43s/it]test:5/58 | accuracy 137  0.85625:   7%|▋         | 4/58 [00:37<05:47,  6.43s/it]  test:5/58 | accuracy 137  0.85625:   9%|▊         | 5/58 [00:37<06:51,  7.76s/it]test:6/58 | accuracy 164  0.8541666666666666:   9%|▊         | 5/58 [00:44<06:51,  7.76s/it]test:6/58 | accuracy 164  0.8541666666666666:  10%|█         | 6/58 [00:44<06:28,  7.47s/it]test:7/58 | accuracy 195  0.8705357142857143:  10%|█         | 6/58 [00:51<06:28,  7.47s/it]test:7/58 | accuracy 195  0.8705357142857143:  12%|█▏        | 7/58 [00:51<06:11,  7.29s/it]test:8/58 | accuracy 225  0.87890625:  12%|█▏        | 7/58 [00:59<06:11,  7.29s/it]        test:8/58 | accuracy 225  0.87890625:  14%|█▍        | 8/58 [00:59<06:12,  7.44s/it]test:9/58 | accuracy 254  0.8819444444444444:  14%|█▍        | 8/58 [01:07<06:12,  7.44s/it]test:9/58 | accuracy 254  0.8819444444444444:  16%|█▌        | 9/58 [01:07<06:13,  7.63s/it]Traceback (most recent call last):
  File "commonsense_evaluate.py", line 346, in <module>
    main()
  File "commonsense_evaluate.py", line 124, in main
    prompts,outputs = evaluate(instructions)
  File "commonsense_evaluate.py", line 94, in evaluate
    generation_output = model.generate(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py", line 2063, in generate
    result = self._beam_search(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py", line 3238, in _beam_search
    outputs = self(**model_inputs, return_dict=True)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 171, in new_forward
    return module._hf_hook.post_forward(module, output)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 388, in post_forward
    output = send_to_device(output, self.input_device, skip_keys=self.skip_keys)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/utils/operations.py", line 184, in send_to_device
    {
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/utils/operations.py", line 185, in <dictcomp>
    k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/utils/operations.py", line 156, in send_to_device
    return tensor.to(device, non_blocking=non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.06 GiB. GPU 0 has a total capacity of 39.38 GiB of which 8.61 GiB is free. Including non-PyTorch memory, this process has 30.73 GiB memory in use. Of the allocated memory 24.70 GiB is allocated by PyTorch, and 5.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
test:9/58 | accuracy 254  0.8819444444444444:  16%|█▌        | 9/58 [01:20<07:15,  8.90s/it]MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.91s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.92s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.92s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.90s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.91s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.91s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.90s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:21,  1.91s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.96s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.94s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:21<00:15,  1.92s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:23<00:13,  1.92s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.92s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.91s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.91s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.91s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.90s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.90s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors #####
  0%|          | 0/62 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/62 | accuracy 23  0.71875:   0%|          | 0/62 [00:04<?, ?it/s]test:1/62 | accuracy 23  0.71875:   2%|▏         | 1/62 [00:04<04:54,  4.83s/it]test:2/62 | accuracy 50  0.78125:   2%|▏         | 1/62 [00:08<04:54,  4.83s/it]test:2/62 | accuracy 50  0.78125:   3%|▎         | 2/62 [00:08<04:06,  4.12s/it]test:3/62 | accuracy 77  0.8020833333333334:   3%|▎         | 2/62 [00:12<04:06,  4.12s/it]test:3/62 | accuracy 77  0.8020833333333334:   5%|▍         | 3/62 [00:12<03:49,  3.90s/it]test:4/62 | accuracy 101  0.7890625:   5%|▍         | 3/62 [00:15<03:49,  3.90s/it]        test:4/62 | accuracy 101  0.7890625:   6%|▋         | 4/62 [00:15<03:40,  3.80s/it]test:5/62 | accuracy 131  0.81875:   6%|▋         | 4/62 [00:19<03:40,  3.80s/it]  test:5/62 | accuracy 131  0.81875:   8%|▊         | 5/62 [00:19<03:33,  3.75s/it]test:6/62 | accuracy 160  0.8333333333333334:   8%|▊         | 5/62 [00:22<03:33,  3.75s/it]test:6/62 | accuracy 160  0.8333333333333334:  10%|▉         | 6/62 [00:22<03:26,  3.69s/it]test:7/62 | accuracy 189  0.84375:  10%|▉         | 6/62 [00:26<03:26,  3.69s/it]           test:7/62 | accuracy 189  0.84375:  11%|█▏        | 7/62 [00:26<03:28,  3.79s/it]test:8/62 | accuracy 213  0.83203125:  11%|█▏        | 7/62 [00:30<03:28,  3.79s/it]test:8/62 | accuracy 213  0.83203125:  13%|█▎        | 8/62 [00:30<03:23,  3.76s/it]test:9/62 | accuracy 238  0.8263888888888888:  13%|█▎        | 8/62 [00:34<03:23,  3.76s/it]test:9/62 | accuracy 238  0.8263888888888888:  15%|█▍        | 9/62 [00:34<03:24,  3.86s/it]test:10/62 | accuracy 263  0.821875:  15%|█▍        | 9/62 [00:38<03:24,  3.86s/it]         test:10/62 | accuracy 263  0.821875:  16%|█▌        | 10/62 [00:38<03:18,  3.81s/it]test:11/62 | accuracy 291  0.8267045454545454:  16%|█▌        | 10/62 [00:42<03:18,  3.81s/it]test:11/62 | accuracy 291  0.8267045454545454:  18%|█▊        | 11/62 [00:42<03:14,  3.82s/it]test:12/62 | accuracy 319  0.8307291666666666:  18%|█▊        | 11/62 [00:46<03:14,  3.82s/it]test:12/62 | accuracy 319  0.8307291666666666:  19%|█▉        | 12/62 [00:46<03:17,  3.95s/it]test:13/62 | accuracy 348  0.8365384615384616:  19%|█▉        | 12/62 [00:50<03:17,  3.95s/it]test:13/62 | accuracy 348  0.8365384615384616:  21%|██        | 13/62 [00:50<03:17,  4.03s/it]test:14/62 | accuracy 372  0.8303571428571429:  21%|██        | 13/62 [00:54<03:17,  4.03s/it]test:14/62 | accuracy 372  0.8303571428571429:  23%|██▎       | 14/62 [00:54<03:10,  3.97s/it]test:15/62 | accuracy 397  0.8270833333333333:  23%|██▎       | 14/62 [00:58<03:10,  3.97s/it]test:15/62 | accuracy 397  0.8270833333333333:  24%|██▍       | 15/62 [00:58<03:06,  3.96s/it]test:16/62 | accuracy 422  0.82421875:  24%|██▍       | 15/62 [01:02<03:06,  3.96s/it]        test:16/62 | accuracy 422  0.82421875:  26%|██▌       | 16/62 [01:02<03:03,  3.99s/it]test:17/62 | accuracy 449  0.8253676470588235:  26%|██▌       | 16/62 [01:06<03:03,  3.99s/it]test:17/62 | accuracy 449  0.8253676470588235:  27%|██▋       | 17/62 [01:06<02:56,  3.93s/it]test:18/62 | accuracy 474  0.8229166666666666:  27%|██▋       | 17/62 [01:09<02:56,  3.93s/it]test:18/62 | accuracy 474  0.8229166666666666:  29%|██▉       | 18/62 [01:09<02:47,  3.80s/it]test:19/62 | accuracy 497  0.8174342105263158:  29%|██▉       | 18/62 [01:13<02:47,  3.80s/it]test:19/62 | accuracy 497  0.8174342105263158:  31%|███       | 19/62 [01:13<02:40,  3.73s/it]test:20/62 | accuracy 527  0.8234375:  31%|███       | 19/62 [01:17<02:40,  3.73s/it]         test:20/62 | accuracy 527  0.8234375:  32%|███▏      | 20/62 [01:17<02:38,  3.76s/it]test:21/62 | accuracy 555  0.8258928571428571:  32%|███▏      | 20/62 [01:21<02:38,  3.76s/it]test:21/62 | accuracy 555  0.8258928571428571:  34%|███▍      | 21/62 [01:21<02:34,  3.76s/it]test:22/62 | accuracy 584  0.8295454545454546:  34%|███▍      | 21/62 [01:24<02:34,  3.76s/it]test:22/62 | accuracy 584  0.8295454545454546:  35%|███▌      | 22/62 [01:24<02:27,  3.69s/it]test:23/62 | accuracy 612  0.8315217391304348:  35%|███▌      | 22/62 [01:28<02:27,  3.69s/it]test:23/62 | accuracy 612  0.8315217391304348:  37%|███▋      | 23/62 [01:28<02:28,  3.81s/it]test:24/62 | accuracy 640  0.8333333333333334:  37%|███▋      | 23/62 [01:32<02:28,  3.81s/it]test:24/62 | accuracy 640  0.8333333333333334:  39%|███▊      | 24/62 [01:32<02:30,  3.96s/it]test:25/62 | accuracy 663  0.82875:  39%|███▊      | 24/62 [01:36<02:30,  3.96s/it]           test:25/62 | accuracy 663  0.82875:  40%|████      | 25/62 [01:36<02:23,  3.88s/it]test:26/62 | accuracy 687  0.8257211538461539:  40%|████      | 25/62 [01:40<02:23,  3.88s/it]test:26/62 | accuracy 687  0.8257211538461539:  42%|████▏     | 26/62 [01:40<02:17,  3.82s/it]test:27/62 | accuracy 710  0.8217592592592593:  42%|████▏     | 26/62 [01:44<02:17,  3.82s/it]test:27/62 | accuracy 710  0.8217592592592593:  44%|████▎     | 27/62 [01:44<02:15,  3.86s/it]test:28/62 | accuracy 734  0.8191964285714286:  44%|████▎     | 27/62 [01:47<02:15,  3.86s/it]test:28/62 | accuracy 734  0.8191964285714286:  45%|████▌     | 28/62 [01:47<02:09,  3.81s/it]test:29/62 | accuracy 759  0.8178879310344828:  45%|████▌     | 28/62 [01:51<02:09,  3.81s/it]test:29/62 | accuracy 759  0.8178879310344828:  47%|████▋     | 29/62 [01:51<02:06,  3.85s/it]test:30/62 | accuracy 785  0.8177083333333334:  47%|████▋     | 29/62 [01:55<02:06,  3.85s/it]test:30/62 | accuracy 785  0.8177083333333334:  48%|████▊     | 30/62 [01:55<02:02,  3.82s/it]test:31/62 | accuracy 813  0.8195564516129032:  48%|████▊     | 30/62 [01:59<02:02,  3.82s/it]test:31/62 | accuracy 813  0.8195564516129032:  50%|█████     | 31/62 [01:59<01:55,  3.72s/it]test:32/62 | accuracy 841  0.8212890625:  50%|█████     | 31/62 [02:02<01:55,  3.72s/it]      test:32/62 | accuracy 841  0.8212890625:  52%|█████▏    | 32/62 [02:02<01:51,  3.72s/it]test:33/62 | accuracy 866  0.8200757575757576:  52%|█████▏    | 32/62 [02:06<01:51,  3.72s/it]test:33/62 | accuracy 866  0.8200757575757576:  53%|█████▎    | 33/62 [02:06<01:46,  3.69s/it]test:34/62 | accuracy 892  0.8198529411764706:  53%|█████▎    | 33/62 [02:10<01:46,  3.69s/it]test:34/62 | accuracy 892  0.8198529411764706:  55%|█████▍    | 34/62 [02:10<01:44,  3.74s/it]test:35/62 | accuracy 920  0.8214285714285714:  55%|█████▍    | 34/62 [02:14<01:44,  3.74s/it]test:35/62 | accuracy 920  0.8214285714285714:  56%|█████▋    | 35/62 [02:14<01:44,  3.88s/it]test:36/62 | accuracy 948  0.8229166666666666:  56%|█████▋    | 35/62 [02:18<01:44,  3.88s/it]test:36/62 | accuracy 948  0.8229166666666666:  58%|█████▊    | 36/62 [02:18<01:37,  3.77s/it]test:37/62 | accuracy 975  0.8234797297297297:  58%|█████▊    | 36/62 [02:21<01:37,  3.77s/it]test:37/62 | accuracy 975  0.8234797297297297:  60%|█████▉    | 37/62 [02:21<01:34,  3.78s/it]test:38/62 | accuracy 1005  0.8264802631578947:  60%|█████▉    | 37/62 [02:25<01:34,  3.78s/it]test:38/62 | accuracy 1005  0.8264802631578947:  61%|██████▏   | 38/62 [02:25<01:29,  3.73s/it]test:39/62 | accuracy 1030  0.8253205128205128:  61%|██████▏   | 38/62 [02:29<01:29,  3.73s/it]test:39/62 | accuracy 1030  0.8253205128205128:  63%|██████▎   | 39/62 [02:29<01:25,  3.73s/it]test:40/62 | accuracy 1055  0.82421875:  63%|██████▎   | 39/62 [02:32<01:25,  3.73s/it]        test:40/62 | accuracy 1055  0.82421875:  65%|██████▍   | 40/62 [02:33<01:22,  3.75s/it]test:41/62 | accuracy 1080  0.823170731707317:  65%|██████▍   | 40/62 [02:36<01:22,  3.75s/it]test:41/62 | accuracy 1080  0.823170731707317:  66%|██████▌   | 41/62 [02:36<01:17,  3.71s/it]test:42/62 | accuracy 1104  0.8214285714285714:  66%|██████▌   | 41/62 [02:40<01:17,  3.71s/it]test:42/62 | accuracy 1104  0.8214285714285714:  68%|██████▊   | 42/62 [02:40<01:13,  3.69s/it]test:43/62 | accuracy 1128  0.8197674418604651:  68%|██████▊   | 42/62 [02:43<01:13,  3.69s/it]test:43/62 | accuracy 1128  0.8197674418604651:  69%|██████▉   | 43/62 [02:43<01:10,  3.69s/it]test:44/62 | accuracy 1151  0.8174715909090909:  69%|██████▉   | 43/62 [02:47<01:10,  3.69s/it]test:44/62 | accuracy 1151  0.8174715909090909:  71%|███████   | 44/62 [02:47<01:05,  3.67s/it]test:45/62 | accuracy 1174  0.8152777777777778:  71%|███████   | 44/62 [02:51<01:05,  3.67s/it]test:45/62 | accuracy 1174  0.8152777777777778:  73%|███████▎  | 45/62 [02:51<01:03,  3.72s/it]test:46/62 | accuracy 1199  0.8145380434782609:  73%|███████▎  | 45/62 [02:55<01:03,  3.72s/it]test:46/62 | accuracy 1199  0.8145380434782609:  74%|███████▍  | 46/62 [02:55<01:00,  3.79s/it]test:47/62 | accuracy 1224  0.8138297872340425:  74%|███████▍  | 46/62 [02:59<01:00,  3.79s/it]test:47/62 | accuracy 1224  0.8138297872340425:  76%|███████▌  | 47/62 [02:59<00:57,  3.84s/it]test:48/62 | accuracy 1250  0.8138020833333334:  76%|███████▌  | 47/62 [03:02<00:57,  3.84s/it]test:48/62 | accuracy 1250  0.8138020833333334:  77%|███████▋  | 48/62 [03:02<00:52,  3.78s/it]test:49/62 | accuracy 1276  0.8137755102040817:  77%|███████▋  | 48/62 [03:06<00:52,  3.78s/it]test:49/62 | accuracy 1276  0.8137755102040817:  79%|███████▉  | 49/62 [03:06<00:47,  3.67s/it]test:50/62 | accuracy 1301  0.813125:  79%|███████▉  | 49/62 [03:10<00:47,  3.67s/it]          test:50/62 | accuracy 1301  0.813125:  81%|████████  | 50/62 [03:10<00:44,  3.73s/it]test:51/62 | accuracy 1328  0.8137254901960784:  81%|████████  | 50/62 [03:14<00:44,  3.73s/it]test:51/62 | accuracy 1328  0.8137254901960784:  82%|████████▏ | 51/62 [03:14<00:41,  3.76s/it]test:52/62 | accuracy 1356  0.8149038461538461:  82%|████████▏ | 51/62 [03:17<00:41,  3.76s/it]test:52/62 | accuracy 1356  0.8149038461538461:  84%|████████▍ | 52/62 [03:17<00:37,  3.77s/it]test:53/62 | accuracy 1382  0.8148584905660378:  84%|████████▍ | 52/62 [03:21<00:37,  3.77s/it]test:53/62 | accuracy 1382  0.8148584905660378:  85%|████████▌ | 53/62 [03:21<00:33,  3.77s/it]test:54/62 | accuracy 1406  0.8136574074074074:  85%|████████▌ | 53/62 [03:25<00:33,  3.77s/it]test:54/62 | accuracy 1406  0.8136574074074074:  87%|████████▋ | 54/62 [03:25<00:30,  3.76s/it]test:55/62 | accuracy 1433  0.8142045454545455:  87%|████████▋ | 54/62 [03:29<00:30,  3.76s/it]test:55/62 | accuracy 1433  0.8142045454545455:  89%|████████▊ | 55/62 [03:29<00:26,  3.74s/it]test:56/62 | accuracy 1462  0.8158482142857143:  89%|████████▊ | 55/62 [03:32<00:26,  3.74s/it]test:56/62 | accuracy 1462  0.8158482142857143:  90%|█████████ | 56/62 [03:32<00:22,  3.69s/it]test:57/62 | accuracy 1489  0.8163377192982456:  90%|█████████ | 56/62 [03:36<00:22,  3.69s/it]test:57/62 | accuracy 1489  0.8163377192982456:  92%|█████████▏| 57/62 [03:36<00:18,  3.68s/it]test:58/62 | accuracy 1512  0.8146551724137931:  92%|█████████▏| 57/62 [03:40<00:18,  3.68s/it]test:58/62 | accuracy 1512  0.8146551724137931:  94%|█████████▎| 58/62 [03:40<00:15,  3.77s/it]test:59/62 | accuracy 1539  0.8151483050847458:  94%|█████████▎| 58/62 [03:43<00:15,  3.77s/it]test:59/62 | accuracy 1539  0.8151483050847458:  95%|█████████▌| 59/62 [03:43<00:11,  3.75s/it]test:60/62 | accuracy 1567  0.8161458333333333:  95%|█████████▌| 59/62 [03:47<00:11,  3.75s/it]test:60/62 | accuracy 1567  0.8161458333333333:  97%|█████████▋| 60/62 [03:47<00:07,  3.73s/it]test:61/62 | accuracy 1593  0.8160860655737705:  97%|█████████▋| 60/62 [03:51<00:07,  3.73s/it]test:61/62 | accuracy 1593  0.8160860655737705:  98%|█████████▊| 61/62 [03:51<00:03,  3.72s/it]test:62/62 | accuracy 1595  0.8162743091095189:  98%|█████████▊| 61/62 [03:52<00:03,  3.72s/it]test:62/62 | accuracy 1595  0.8162743091095189: 100%|██████████| 62/62 [03:52<00:00,  2.83s/it]test:62/62 | accuracy 1595  0.8162743091095189: 100%|██████████| 62/62 [03:52<00:00,  3.74s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.94s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:33,  1.97s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:31,  1.95s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.92s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.91s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.91s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.90s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.90s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.90s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.89s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.90s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.91s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.91s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.90s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.90s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.90s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.90s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.96s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors #####
  0%|          | 0/314 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/314 | accuracy 26  0.8125:   0%|          | 0/314 [00:06<?, ?it/s]test:1/314 | accuracy 26  0.8125:   0%|          | 1/314 [00:06<34:22,  6.59s/it]test:2/314 | accuracy 57  0.890625:   0%|          | 1/314 [00:12<34:22,  6.59s/it]test:2/314 | accuracy 57  0.890625:   1%|          | 2/314 [00:12<31:40,  6.09s/it]test:3/314 | accuracy 80  0.8333333333333334:   1%|          | 2/314 [00:18<31:40,  6.09s/it]test:3/314 | accuracy 80  0.8333333333333334:   1%|          | 3/314 [00:18<31:02,  5.99s/it]test:4/314 | accuracy 108  0.84375:   1%|          | 3/314 [00:24<31:02,  5.99s/it]          test:4/314 | accuracy 108  0.84375:   1%|▏         | 4/314 [00:24<31:46,  6.15s/it]test:5/314 | accuracy 136  0.85:   1%|▏         | 4/314 [00:31<31:46,  6.15s/it]   test:5/314 | accuracy 136  0.85:   2%|▏         | 5/314 [00:31<32:42,  6.35s/it]test:6/314 | accuracy 165  0.859375:   2%|▏         | 5/314 [00:36<32:42,  6.35s/it]test:6/314 | accuracy 165  0.859375:   2%|▏         | 6/314 [00:36<31:08,  6.06s/it]test:7/314 | accuracy 196  0.875:   2%|▏         | 6/314 [00:42<31:08,  6.06s/it]   test:7/314 | accuracy 196  0.875:   2%|▏         | 7/314 [00:42<30:51,  6.03s/it]test:8/314 | accuracy 222  0.8671875:   2%|▏         | 7/314 [00:48<30:51,  6.03s/it]test:8/314 | accuracy 222  0.8671875:   3%|▎         | 8/314 [00:48<30:08,  5.91s/it]test:9/314 | accuracy 250  0.8680555555555556:   3%|▎         | 8/314 [00:54<30:08,  5.91s/it]test:9/314 | accuracy 250  0.8680555555555556:   3%|▎         | 9/314 [00:54<29:32,  5.81s/it]test:10/314 | accuracy 281  0.878125:   3%|▎         | 9/314 [01:00<29:32,  5.81s/it]         test:10/314 | accuracy 281  0.878125:   3%|▎         | 10/314 [01:00<30:03,  5.93s/it]test:11/314 | accuracy 310  0.8806818181818182:   3%|▎         | 10/314 [01:06<30:03,  5.93s/it]test:11/314 | accuracy 310  0.8806818181818182:   4%|▎         | 11/314 [01:06<29:58,  5.94s/it]test:12/314 | accuracy 335  0.8723958333333334:   4%|▎         | 11/314 [01:11<29:58,  5.94s/it]test:12/314 | accuracy 335  0.8723958333333334:   4%|▍         | 12/314 [01:11<29:14,  5.81s/it]test:13/314 | accuracy 363  0.8725961538461539:   4%|▍         | 12/314 [01:18<29:14,  5.81s/it]test:13/314 | accuracy 363  0.8725961538461539:   4%|▍         | 13/314 [01:18<29:59,  5.98s/it]test:14/314 | accuracy 390  0.8705357142857143:   4%|▍         | 13/314 [01:24<29:59,  5.98s/it]test:14/314 | accuracy 390  0.8705357142857143:   4%|▍         | 14/314 [01:24<29:52,  5.97s/it]test:15/314 | accuracy 419  0.8729166666666667:   4%|▍         | 14/314 [01:30<29:52,  5.97s/it]test:15/314 | accuracy 419  0.8729166666666667:   5%|▍         | 15/314 [01:30<30:06,  6.04s/it]test:16/314 | accuracy 449  0.876953125:   5%|▍         | 15/314 [01:36<30:06,  6.04s/it]       test:16/314 | accuracy 449  0.876953125:   5%|▌         | 16/314 [01:36<30:12,  6.08s/it]test:17/314 | accuracy 478  0.8786764705882353:   5%|▌         | 16/314 [01:43<30:12,  6.08s/it]test:17/314 | accuracy 478  0.8786764705882353:   5%|▌         | 17/314 [01:43<31:32,  6.37s/it]test:18/314 | accuracy 507  0.8802083333333334:   5%|▌         | 17/314 [01:49<31:32,  6.37s/it]test:18/314 | accuracy 507  0.8802083333333334:   6%|▌         | 18/314 [01:49<30:49,  6.25s/it]test:19/314 | accuracy 535  0.8799342105263158:   6%|▌         | 18/314 [01:54<30:49,  6.25s/it]test:19/314 | accuracy 535  0.8799342105263158:   6%|▌         | 19/314 [01:54<29:20,  5.97s/it]test:20/314 | accuracy 559  0.8734375:   6%|▌         | 19/314 [02:00<29:20,  5.97s/it]         test:20/314 | accuracy 559  0.8734375:   6%|▋         | 20/314 [02:00<28:36,  5.84s/it]test:21/314 | accuracy 591  0.8794642857142857:   6%|▋         | 20/314 [02:06<28:36,  5.84s/it]test:21/314 | accuracy 591  0.8794642857142857:   7%|▋         | 21/314 [02:06<28:49,  5.90s/it]test:22/314 | accuracy 622  0.8835227272727273:   7%|▋         | 21/314 [02:12<28:49,  5.90s/it]test:22/314 | accuracy 622  0.8835227272727273:   7%|▋         | 22/314 [02:12<29:48,  6.13s/it]test:23/314 | accuracy 652  0.8858695652173914:   7%|▋         | 22/314 [02:18<29:48,  6.13s/it]test:23/314 | accuracy 652  0.8858695652173914:   7%|▋         | 23/314 [02:18<28:56,  5.97s/it]test:24/314 | accuracy 682  0.8880208333333334:   7%|▋         | 23/314 [02:23<28:56,  5.97s/it]test:24/314 | accuracy 682  0.8880208333333334:   8%|▊         | 24/314 [02:23<27:44,  5.74s/it]test:25/314 | accuracy 711  0.88875:   8%|▊         | 24/314 [02:30<27:44,  5.74s/it]           test:25/314 | accuracy 711  0.88875:   8%|▊         | 25/314 [02:30<29:38,  6.15s/it]test:26/314 | accuracy 740  0.8894230769230769:   8%|▊         | 25/314 [02:37<29:38,  6.15s/it]test:26/314 | accuracy 740  0.8894230769230769:   8%|▊         | 26/314 [02:37<29:37,  6.17s/it]test:27/314 | accuracy 769  0.8900462962962963:   8%|▊         | 26/314 [02:43<29:37,  6.17s/it]test:27/314 | accuracy 769  0.8900462962962963:   9%|▊         | 27/314 [02:43<29:33,  6.18s/it]test:28/314 | accuracy 798  0.890625:   9%|▊         | 27/314 [02:49<29:33,  6.18s/it]          test:28/314 | accuracy 798  0.890625:   9%|▉         | 28/314 [02:49<30:00,  6.30s/it]test:29/314 | accuracy 829  0.8933189655172413:   9%|▉         | 28/314 [02:55<30:00,  6.30s/it]test:29/314 | accuracy 829  0.8933189655172413:   9%|▉         | 29/314 [02:55<28:53,  6.08s/it]test:30/314 | accuracy 856  0.8916666666666667:   9%|▉         | 29/314 [03:01<28:53,  6.08s/it]test:30/314 | accuracy 856  0.8916666666666667:  10%|▉         | 30/314 [03:01<28:11,  5.96s/it]test:31/314 | accuracy 885  0.8921370967741935:  10%|▉         | 30/314 [03:06<28:11,  5.96s/it]test:31/314 | accuracy 885  0.8921370967741935:  10%|▉         | 31/314 [03:06<27:49,  5.90s/it]test:32/314 | accuracy 911  0.8896484375:  10%|▉         | 31/314 [03:13<27:49,  5.90s/it]      test:32/314 | accuracy 911  0.8896484375:  10%|█         | 32/314 [03:13<28:35,  6.08s/it]test:33/314 | accuracy 941  0.8910984848484849:  10%|█         | 32/314 [03:18<28:35,  6.08s/it]test:33/314 | accuracy 941  0.8910984848484849:  11%|█         | 33/314 [03:18<27:49,  5.94s/it]test:34/314 | accuracy 971  0.8924632352941176:  11%|█         | 33/314 [03:24<27:49,  5.94s/it]test:34/314 | accuracy 971  0.8924632352941176:  11%|█         | 34/314 [03:24<27:08,  5.81s/it]test:35/314 | accuracy 1001  0.89375:  11%|█         | 34/314 [03:29<27:08,  5.81s/it]          test:35/314 | accuracy 1001  0.89375:  11%|█         | 35/314 [03:29<26:15,  5.65s/it]test:36/314 | accuracy 1028  0.8923611111111112:  11%|█         | 35/314 [03:35<26:15,  5.65s/it]test:36/314 | accuracy 1028  0.8923611111111112:  11%|█▏        | 36/314 [03:35<26:10,  5.65s/it]test:37/314 | accuracy 1056  0.8918918918918919:  11%|█▏        | 36/314 [03:42<26:10,  5.65s/it]test:37/314 | accuracy 1056  0.8918918918918919:  12%|█▏        | 37/314 [03:42<28:17,  6.13s/it]test:38/314 | accuracy 1086  0.8930921052631579:  12%|█▏        | 37/314 [03:47<28:17,  6.13s/it]test:38/314 | accuracy 1086  0.8930921052631579:  12%|█▏        | 38/314 [03:47<26:49,  5.83s/it]test:39/314 | accuracy 1114  0.8926282051282052:  12%|█▏        | 38/314 [03:53<26:49,  5.83s/it]test:39/314 | accuracy 1114  0.8926282051282052:  12%|█▏        | 39/314 [03:53<26:24,  5.76s/it]test:40/314 | accuracy 1144  0.89375:  12%|█▏        | 39/314 [03:59<26:24,  5.76s/it]           test:40/314 | accuracy 1144  0.89375:  13%|█▎        | 40/314 [03:59<26:55,  5.90s/it]test:41/314 | accuracy 1170  0.8917682926829268:  13%|█▎        | 40/314 [04:05<26:55,  5.90s/it]test:41/314 | accuracy 1170  0.8917682926829268:  13%|█▎        | 41/314 [04:05<27:02,  5.94s/it]test:42/314 | accuracy 1199  0.8921130952380952:  13%|█▎        | 41/314 [04:11<27:02,  5.94s/it]test:42/314 | accuracy 1199  0.8921130952380952:  13%|█▎        | 42/314 [04:11<26:55,  5.94s/it]test:43/314 | accuracy 1229  0.8931686046511628:  13%|█▎        | 42/314 [04:18<26:55,  5.94s/it]test:43/314 | accuracy 1229  0.8931686046511628:  14%|█▎        | 43/314 [04:18<27:44,  6.14s/it]test:44/314 | accuracy 1255  0.8913352272727273:  14%|█▎        | 43/314 [04:24<27:44,  6.14s/it]test:44/314 | accuracy 1255  0.8913352272727273:  14%|█▍        | 44/314 [04:24<27:45,  6.17s/it]test:45/314 | accuracy 1286  0.8930555555555556:  14%|█▍        | 44/314 [04:30<27:45,  6.17s/it]test:45/314 | accuracy 1286  0.8930555555555556:  14%|█▍        | 45/314 [04:30<27:47,  6.20s/it]test:46/314 | accuracy 1314  0.8926630434782609:  14%|█▍        | 45/314 [04:37<27:47,  6.20s/it]test:46/314 | accuracy 1314  0.8926630434782609:  15%|█▍        | 46/314 [04:37<28:07,  6.30s/it]test:47/314 | accuracy 1342  0.8922872340425532:  15%|█▍        | 46/314 [04:42<28:07,  6.30s/it]test:47/314 | accuracy 1342  0.8922872340425532:  15%|█▍        | 47/314 [04:42<26:28,  5.95s/it]test:48/314 | accuracy 1368  0.890625:  15%|█▍        | 47/314 [04:48<26:28,  5.95s/it]          test:48/314 | accuracy 1368  0.890625:  15%|█▌        | 48/314 [04:48<26:02,  5.87s/it]test:49/314 | accuracy 1395  0.8896683673469388:  15%|█▌        | 48/314 [04:54<26:02,  5.87s/it]test:49/314 | accuracy 1395  0.8896683673469388:  16%|█▌        | 49/314 [04:54<26:46,  6.06s/it]test:50/314 | accuracy 1423  0.889375:  16%|█▌        | 49/314 [05:01<26:46,  6.06s/it]          test:50/314 | accuracy 1423  0.889375:  16%|█▌        | 50/314 [05:01<27:30,  6.25s/it]test:51/314 | accuracy 1451  0.8890931372549019:  16%|█▌        | 50/314 [05:06<27:30,  6.25s/it]test:51/314 | accuracy 1451  0.8890931372549019:  16%|█▌        | 51/314 [05:06<26:23,  6.02s/it]test:52/314 | accuracy 1481  0.8900240384615384:  16%|█▌        | 51/314 [05:12<26:23,  6.02s/it]test:52/314 | accuracy 1481  0.8900240384615384:  17%|█▋        | 52/314 [05:12<25:25,  5.82s/it]test:53/314 | accuracy 1508  0.8891509433962265:  17%|█▋        | 52/314 [05:18<25:25,  5.82s/it]test:53/314 | accuracy 1508  0.8891509433962265:  17%|█▋        | 53/314 [05:18<26:32,  6.10s/it]test:54/314 | accuracy 1539  0.890625:  17%|█▋        | 53/314 [05:25<26:32,  6.10s/it]          test:54/314 | accuracy 1539  0.890625:  17%|█▋        | 54/314 [05:25<27:25,  6.33s/it]test:55/314 | accuracy 1568  0.8909090909090909:  17%|█▋        | 54/314 [05:31<27:25,  6.33s/it]test:55/314 | accuracy 1568  0.8909090909090909:  18%|█▊        | 55/314 [05:31<26:29,  6.14s/it]test:56/314 | accuracy 1598  0.8917410714285714:  18%|█▊        | 55/314 [05:37<26:29,  6.14s/it]test:56/314 | accuracy 1598  0.8917410714285714:  18%|█▊        | 56/314 [05:37<26:05,  6.07s/it]test:57/314 | accuracy 1625  0.8908991228070176:  18%|█▊        | 56/314 [05:42<26:05,  6.07s/it]test:57/314 | accuracy 1625  0.8908991228070176:  18%|█▊        | 57/314 [05:42<25:16,  5.90s/it]test:58/314 | accuracy 1650  0.8890086206896551:  18%|█▊        | 57/314 [05:48<25:16,  5.90s/it]test:58/314 | accuracy 1650  0.8890086206896551:  18%|█▊        | 58/314 [05:48<25:16,  5.92s/it]test:59/314 | accuracy 1679  0.8893008474576272:  18%|█▊        | 58/314 [05:55<25:16,  5.92s/it]test:59/314 | accuracy 1679  0.8893008474576272:  19%|█▉        | 59/314 [05:55<26:39,  6.27s/it]test:60/314 | accuracy 1710  0.890625:  19%|█▉        | 59/314 [06:01<26:39,  6.27s/it]          test:60/314 | accuracy 1710  0.890625:  19%|█▉        | 60/314 [06:01<25:47,  6.09s/it]test:61/314 | accuracy 1741  0.891905737704918:  19%|█▉        | 60/314 [06:07<25:47,  6.09s/it]test:61/314 | accuracy 1741  0.891905737704918:  19%|█▉        | 61/314 [06:07<26:06,  6.19s/it]test:62/314 | accuracy 1770  0.8921370967741935:  19%|█▉        | 61/314 [06:14<26:06,  6.19s/it]test:62/314 | accuracy 1770  0.8921370967741935:  20%|█▉        | 62/314 [06:14<26:53,  6.40s/it]test:63/314 | accuracy 1799  0.8923611111111112:  20%|█▉        | 62/314 [06:20<26:53,  6.40s/it]test:63/314 | accuracy 1799  0.8923611111111112:  20%|██        | 63/314 [06:20<25:28,  6.09s/it]test:64/314 | accuracy 1829  0.89306640625:  20%|██        | 63/314 [06:25<25:28,  6.09s/it]     test:64/314 | accuracy 1829  0.89306640625:  20%|██        | 64/314 [06:25<24:50,  5.96s/it]test:65/314 | accuracy 1858  0.8932692307692308:  20%|██        | 64/314 [06:32<24:50,  5.96s/it]test:65/314 | accuracy 1858  0.8932692307692308:  21%|██        | 65/314 [06:32<25:21,  6.11s/it]test:66/314 | accuracy 1883  0.8915719696969697:  21%|██        | 65/314 [06:37<25:21,  6.11s/it]test:66/314 | accuracy 1883  0.8915719696969697:  21%|██        | 66/314 [06:37<24:21,  5.89s/it]test:67/314 | accuracy 1914  0.8927238805970149:  21%|██        | 66/314 [06:43<24:21,  5.89s/it]test:67/314 | accuracy 1914  0.8927238805970149:  21%|██▏       | 67/314 [06:43<23:54,  5.81s/it]test:68/314 | accuracy 1946  0.8943014705882353:  21%|██▏       | 67/314 [06:50<23:54,  5.81s/it]test:68/314 | accuracy 1946  0.8943014705882353:  22%|██▏       | 68/314 [06:50<25:49,  6.30s/it]test:69/314 | accuracy 1976  0.894927536231884:  22%|██▏       | 68/314 [06:56<25:49,  6.30s/it] test:69/314 | accuracy 1976  0.894927536231884:  22%|██▏       | 69/314 [06:56<24:55,  6.10s/it]test:70/314 | accuracy 2002  0.89375:  22%|██▏       | 69/314 [07:01<24:55,  6.10s/it]          test:70/314 | accuracy 2002  0.89375:  22%|██▏       | 70/314 [07:01<23:44,  5.84s/it]test:71/314 | accuracy 2032  0.8943661971830986:  22%|██▏       | 70/314 [07:06<23:44,  5.84s/it]test:71/314 | accuracy 2032  0.8943661971830986:  23%|██▎       | 71/314 [07:06<22:57,  5.67s/it]test:72/314 | accuracy 2061  0.89453125:  23%|██▎       | 71/314 [07:12<22:57,  5.67s/it]        test:72/314 | accuracy 2061  0.89453125:  23%|██▎       | 72/314 [07:12<22:30,  5.58s/it]test:73/314 | accuracy 2091  0.8951198630136986:  23%|██▎       | 72/314 [07:18<22:30,  5.58s/it]test:73/314 | accuracy 2091  0.8951198630136986:  23%|██▎       | 73/314 [07:18<23:13,  5.78s/it]test:74/314 | accuracy 2120  0.8952702702702703:  23%|██▎       | 73/314 [07:23<23:13,  5.78s/it]test:74/314 | accuracy 2120  0.8952702702702703:  24%|██▎       | 74/314 [07:23<22:11,  5.55s/it]test:75/314 | accuracy 2148  0.895:  24%|██▎       | 74/314 [07:29<22:11,  5.55s/it]             test:75/314 | accuracy 2148  0.895:  24%|██▍       | 75/314 [07:29<22:24,  5.62s/it]test:76/314 | accuracy 2174  0.8939144736842105:  24%|██▍       | 75/314 [07:34<22:24,  5.62s/it]test:76/314 | accuracy 2174  0.8939144736842105:  24%|██▍       | 76/314 [07:34<22:09,  5.59s/it]test:77/314 | accuracy 2200  0.8928571428571429:  24%|██▍       | 76/314 [07:39<22:09,  5.59s/it]test:77/314 | accuracy 2200  0.8928571428571429:  25%|██▍       | 77/314 [07:39<21:26,  5.43s/it]test:78/314 | accuracy 2228  0.8926282051282052:  25%|██▍       | 77/314 [07:45<21:26,  5.43s/it]test:78/314 | accuracy 2228  0.8926282051282052:  25%|██▍       | 78/314 [07:45<21:52,  5.56s/it]test:79/314 | accuracy 2259  0.8935917721518988:  25%|██▍       | 78/314 [07:52<21:52,  5.56s/it]test:79/314 | accuracy 2259  0.8935917721518988:  25%|██▌       | 79/314 [07:52<23:03,  5.89s/it]test:80/314 | accuracy 2288  0.89375:  25%|██▌       | 79/314 [07:58<23:03,  5.89s/it]           test:80/314 | accuracy 2288  0.89375:  25%|██▌       | 80/314 [07:58<22:48,  5.85s/it]test:81/314 | accuracy 2319  0.8946759259259259:  25%|██▌       | 80/314 [08:03<22:48,  5.85s/it]test:81/314 | accuracy 2319  0.8946759259259259:  26%|██▌       | 81/314 [08:04<22:38,  5.83s/it]test:82/314 | accuracy 2348  0.8948170731707317:  26%|██▌       | 81/314 [08:09<22:38,  5.83s/it]test:82/314 | accuracy 2348  0.8948170731707317:  26%|██▌       | 82/314 [08:09<22:29,  5.81s/it]test:83/314 | accuracy 2380  0.8960843373493976:  26%|██▌       | 82/314 [08:16<22:29,  5.81s/it]test:83/314 | accuracy 2380  0.8960843373493976:  26%|██▋       | 83/314 [08:16<23:31,  6.11s/it]test:84/314 | accuracy 2408  0.8958333333333334:  26%|██▋       | 83/314 [08:23<23:31,  6.11s/it]test:84/314 | accuracy 2408  0.8958333333333334:  27%|██▋       | 84/314 [08:23<24:13,  6.32s/it]test:85/314 | accuracy 2437  0.8959558823529412:  27%|██▋       | 84/314 [08:29<24:13,  6.32s/it]test:85/314 | accuracy 2437  0.8959558823529412:  27%|██▋       | 85/314 [08:29<24:00,  6.29s/it]test:86/314 | accuracy 2465  0.8957122093023255:  27%|██▋       | 85/314 [08:35<24:00,  6.29s/it]test:86/314 | accuracy 2465  0.8957122093023255:  27%|██▋       | 86/314 [08:35<23:58,  6.31s/it]test:87/314 | accuracy 2491  0.8947557471264368:  27%|██▋       | 86/314 [08:42<23:58,  6.31s/it]test:87/314 | accuracy 2491  0.8947557471264368:  28%|██▊       | 87/314 [08:42<23:39,  6.25s/it]test:88/314 | accuracy 2519  0.89453125:  28%|██▊       | 87/314 [08:47<23:39,  6.25s/it]        test:88/314 | accuracy 2519  0.89453125:  28%|██▊       | 88/314 [08:47<22:31,  5.98s/it]test:89/314 | accuracy 2546  0.8939606741573034:  28%|██▊       | 88/314 [08:53<22:31,  5.98s/it]test:89/314 | accuracy 2546  0.8939606741573034:  28%|██▊       | 89/314 [08:53<22:22,  5.97s/it]test:90/314 | accuracy 2576  0.8944444444444445:  28%|██▊       | 89/314 [08:59<22:22,  5.97s/it]test:90/314 | accuracy 2576  0.8944444444444445:  29%|██▊       | 90/314 [08:59<22:16,  5.97s/it]test:91/314 | accuracy 2606  0.8949175824175825:  29%|██▊       | 90/314 [09:05<22:16,  5.97s/it]test:91/314 | accuracy 2606  0.8949175824175825:  29%|██▉       | 91/314 [09:05<22:09,  5.96s/it]test:92/314 | accuracy 2636  0.8953804347826086:  29%|██▉       | 91/314 [09:10<22:09,  5.96s/it]test:92/314 | accuracy 2636  0.8953804347826086:  29%|██▉       | 92/314 [09:10<21:34,  5.83s/it]test:93/314 | accuracy 2665  0.895497311827957:  29%|██▉       | 92/314 [09:17<21:34,  5.83s/it] test:93/314 | accuracy 2665  0.895497311827957:  30%|██▉       | 93/314 [09:17<22:02,  5.99s/it]test:94/314 | accuracy 2695  0.8959441489361702:  30%|██▉       | 93/314 [09:22<22:02,  5.99s/it]test:94/314 | accuracy 2695  0.8959441489361702:  30%|██▉       | 94/314 [09:23<21:47,  5.94s/it]test:95/314 | accuracy 2722  0.8953947368421052:  30%|██▉       | 94/314 [09:28<21:47,  5.94s/it]test:95/314 | accuracy 2722  0.8953947368421052:  30%|███       | 95/314 [09:28<20:51,  5.71s/it]test:96/314 | accuracy 2752  0.8958333333333334:  30%|███       | 95/314 [09:33<20:51,  5.71s/it]test:96/314 | accuracy 2752  0.8958333333333334:  31%|███       | 96/314 [09:33<20:39,  5.69s/it]test:97/314 | accuracy 2779  0.8952963917525774:  31%|███       | 96/314 [09:39<20:39,  5.69s/it]test:97/314 | accuracy 2779  0.8952963917525774:  31%|███       | 97/314 [09:39<20:25,  5.65s/it]test:98/314 | accuracy 2807  0.8950892857142857:  31%|███       | 97/314 [09:45<20:25,  5.65s/it]test:98/314 | accuracy 2807  0.8950892857142857:  31%|███       | 98/314 [09:45<20:59,  5.83s/it]test:99/314 | accuracy 2837  0.8955176767676768:  31%|███       | 98/314 [09:51<20:59,  5.83s/it]test:99/314 | accuracy 2837  0.8955176767676768:  32%|███▏      | 99/314 [09:51<21:04,  5.88s/it]test:100/314 | accuracy 2864  0.895:  32%|███▏      | 99/314 [09:58<21:04,  5.88s/it]            test:100/314 | accuracy 2864  0.895:  32%|███▏      | 100/314 [09:58<21:43,  6.09s/it]test:101/314 | accuracy 2892  0.8948019801980198:  32%|███▏      | 100/314 [10:03<21:43,  6.09s/it]test:101/314 | accuracy 2892  0.8948019801980198:  32%|███▏      | 101/314 [10:03<21:16,  5.99s/it]test:102/314 | accuracy 2922  0.8952205882352942:  32%|███▏      | 101/314 [10:13<21:16,  5.99s/it]test:102/314 | accuracy 2922  0.8952205882352942:  32%|███▏      | 102/314 [10:13<24:57,  7.07s/it]test:103/314 | accuracy 2952  0.8956310679611651:  32%|███▏      | 102/314 [10:23<24:57,  7.07s/it]test:103/314 | accuracy 2952  0.8956310679611651:  33%|███▎      | 103/314 [10:23<27:33,  7.84s/it]test:104/314 | accuracy 2981  0.8957331730769231:  33%|███▎      | 103/314 [10:32<27:33,  7.84s/it]test:104/314 | accuracy 2981  0.8957331730769231:  33%|███▎      | 104/314 [10:32<28:45,  8.21s/it]test:105/314 | accuracy 3013  0.8967261904761905:  33%|███▎      | 104/314 [10:41<28:45,  8.21s/it]test:105/314 | accuracy 3013  0.8967261904761905:  33%|███▎      | 105/314 [10:41<29:28,  8.46s/it]test:106/314 | accuracy 3045  0.8977004716981132:  33%|███▎      | 105/314 [10:50<29:28,  8.46s/it]test:106/314 | accuracy 3045  0.8977004716981132:  34%|███▍      | 106/314 [10:50<29:58,  8.64s/it]test:107/314 | accuracy 3077  0.8986565420560748:  34%|███▍      | 106/314 [10:59<29:58,  8.64s/it]test:107/314 | accuracy 3077  0.8986565420560748:  34%|███▍      | 107/314 [10:59<30:05,  8.72s/it]test:108/314 | accuracy 3107  0.8990162037037037:  34%|███▍      | 107/314 [11:07<30:05,  8.72s/it]test:108/314 | accuracy 3107  0.8990162037037037:  34%|███▍      | 108/314 [11:08<29:58,  8.73s/it]test:109/314 | accuracy 3136  0.8990825688073395:  34%|███▍      | 108/314 [11:17<29:58,  8.73s/it]test:109/314 | accuracy 3136  0.8990825688073395:  35%|███▍      | 109/314 [11:17<30:14,  8.85s/it]test:110/314 | accuracy 3168  0.9:  35%|███▍      | 109/314 [11:26<30:14,  8.85s/it]               test:110/314 | accuracy 3168  0.9:  35%|███▌      | 110/314 [11:26<30:25,  8.95s/it]test:111/314 | accuracy 3199  0.9006193693693694:  35%|███▌      | 110/314 [11:35<30:25,  8.95s/it]test:111/314 | accuracy 3199  0.9006193693693694:  35%|███▌      | 111/314 [11:35<30:48,  9.11s/it]test:112/314 | accuracy 3230  0.9012276785714286:  35%|███▌      | 111/314 [11:44<30:48,  9.11s/it]test:112/314 | accuracy 3230  0.9012276785714286:  36%|███▌      | 112/314 [11:44<30:36,  9.09s/it]test:113/314 | accuracy 3262  0.9021017699115044:  36%|███▌      | 112/314 [11:54<30:36,  9.09s/it]test:113/314 | accuracy 3262  0.9021017699115044:  36%|███▌      | 113/314 [11:54<31:25,  9.38s/it]test:114/314 | accuracy 3294  0.9029605263157895:  36%|███▌      | 113/314 [12:04<31:25,  9.38s/it]test:114/314 | accuracy 3294  0.9029605263157895:  36%|███▋      | 114/314 [12:04<31:03,  9.32s/it]test:115/314 | accuracy 3324  0.9032608695652173:  36%|███▋      | 114/314 [12:13<31:03,  9.32s/it]test:115/314 | accuracy 3324  0.9032608695652173:  37%|███▋      | 115/314 [12:13<30:53,  9.32s/it]test:116/314 | accuracy 3356  0.9040948275862069:  37%|███▋      | 115/314 [12:21<30:53,  9.32s/it]test:116/314 | accuracy 3356  0.9040948275862069:  37%|███▋      | 116/314 [12:22<30:04,  9.12s/it]test:117/314 | accuracy 3388  0.9049145299145299:  37%|███▋      | 116/314 [12:30<30:04,  9.12s/it]test:117/314 | accuracy 3388  0.9049145299145299:  37%|███▋      | 117/314 [12:30<29:39,  9.03s/it]test:118/314 | accuracy 3419  0.9054555084745762:  37%|███▋      | 117/314 [12:40<29:39,  9.03s/it]test:118/314 | accuracy 3419  0.9054555084745762:  38%|███▊      | 118/314 [12:40<29:47,  9.12s/it]test:119/314 | accuracy 3450  0.9059873949579832:  38%|███▊      | 118/314 [12:49<29:47,  9.12s/it]test:119/314 | accuracy 3450  0.9059873949579832:  38%|███▊      | 119/314 [12:49<30:01,  9.24s/it]test:120/314 | accuracy 3481  0.9065104166666667:  38%|███▊      | 119/314 [12:58<30:01,  9.24s/it]test:120/314 | accuracy 3481  0.9065104166666667:  38%|███▊      | 120/314 [12:58<29:43,  9.19s/it]test:121/314 | accuracy 3511  0.9067665289256198:  38%|███▊      | 120/314 [13:07<29:43,  9.19s/it]test:121/314 | accuracy 3511  0.9067665289256198:  39%|███▊      | 121/314 [13:07<29:16,  9.10s/it]test:122/314 | accuracy 3542  0.9072745901639344:  39%|███▊      | 121/314 [13:16<29:16,  9.10s/it]test:122/314 | accuracy 3542  0.9072745901639344:  39%|███▉      | 122/314 [13:16<29:12,  9.13s/it]test:123/314 | accuracy 3573  0.9077743902439024:  39%|███▉      | 122/314 [13:26<29:12,  9.13s/it]test:123/314 | accuracy 3573  0.9077743902439024:  39%|███▉      | 123/314 [13:26<29:18,  9.21s/it]test:124/314 | accuracy 3603  0.9080141129032258:  39%|███▉      | 123/314 [13:35<29:18,  9.21s/it]test:124/314 | accuracy 3603  0.9080141129032258:  39%|███▉      | 124/314 [13:35<28:47,  9.09s/it]test:125/314 | accuracy 3635  0.90875:  39%|███▉      | 124/314 [13:44<28:47,  9.09s/it]           test:125/314 | accuracy 3635  0.90875:  40%|███▉      | 125/314 [13:44<28:58,  9.20s/it]test:126/314 | accuracy 3663  0.9084821428571429:  40%|███▉      | 125/314 [13:53<28:58,  9.20s/it]test:126/314 | accuracy 3663  0.9084821428571429:  40%|████      | 126/314 [13:53<28:46,  9.19s/it]test:127/314 | accuracy 3694  0.9089566929133859:  40%|████      | 126/314 [14:03<28:46,  9.19s/it]test:127/314 | accuracy 3694  0.9089566929133859:  40%|████      | 127/314 [14:03<28:57,  9.29s/it]test:128/314 | accuracy 3725  0.909423828125:  40%|████      | 127/314 [14:13<28:57,  9.29s/it]    test:128/314 | accuracy 3725  0.909423828125:  41%|████      | 128/314 [14:13<29:41,  9.58s/it]test:129/314 | accuracy 3756  0.9098837209302325:  41%|████      | 128/314 [14:22<29:41,  9.58s/it]test:129/314 | accuracy 3756  0.9098837209302325:  41%|████      | 129/314 [14:22<29:20,  9.52s/it]test:130/314 | accuracy 3786  0.9100961538461538:  41%|████      | 129/314 [14:31<29:20,  9.52s/it]test:130/314 | accuracy 3786  0.9100961538461538:  41%|████▏     | 130/314 [14:31<28:25,  9.27s/it]test:131/314 | accuracy 3817  0.910543893129771:  41%|████▏     | 130/314 [14:40<28:25,  9.27s/it] test:131/314 | accuracy 3817  0.910543893129771:  42%|████▏     | 131/314 [14:40<28:04,  9.21s/it]test:132/314 | accuracy 3848  0.9109848484848485:  42%|████▏     | 131/314 [14:49<28:04,  9.21s/it]test:132/314 | accuracy 3848  0.9109848484848485:  42%|████▏     | 132/314 [14:49<27:29,  9.06s/it]test:133/314 | accuracy 3880  0.9116541353383458:  42%|████▏     | 132/314 [14:58<27:29,  9.06s/it]test:133/314 | accuracy 3880  0.9116541353383458:  42%|████▏     | 133/314 [14:58<27:50,  9.23s/it]test:134/314 | accuracy 3911  0.9120802238805971:  42%|████▏     | 133/314 [15:07<27:50,  9.23s/it]test:134/314 | accuracy 3911  0.9120802238805971:  43%|████▎     | 134/314 [15:08<27:34,  9.19s/it]test:135/314 | accuracy 3941  0.9122685185185185:  43%|████▎     | 134/314 [15:16<27:34,  9.19s/it]test:135/314 | accuracy 3941  0.9122685185185185:  43%|████▎     | 135/314 [15:16<27:10,  9.11s/it]test:136/314 | accuracy 3973  0.9129136029411765:  43%|████▎     | 135/314 [15:26<27:10,  9.11s/it]test:136/314 | accuracy 3973  0.9129136029411765:  43%|████▎     | 136/314 [15:26<27:33,  9.29s/it]test:137/314 | accuracy 4004  0.9133211678832117:  43%|████▎     | 136/314 [15:35<27:33,  9.29s/it]test:137/314 | accuracy 4004  0.9133211678832117:  44%|████▎     | 137/314 [15:35<26:45,  9.07s/it]test:138/314 | accuracy 4035  0.9137228260869565:  44%|████▎     | 137/314 [15:44<26:45,  9.07s/it]test:138/314 | accuracy 4035  0.9137228260869565:  44%|████▍     | 138/314 [15:44<26:42,  9.11s/it]test:139/314 | accuracy 4064  0.9136690647482014:  44%|████▍     | 138/314 [15:53<26:42,  9.11s/it]test:139/314 | accuracy 4064  0.9136690647482014:  44%|████▍     | 139/314 [15:53<26:15,  9.00s/it]test:140/314 | accuracy 4095  0.9140625:  44%|████▍     | 139/314 [16:01<26:15,  9.00s/it]         test:140/314 | accuracy 4095  0.9140625:  45%|████▍     | 140/314 [16:01<25:47,  8.89s/it]test:141/314 | accuracy 4125  0.9142287234042553:  45%|████▍     | 140/314 [16:10<25:47,  8.89s/it]test:141/314 | accuracy 4125  0.9142287234042553:  45%|████▍     | 141/314 [16:10<25:50,  8.96s/it]test:142/314 | accuracy 4157  0.9148327464788732:  45%|████▍     | 141/314 [16:20<25:50,  8.96s/it]test:142/314 | accuracy 4157  0.9148327464788732:  45%|████▌     | 142/314 [16:20<26:02,  9.09s/it]test:143/314 | accuracy 4189  0.9154283216783217:  45%|████▌     | 142/314 [16:29<26:02,  9.09s/it]test:143/314 | accuracy 4189  0.9154283216783217:  46%|████▌     | 143/314 [16:29<25:50,  9.07s/it]test:144/314 | accuracy 4221  0.916015625:  46%|████▌     | 143/314 [16:38<25:50,  9.07s/it]       test:144/314 | accuracy 4221  0.916015625:  46%|████▌     | 144/314 [16:38<25:55,  9.15s/it]test:145/314 | accuracy 4253  0.9165948275862069:  46%|████▌     | 144/314 [16:47<25:55,  9.15s/it]test:145/314 | accuracy 4253  0.9165948275862069:  46%|████▌     | 145/314 [16:47<25:44,  9.14s/it]test:146/314 | accuracy 4285  0.917166095890411:  46%|████▌     | 145/314 [16:56<25:44,  9.14s/it] test:146/314 | accuracy 4285  0.917166095890411:  46%|████▋     | 146/314 [16:57<25:37,  9.15s/it]test:147/314 | accuracy 4317  0.9177295918367347:  46%|████▋     | 146/314 [17:08<25:37,  9.15s/it]test:147/314 | accuracy 4317  0.9177295918367347:  47%|████▋     | 147/314 [17:08<27:12,  9.78s/it]test:148/314 | accuracy 4346  0.917652027027027:  47%|████▋     | 147/314 [17:17<27:12,  9.78s/it] test:148/314 | accuracy 4346  0.917652027027027:  47%|████▋     | 148/314 [17:17<26:17,  9.50s/it]test:149/314 | accuracy 4376  0.9177852348993288:  47%|████▋     | 148/314 [17:25<26:17,  9.50s/it]test:149/314 | accuracy 4376  0.9177852348993288:  47%|████▋     | 149/314 [17:25<25:31,  9.28s/it]test:150/314 | accuracy 4408  0.9183333333333333:  47%|████▋     | 149/314 [17:36<25:31,  9.28s/it]test:150/314 | accuracy 4408  0.9183333333333333:  48%|████▊     | 150/314 [17:36<26:20,  9.64s/it]test:151/314 | accuracy 4439  0.9186672185430463:  48%|████▊     | 150/314 [17:45<26:20,  9.64s/it]test:151/314 | accuracy 4439  0.9186672185430463:  48%|████▊     | 151/314 [17:45<25:58,  9.56s/it]test:152/314 | accuracy 4469  0.9187911184210527:  48%|████▊     | 151/314 [17:54<25:58,  9.56s/it]test:152/314 | accuracy 4469  0.9187911184210527:  48%|████▊     | 152/314 [17:54<25:27,  9.43s/it]test:153/314 | accuracy 4499  0.9189133986928104:  48%|████▊     | 152/314 [18:04<25:27,  9.43s/it]test:153/314 | accuracy 4499  0.9189133986928104:  49%|████▊     | 153/314 [18:04<25:23,  9.46s/it]test:154/314 | accuracy 4531  0.919439935064935:  49%|████▊     | 153/314 [18:13<25:23,  9.46s/it] test:154/314 | accuracy 4531  0.919439935064935:  49%|████▉     | 154/314 [18:13<24:48,  9.30s/it]test:155/314 | accuracy 4563  0.9199596774193548:  49%|████▉     | 154/314 [18:22<24:48,  9.30s/it]test:155/314 | accuracy 4563  0.9199596774193548:  49%|████▉     | 155/314 [18:22<24:28,  9.24s/it]test:156/314 | accuracy 4594  0.9202724358974359:  49%|████▉     | 155/314 [18:31<24:28,  9.24s/it]test:156/314 | accuracy 4594  0.9202724358974359:  50%|████▉     | 156/314 [18:31<24:31,  9.31s/it]test:157/314 | accuracy 4625  0.9205812101910829:  50%|████▉     | 156/314 [18:41<24:31,  9.31s/it]test:157/314 | accuracy 4625  0.9205812101910829:  50%|█████     | 157/314 [18:41<24:20,  9.30s/it]test:158/314 | accuracy 4656  0.9208860759493671:  50%|█████     | 157/314 [18:50<24:20,  9.30s/it]test:158/314 | accuracy 4656  0.9208860759493671:  50%|█████     | 158/314 [18:50<23:54,  9.19s/it]test:159/314 | accuracy 4687  0.921187106918239:  50%|█████     | 158/314 [18:59<23:54,  9.19s/it] test:159/314 | accuracy 4687  0.921187106918239:  51%|█████     | 159/314 [18:59<23:57,  9.28s/it]test:160/314 | accuracy 4718  0.921484375:  51%|█████     | 159/314 [19:08<23:57,  9.28s/it]      test:160/314 | accuracy 4718  0.921484375:  51%|█████     | 160/314 [19:08<23:48,  9.28s/it]test:161/314 | accuracy 4747  0.921389751552795:  51%|█████     | 160/314 [19:18<23:48,  9.28s/it]test:161/314 | accuracy 4747  0.921389751552795:  51%|█████▏    | 161/314 [19:18<23:49,  9.34s/it]test:162/314 | accuracy 4777  0.9214891975308642:  51%|█████▏    | 161/314 [19:27<23:49,  9.34s/it]test:162/314 | accuracy 4777  0.9214891975308642:  52%|█████▏    | 162/314 [19:27<23:16,  9.18s/it]test:163/314 | accuracy 4809  0.9219708588957055:  52%|█████▏    | 162/314 [19:36<23:16,  9.18s/it]test:163/314 | accuracy 4809  0.9219708588957055:  52%|█████▏    | 163/314 [19:36<23:06,  9.18s/it]test:164/314 | accuracy 4840  0.9222560975609756:  52%|█████▏    | 163/314 [19:45<23:06,  9.18s/it]test:164/314 | accuracy 4840  0.9222560975609756:  52%|█████▏    | 164/314 [19:45<22:57,  9.18s/it]test:165/314 | accuracy 4871  0.9225378787878787:  52%|█████▏    | 164/314 [19:55<22:57,  9.18s/it]test:165/314 | accuracy 4871  0.9225378787878787:  53%|█████▎    | 165/314 [19:55<23:08,  9.32s/it]test:166/314 | accuracy 4902  0.922816265060241:  53%|█████▎    | 165/314 [20:04<23:08,  9.32s/it] test:166/314 | accuracy 4902  0.922816265060241:  53%|█████▎    | 166/314 [20:04<22:57,  9.31s/it]test:167/314 | accuracy 4933  0.9230913173652695:  53%|█████▎    | 166/314 [20:13<22:57,  9.31s/it]test:167/314 | accuracy 4933  0.9230913173652695:  53%|█████▎    | 167/314 [20:13<22:41,  9.26s/it]test:168/314 | accuracy 4965  0.9235491071428571:  53%|█████▎    | 167/314 [20:22<22:41,  9.26s/it]test:168/314 | accuracy 4965  0.9235491071428571:  54%|█████▎    | 168/314 [20:22<22:14,  9.14s/it]test:169/314 | accuracy 4994  0.9234467455621301:  54%|█████▎    | 168/314 [20:31<22:14,  9.14s/it]test:169/314 | accuracy 4994  0.9234467455621301:  54%|█████▍    | 169/314 [20:31<22:11,  9.19s/it]test:170/314 | accuracy 5025  0.9237132352941176:  54%|█████▍    | 169/314 [20:41<22:11,  9.19s/it]test:170/314 | accuracy 5025  0.9237132352941176:  54%|█████▍    | 170/314 [20:41<22:36,  9.42s/it]test:171/314 | accuracy 5056  0.9239766081871345:  54%|█████▍    | 170/314 [20:50<22:36,  9.42s/it]test:171/314 | accuracy 5056  0.9239766081871345:  54%|█████▍    | 171/314 [20:50<22:19,  9.36s/it]test:172/314 | accuracy 5087  0.9242369186046512:  54%|█████▍    | 171/314 [20:59<22:19,  9.36s/it]test:172/314 | accuracy 5087  0.9242369186046512:  55%|█████▍    | 172/314 [21:00<22:00,  9.30s/it]test:173/314 | accuracy 5117  0.9243135838150289:  55%|█████▍    | 172/314 [21:09<22:00,  9.30s/it]test:173/314 | accuracy 5117  0.9243135838150289:  55%|█████▌    | 173/314 [21:09<21:44,  9.25s/it]test:174/314 | accuracy 5149  0.9247485632183908:  55%|█████▌    | 173/314 [21:17<21:44,  9.25s/it]test:174/314 | accuracy 5149  0.9247485632183908:  55%|█████▌    | 174/314 [21:18<21:17,  9.13s/it]test:175/314 | accuracy 5178  0.9246428571428571:  55%|█████▌    | 174/314 [21:27<21:17,  9.13s/it]test:175/314 | accuracy 5178  0.9246428571428571:  56%|█████▌    | 175/314 [21:27<21:14,  9.17s/it]test:176/314 | accuracy 5210  0.9250710227272727:  56%|█████▌    | 175/314 [21:36<21:14,  9.17s/it]test:176/314 | accuracy 5210  0.9250710227272727:  56%|█████▌    | 176/314 [21:36<21:11,  9.21s/it]test:177/314 | accuracy 5240  0.9251412429378532:  56%|█████▌    | 176/314 [21:45<21:11,  9.21s/it]test:177/314 | accuracy 5240  0.9251412429378532:  56%|█████▋    | 177/314 [21:45<21:01,  9.21s/it]test:178/314 | accuracy 5272  0.925561797752809:  56%|█████▋    | 177/314 [21:54<21:01,  9.21s/it] test:178/314 | accuracy 5272  0.925561797752809:  57%|█████▋    | 178/314 [21:55<20:51,  9.20s/it]test:179/314 | accuracy 5303  0.9258030726256983:  57%|█████▋    | 178/314 [22:04<20:51,  9.20s/it]test:179/314 | accuracy 5303  0.9258030726256983:  57%|█████▋    | 179/314 [22:04<20:48,  9.24s/it]test:180/314 | accuracy 5335  0.9262152777777778:  57%|█████▋    | 179/314 [22:13<20:48,  9.24s/it]test:180/314 | accuracy 5335  0.9262152777777778:  57%|█████▋    | 180/314 [22:13<20:47,  9.31s/it]test:181/314 | accuracy 5367  0.9266229281767956:  57%|█████▋    | 180/314 [22:23<20:47,  9.31s/it]test:181/314 | accuracy 5367  0.9266229281767956:  58%|█████▊    | 181/314 [22:23<20:40,  9.33s/it]test:182/314 | accuracy 5398  0.9268543956043956:  58%|█████▊    | 181/314 [22:32<20:40,  9.33s/it]test:182/314 | accuracy 5398  0.9268543956043956:  58%|█████▊    | 182/314 [22:32<20:28,  9.31s/it]test:183/314 | accuracy 5428  0.9269125683060109:  58%|█████▊    | 182/314 [22:41<20:28,  9.31s/it]test:183/314 | accuracy 5428  0.9269125683060109:  58%|█████▊    | 183/314 [22:41<19:58,  9.15s/it]test:184/314 | accuracy 5459  0.927139945652174:  58%|█████▊    | 183/314 [22:50<19:58,  9.15s/it] test:184/314 | accuracy 5459  0.927139945652174:  59%|█████▊    | 184/314 [22:50<20:04,  9.27s/it]test:185/314 | accuracy 5490  0.9273648648648649:  59%|█████▊    | 184/314 [23:00<20:04,  9.27s/it]test:185/314 | accuracy 5490  0.9273648648648649:  59%|█████▉    | 185/314 [23:00<19:59,  9.29s/it]test:186/314 | accuracy 5521  0.9275873655913979:  59%|█████▉    | 185/314 [23:08<19:59,  9.29s/it]test:186/314 | accuracy 5521  0.9275873655913979:  59%|█████▉    | 186/314 [23:09<19:34,  9.18s/it]test:187/314 | accuracy 5550  0.9274732620320856:  59%|█████▉    | 186/314 [23:18<19:34,  9.18s/it]test:187/314 | accuracy 5550  0.9274732620320856:  60%|█████▉    | 187/314 [23:18<19:28,  9.20s/it]test:188/314 | accuracy 5581  0.9276928191489362:  60%|█████▉    | 187/314 [23:27<19:28,  9.20s/it]test:188/314 | accuracy 5581  0.9276928191489362:  60%|█████▉    | 188/314 [23:27<19:05,  9.09s/it]test:189/314 | accuracy 5611  0.927744708994709:  60%|█████▉    | 188/314 [23:36<19:05,  9.09s/it] test:189/314 | accuracy 5611  0.927744708994709:  60%|██████    | 189/314 [23:36<18:55,  9.08s/it]test:190/314 | accuracy 5640  0.9276315789473685:  60%|██████    | 189/314 [23:45<18:55,  9.08s/it]test:190/314 | accuracy 5640  0.9276315789473685:  61%|██████    | 190/314 [23:45<18:42,  9.06s/it]test:191/314 | accuracy 5671  0.9278468586387435:  61%|██████    | 190/314 [23:54<18:42,  9.06s/it]test:191/314 | accuracy 5671  0.9278468586387435:  61%|██████    | 191/314 [23:54<18:39,  9.10s/it]test:192/314 | accuracy 5700  0.927734375:  61%|██████    | 191/314 [24:03<18:39,  9.10s/it]       test:192/314 | accuracy 5700  0.927734375:  61%|██████    | 192/314 [24:03<18:20,  9.02s/it]test:193/314 | accuracy 5730  0.9277849740932642:  61%|██████    | 192/314 [24:12<18:20,  9.02s/it]test:193/314 | accuracy 5730  0.9277849740932642:  61%|██████▏   | 193/314 [24:12<18:13,  9.04s/it]test:194/314 | accuracy 5761  0.9279961340206185:  61%|██████▏   | 193/314 [24:21<18:13,  9.04s/it]test:194/314 | accuracy 5761  0.9279961340206185:  62%|██████▏   | 194/314 [24:21<18:15,  9.13s/it]test:195/314 | accuracy 5792  0.9282051282051282:  62%|██████▏   | 194/314 [24:30<18:15,  9.13s/it]test:195/314 | accuracy 5792  0.9282051282051282:  62%|██████▏   | 195/314 [24:30<18:00,  9.08s/it]test:196/314 | accuracy 5824  0.9285714285714286:  62%|██████▏   | 195/314 [24:39<18:00,  9.08s/it]test:196/314 | accuracy 5824  0.9285714285714286:  62%|██████▏   | 196/314 [24:39<17:58,  9.14s/it]test:197/314 | accuracy 5853  0.9284581218274112:  62%|██████▏   | 196/314 [24:48<17:58,  9.14s/it]test:197/314 | accuracy 5853  0.9284581218274112:  63%|██████▎   | 197/314 [24:48<17:37,  9.04s/it]test:198/314 | accuracy 5884  0.9286616161616161:  63%|██████▎   | 197/314 [24:57<17:37,  9.04s/it]test:198/314 | accuracy 5884  0.9286616161616161:  63%|██████▎   | 198/314 [24:57<17:25,  9.01s/it]test:199/314 | accuracy 5914  0.9287060301507538:  63%|██████▎   | 198/314 [25:06<17:25,  9.01s/it]test:199/314 | accuracy 5914  0.9287060301507538:  63%|██████▎   | 199/314 [25:06<17:15,  9.01s/it]test:200/314 | accuracy 5945  0.92890625:  63%|██████▎   | 199/314 [25:15<17:15,  9.01s/it]        test:200/314 | accuracy 5945  0.92890625:  64%|██████▎   | 200/314 [25:15<17:10,  9.04s/it]test:201/314 | accuracy 5977  0.9292599502487562:  64%|██████▎   | 200/314 [25:24<17:10,  9.04s/it]test:201/314 | accuracy 5977  0.9292599502487562:  64%|██████▍   | 201/314 [25:24<16:53,  8.97s/it]test:202/314 | accuracy 6008  0.9294554455445545:  64%|██████▍   | 201/314 [25:33<16:53,  8.97s/it]test:202/314 | accuracy 6008  0.9294554455445545:  64%|██████▍   | 202/314 [25:33<16:42,  8.95s/it]test:203/314 | accuracy 6040  0.9298029556650246:  64%|██████▍   | 202/314 [25:42<16:42,  8.95s/it]test:203/314 | accuracy 6040  0.9298029556650246:  65%|██████▍   | 203/314 [25:42<16:42,  9.03s/it]test:204/314 | accuracy 6072  0.9301470588235294:  65%|██████▍   | 203/314 [25:51<16:42,  9.03s/it]test:204/314 | accuracy 6072  0.9301470588235294:  65%|██████▍   | 204/314 [25:51<16:31,  9.01s/it]test:205/314 | accuracy 6104  0.9304878048780488:  65%|██████▍   | 204/314 [26:00<16:31,  9.01s/it]test:205/314 | accuracy 6104  0.9304878048780488:  65%|██████▌   | 205/314 [26:01<16:35,  9.13s/it]test:206/314 | accuracy 6135  0.9306735436893204:  65%|██████▌   | 205/314 [26:10<16:35,  9.13s/it]test:206/314 | accuracy 6135  0.9306735436893204:  66%|██████▌   | 206/314 [26:10<16:33,  9.20s/it]test:207/314 | accuracy 6165  0.9307065217391305:  66%|██████▌   | 206/314 [26:19<16:33,  9.20s/it]test:207/314 | accuracy 6165  0.9307065217391305:  66%|██████▌   | 207/314 [26:19<16:29,  9.25s/it]test:208/314 | accuracy 6196  0.9308894230769231:  66%|██████▌   | 207/314 [26:28<16:29,  9.25s/it]test:208/314 | accuracy 6196  0.9308894230769231:  66%|██████▌   | 208/314 [26:28<16:10,  9.15s/it]test:209/314 | accuracy 6227  0.9310705741626795:  66%|██████▌   | 208/314 [26:37<16:10,  9.15s/it]test:209/314 | accuracy 6227  0.9310705741626795:  67%|██████▋   | 209/314 [26:37<16:02,  9.17s/it]test:210/314 | accuracy 6259  0.9313988095238095:  67%|██████▋   | 209/314 [26:46<16:02,  9.17s/it]test:210/314 | accuracy 6259  0.9313988095238095:  67%|██████▋   | 210/314 [26:47<15:52,  9.16s/it]test:211/314 | accuracy 6289  0.9314277251184834:  67%|██████▋   | 210/314 [26:56<15:52,  9.16s/it]test:211/314 | accuracy 6289  0.9314277251184834:  67%|██████▋   | 211/314 [26:56<15:56,  9.29s/it]test:212/314 | accuracy 6318  0.9313089622641509:  67%|██████▋   | 211/314 [27:05<15:56,  9.29s/it]test:212/314 | accuracy 6318  0.9313089622641509:  68%|██████▊   | 212/314 [27:05<15:28,  9.10s/it]test:213/314 | accuracy 6349  0.9314847417840375:  68%|██████▊   | 212/314 [27:15<15:28,  9.10s/it]test:213/314 | accuracy 6349  0.9314847417840375:  68%|██████▊   | 213/314 [27:15<15:43,  9.34s/it]test:214/314 | accuracy 6381  0.931804906542056:  68%|██████▊   | 213/314 [27:23<15:43,  9.34s/it] test:214/314 | accuracy 6381  0.931804906542056:  68%|██████▊   | 214/314 [27:23<15:15,  9.16s/it]test:215/314 | accuracy 6410  0.9316860465116279:  68%|██████▊   | 214/314 [27:33<15:15,  9.16s/it]test:215/314 | accuracy 6410  0.9316860465116279:  68%|██████▊   | 215/314 [27:33<15:15,  9.25s/it]test:216/314 | accuracy 6441  0.9318576388888888:  68%|██████▊   | 215/314 [27:42<15:15,  9.25s/it]test:216/314 | accuracy 6441  0.9318576388888888:  69%|██████▉   | 216/314 [27:42<15:03,  9.22s/it]test:217/314 | accuracy 6471  0.9318836405529954:  69%|██████▉   | 216/314 [27:51<15:03,  9.22s/it]test:217/314 | accuracy 6471  0.9318836405529954:  69%|██████▉   | 217/314 [27:51<14:54,  9.22s/it]test:218/314 | accuracy 6503  0.9321961009174312:  69%|██████▉   | 217/314 [28:01<14:54,  9.22s/it]test:218/314 | accuracy 6503  0.9321961009174312:  69%|██████▉   | 218/314 [28:01<14:49,  9.27s/it]test:219/314 | accuracy 6535  0.932505707762557:  69%|██████▉   | 218/314 [28:11<14:49,  9.27s/it] test:219/314 | accuracy 6535  0.932505707762557:  70%|██████▉   | 219/314 [28:11<15:04,  9.52s/it]test:220/314 | accuracy 6565  0.9325284090909091:  70%|██████▉   | 219/314 [28:20<15:04,  9.52s/it]test:220/314 | accuracy 6565  0.9325284090909091:  70%|███████   | 220/314 [28:20<14:45,  9.42s/it]test:221/314 | accuracy 6597  0.9328337104072398:  70%|███████   | 220/314 [28:29<14:45,  9.42s/it]test:221/314 | accuracy 6597  0.9328337104072398:  70%|███████   | 221/314 [28:29<14:32,  9.38s/it]test:222/314 | accuracy 6628  0.9329954954954955:  70%|███████   | 221/314 [28:38<14:32,  9.38s/it]test:222/314 | accuracy 6628  0.9329954954954955:  71%|███████   | 222/314 [28:38<14:09,  9.23s/it]test:223/314 | accuracy 6658  0.9330156950672646:  71%|███████   | 222/314 [28:47<14:09,  9.23s/it]test:223/314 | accuracy 6658  0.9330156950672646:  71%|███████   | 223/314 [28:47<13:59,  9.22s/it]test:224/314 | accuracy 6690  0.9333147321428571:  71%|███████   | 223/314 [28:56<13:59,  9.22s/it]test:224/314 | accuracy 6690  0.9333147321428571:  71%|███████▏  | 224/314 [28:56<13:37,  9.08s/it]test:225/314 | accuracy 6720  0.9333333333333333:  71%|███████▏  | 224/314 [29:05<13:37,  9.08s/it]test:225/314 | accuracy 6720  0.9333333333333333:  72%|███████▏  | 225/314 [29:05<13:25,  9.06s/it]test:226/314 | accuracy 6752  0.9336283185840708:  72%|███████▏  | 225/314 [29:15<13:25,  9.06s/it]test:226/314 | accuracy 6752  0.9336283185840708:  72%|███████▏  | 226/314 [29:15<13:34,  9.25s/it]test:227/314 | accuracy 6783  0.9337830396475771:  72%|███████▏  | 226/314 [29:24<13:34,  9.25s/it]test:227/314 | accuracy 6783  0.9337830396475771:  72%|███████▏  | 227/314 [29:24<13:34,  9.37s/it]test:228/314 | accuracy 6815  0.9340734649122807:  72%|███████▏  | 227/314 [29:33<13:34,  9.37s/it]test:228/314 | accuracy 6815  0.9340734649122807:  73%|███████▎  | 228/314 [29:34<13:17,  9.28s/it]test:229/314 | accuracy 6847  0.9343613537117904:  73%|███████▎  | 228/314 [29:43<13:17,  9.28s/it]test:229/314 | accuracy 6847  0.9343613537117904:  73%|███████▎  | 229/314 [29:43<13:15,  9.36s/it]test:230/314 | accuracy 6878  0.9345108695652173:  73%|███████▎  | 229/314 [29:52<13:15,  9.36s/it]test:230/314 | accuracy 6878  0.9345108695652173:  73%|███████▎  | 230/314 [29:53<13:08,  9.38s/it]test:231/314 | accuracy 6908  0.9345238095238095:  73%|███████▎  | 230/314 [30:01<13:08,  9.38s/it]test:231/314 | accuracy 6908  0.9345238095238095:  74%|███████▎  | 231/314 [30:02<12:50,  9.29s/it]test:232/314 | accuracy 6939  0.9346713362068966:  74%|███████▎  | 231/314 [30:10<12:50,  9.29s/it]test:232/314 | accuracy 6939  0.9346713362068966:  74%|███████▍  | 232/314 [30:11<12:34,  9.20s/it]test:233/314 | accuracy 6970  0.9348175965665236:  74%|███████▍  | 232/314 [30:20<12:34,  9.20s/it]test:233/314 | accuracy 6970  0.9348175965665236:  74%|███████▍  | 233/314 [30:20<12:30,  9.27s/it]test:234/314 | accuracy 7002  0.9350961538461539:  74%|███████▍  | 233/314 [30:29<12:30,  9.27s/it]test:234/314 | accuracy 7002  0.9350961538461539:  75%|███████▍  | 234/314 [30:29<12:21,  9.27s/it]test:235/314 | accuracy 7034  0.935372340425532:  75%|███████▍  | 234/314 [30:38<12:21,  9.27s/it] test:235/314 | accuracy 7034  0.935372340425532:  75%|███████▍  | 235/314 [30:39<12:12,  9.27s/it]test:236/314 | accuracy 7066  0.935646186440678:  75%|███████▍  | 235/314 [30:47<12:12,  9.27s/it]test:236/314 | accuracy 7066  0.935646186440678:  75%|███████▌  | 236/314 [30:47<11:53,  9.15s/it]test:237/314 | accuracy 7097  0.9357858649789029:  75%|███████▌  | 236/314 [30:56<11:53,  9.15s/it]test:237/314 | accuracy 7097  0.9357858649789029:  75%|███████▌  | 237/314 [30:57<11:46,  9.17s/it]test:238/314 | accuracy 7129  0.9360556722689075:  75%|███████▌  | 237/314 [31:06<11:46,  9.17s/it]test:238/314 | accuracy 7129  0.9360556722689075:  76%|███████▌  | 238/314 [31:06<11:41,  9.23s/it]test:239/314 | accuracy 7160  0.9361924686192469:  76%|███████▌  | 238/314 [31:16<11:41,  9.23s/it]test:239/314 | accuracy 7160  0.9361924686192469:  76%|███████▌  | 239/314 [31:16<11:42,  9.37s/it]test:240/314 | accuracy 7192  0.9364583333333333:  76%|███████▌  | 239/314 [31:25<11:42,  9.37s/it]test:240/314 | accuracy 7192  0.9364583333333333:  76%|███████▋  | 240/314 [31:25<11:34,  9.38s/it]test:241/314 | accuracy 7224  0.9367219917012448:  76%|███████▋  | 240/314 [31:34<11:34,  9.38s/it]test:241/314 | accuracy 7224  0.9367219917012448:  77%|███████▋  | 241/314 [31:34<11:22,  9.35s/it]test:242/314 | accuracy 7256  0.9369834710743802:  77%|███████▋  | 241/314 [31:44<11:22,  9.35s/it]test:242/314 | accuracy 7256  0.9369834710743802:  77%|███████▋  | 242/314 [31:44<11:13,  9.35s/it]test:243/314 | accuracy 7287  0.9371141975308642:  77%|███████▋  | 242/314 [31:53<11:13,  9.35s/it]test:243/314 | accuracy 7287  0.9371141975308642:  77%|███████▋  | 243/314 [31:53<11:01,  9.32s/it]test:244/314 | accuracy 7318  0.9372438524590164:  77%|███████▋  | 243/314 [32:02<11:01,  9.32s/it]test:244/314 | accuracy 7318  0.9372438524590164:  78%|███████▊  | 244/314 [32:02<10:43,  9.19s/it]test:245/314 | accuracy 7350  0.9375:  78%|███████▊  | 244/314 [32:11<10:43,  9.19s/it]            test:245/314 | accuracy 7350  0.9375:  78%|███████▊  | 245/314 [32:11<10:32,  9.17s/it]test:246/314 | accuracy 7380  0.9375:  78%|███████▊  | 245/314 [32:20<10:32,  9.17s/it]test:246/314 | accuracy 7380  0.9375:  78%|███████▊  | 246/314 [32:20<10:23,  9.17s/it]test:247/314 | accuracy 7412  0.937753036437247:  78%|███████▊  | 246/314 [32:30<10:23,  9.17s/it]test:247/314 | accuracy 7412  0.937753036437247:  79%|███████▊  | 247/314 [32:30<10:22,  9.29s/it]test:248/314 | accuracy 7442  0.9377520161290323:  79%|███████▊  | 247/314 [32:39<10:22,  9.29s/it]test:248/314 | accuracy 7442  0.9377520161290323:  79%|███████▉  | 248/314 [32:39<10:10,  9.24s/it]test:249/314 | accuracy 7473  0.9378765060240963:  79%|███████▉  | 248/314 [32:48<10:10,  9.24s/it]test:249/314 | accuracy 7473  0.9378765060240963:  79%|███████▉  | 249/314 [32:48<09:58,  9.21s/it]test:250/314 | accuracy 7505  0.938125:  79%|███████▉  | 249/314 [32:57<09:58,  9.21s/it]          test:250/314 | accuracy 7505  0.938125:  80%|███████▉  | 250/314 [32:58<09:57,  9.34s/it]test:251/314 | accuracy 7536  0.9382470119521913:  80%|███████▉  | 250/314 [33:07<09:57,  9.34s/it]test:251/314 | accuracy 7536  0.9382470119521913:  80%|███████▉  | 251/314 [33:07<09:44,  9.28s/it]test:252/314 | accuracy 7568  0.9384920634920635:  80%|███████▉  | 251/314 [33:16<09:44,  9.28s/it]test:252/314 | accuracy 7568  0.9384920634920635:  80%|████████  | 252/314 [33:16<09:35,  9.28s/it]test:253/314 | accuracy 7599  0.9386116600790514:  80%|████████  | 252/314 [33:25<09:35,  9.28s/it]test:253/314 | accuracy 7599  0.9386116600790514:  81%|████████  | 253/314 [33:26<09:31,  9.36s/it]test:254/314 | accuracy 7631  0.9388533464566929:  81%|████████  | 253/314 [33:35<09:31,  9.36s/it]test:254/314 | accuracy 7631  0.9388533464566929:  81%|████████  | 254/314 [33:35<09:20,  9.33s/it]test:255/314 | accuracy 7659  0.9386029411764706:  81%|████████  | 254/314 [33:44<09:20,  9.33s/it]test:255/314 | accuracy 7659  0.9386029411764706:  81%|████████  | 255/314 [33:44<09:08,  9.30s/it]test:256/314 | accuracy 7688  0.9384765625:  81%|████████  | 255/314 [33:53<09:08,  9.30s/it]      test:256/314 | accuracy 7688  0.9384765625:  82%|████████▏ | 256/314 [33:53<08:59,  9.29s/it]test:257/314 | accuracy 7720  0.938715953307393:  82%|████████▏ | 256/314 [34:02<08:59,  9.29s/it]test:257/314 | accuracy 7720  0.938715953307393:  82%|████████▏ | 257/314 [34:03<08:47,  9.26s/it]test:258/314 | accuracy 7751  0.9388323643410853:  82%|████████▏ | 257/314 [34:12<08:47,  9.26s/it]test:258/314 | accuracy 7751  0.9388323643410853:  82%|████████▏ | 258/314 [34:12<08:38,  9.26s/it]test:259/314 | accuracy 7782  0.9389478764478765:  82%|████████▏ | 258/314 [34:21<08:38,  9.26s/it]test:259/314 | accuracy 7782  0.9389478764478765:  82%|████████▏ | 259/314 [34:21<08:35,  9.38s/it]test:260/314 | accuracy 7814  0.9391826923076924:  82%|████████▏ | 259/314 [34:30<08:35,  9.38s/it]test:260/314 | accuracy 7814  0.9391826923076924:  83%|████████▎ | 260/314 [34:31<08:21,  9.28s/it]test:261/314 | accuracy 7843  0.9390565134099617:  83%|████████▎ | 260/314 [34:40<08:21,  9.28s/it]test:261/314 | accuracy 7843  0.9390565134099617:  83%|████████▎ | 261/314 [34:40<08:11,  9.28s/it]test:262/314 | accuracy 7874  0.9391698473282443:  83%|████████▎ | 261/314 [34:49<08:11,  9.28s/it]test:262/314 | accuracy 7874  0.9391698473282443:  83%|████████▎ | 262/314 [34:49<08:00,  9.23s/it]test:263/314 | accuracy 7905  0.939282319391635:  83%|████████▎ | 262/314 [34:58<08:00,  9.23s/it] test:263/314 | accuracy 7905  0.939282319391635:  84%|████████▍ | 263/314 [34:58<07:46,  9.14s/it]test:264/314 | accuracy 7936  0.9393939393939394:  84%|████████▍ | 263/314 [35:07<07:46,  9.14s/it]test:264/314 | accuracy 7936  0.9393939393939394:  84%|████████▍ | 264/314 [35:07<07:42,  9.25s/it]test:265/314 | accuracy 7966  0.9393867924528302:  84%|████████▍ | 264/314 [35:16<07:42,  9.25s/it]test:265/314 | accuracy 7966  0.9393867924528302:  84%|████████▍ | 265/314 [35:16<07:31,  9.20s/it]test:266/314 | accuracy 7997  0.9394971804511278:  84%|████████▍ | 265/314 [35:26<07:31,  9.20s/it]test:266/314 | accuracy 7997  0.9394971804511278:  85%|████████▍ | 266/314 [35:26<07:26,  9.31s/it]test:267/314 | accuracy 8029  0.9397237827715356:  85%|████████▍ | 266/314 [35:35<07:26,  9.31s/it]test:267/314 | accuracy 8029  0.9397237827715356:  85%|████████▌ | 267/314 [35:35<07:16,  9.28s/it]test:268/314 | accuracy 8060  0.9398320895522388:  85%|████████▌ | 267/314 [35:45<07:16,  9.28s/it]test:268/314 | accuracy 8060  0.9398320895522388:  85%|████████▌ | 268/314 [35:45<07:09,  9.35s/it]test:269/314 | accuracy 8090  0.9398234200743495:  85%|████████▌ | 268/314 [35:54<07:09,  9.35s/it]test:269/314 | accuracy 8090  0.9398234200743495:  86%|████████▌ | 269/314 [35:54<07:02,  9.39s/it]test:270/314 | accuracy 8121  0.9399305555555556:  86%|████████▌ | 269/314 [36:03<07:02,  9.39s/it]test:270/314 | accuracy 8121  0.9399305555555556:  86%|████████▌ | 270/314 [36:03<06:50,  9.33s/it]test:271/314 | accuracy 8151  0.9399215867158671:  86%|████████▌ | 270/314 [36:12<06:50,  9.33s/it]test:271/314 | accuracy 8151  0.9399215867158671:  86%|████████▋ | 271/314 [36:12<06:36,  9.22s/it]test:272/314 | accuracy 8182  0.9400275735294118:  86%|████████▋ | 271/314 [36:22<06:36,  9.22s/it]test:272/314 | accuracy 8182  0.9400275735294118:  87%|████████▋ | 272/314 [36:22<06:33,  9.37s/it]test:273/314 | accuracy 8212  0.940018315018315:  87%|████████▋ | 272/314 [36:31<06:33,  9.37s/it] test:273/314 | accuracy 8212  0.940018315018315:  87%|████████▋ | 273/314 [36:31<06:19,  9.26s/it]test:274/314 | accuracy 8241  0.9398950729927007:  87%|████████▋ | 273/314 [36:40<06:19,  9.26s/it]test:274/314 | accuracy 8241  0.9398950729927007:  87%|████████▋ | 274/314 [36:40<06:09,  9.24s/it]test:275/314 | accuracy 8270  0.9397727272727273:  87%|████████▋ | 274/314 [36:49<06:09,  9.24s/it]test:275/314 | accuracy 8270  0.9397727272727273:  88%|████████▊ | 275/314 [36:49<05:59,  9.22s/it]test:276/314 | accuracy 8301  0.9398777173913043:  88%|████████▊ | 275/314 [36:59<05:59,  9.22s/it]test:276/314 | accuracy 8301  0.9398777173913043:  88%|████████▊ | 276/314 [36:59<05:50,  9.22s/it]test:277/314 | accuracy 8332  0.9399819494584838:  88%|████████▊ | 276/314 [37:08<05:50,  9.22s/it]test:277/314 | accuracy 8332  0.9399819494584838:  88%|████████▊ | 277/314 [37:08<05:40,  9.21s/it]test:278/314 | accuracy 8363  0.9400854316546763:  88%|████████▊ | 277/314 [37:17<05:40,  9.21s/it]test:278/314 | accuracy 8363  0.9400854316546763:  89%|████████▊ | 278/314 [37:17<05:31,  9.22s/it]test:279/314 | accuracy 8391  0.9398521505376344:  89%|████████▊ | 278/314 [37:26<05:31,  9.22s/it]test:279/314 | accuracy 8391  0.9398521505376344:  89%|████████▉ | 279/314 [37:26<05:18,  9.09s/it]test:280/314 | accuracy 8422  0.9399553571428572:  89%|████████▉ | 279/314 [37:35<05:18,  9.09s/it]test:280/314 | accuracy 8422  0.9399553571428572:  89%|████████▉ | 280/314 [37:36<05:15,  9.29s/it]test:281/314 | accuracy 8452  0.9399466192170819:  89%|████████▉ | 280/314 [37:45<05:15,  9.29s/it]test:281/314 | accuracy 8452  0.9399466192170819:  89%|████████▉ | 281/314 [37:45<05:08,  9.33s/it]test:282/314 | accuracy 8482  0.9399379432624113:  89%|████████▉ | 281/314 [37:54<05:08,  9.33s/it]test:282/314 | accuracy 8482  0.9399379432624113:  90%|████████▉ | 282/314 [37:54<04:57,  9.28s/it]test:283/314 | accuracy 8514  0.9401501766784452:  90%|████████▉ | 282/314 [38:03<04:57,  9.28s/it]test:283/314 | accuracy 8514  0.9401501766784452:  90%|█████████ | 283/314 [38:03<04:46,  9.24s/it]test:284/314 | accuracy 8545  0.9402508802816901:  90%|█████████ | 283/314 [38:12<04:46,  9.24s/it]test:284/314 | accuracy 8545  0.9402508802816901:  90%|█████████ | 284/314 [38:13<04:36,  9.21s/it]test:285/314 | accuracy 8577  0.9404605263157895:  90%|█████████ | 284/314 [38:22<04:36,  9.21s/it]test:285/314 | accuracy 8577  0.9404605263157895:  91%|█████████ | 285/314 [38:22<04:28,  9.24s/it]test:286/314 | accuracy 8607  0.9404501748251748:  91%|█████████ | 285/314 [38:31<04:28,  9.24s/it]test:286/314 | accuracy 8607  0.9404501748251748:  91%|█████████ | 286/314 [38:31<04:18,  9.22s/it]test:287/314 | accuracy 8639  0.9406576655052264:  91%|█████████ | 286/314 [38:40<04:18,  9.22s/it]test:287/314 | accuracy 8639  0.9406576655052264:  91%|█████████▏| 287/314 [38:40<04:09,  9.23s/it]test:288/314 | accuracy 8671  0.9408637152777778:  91%|█████████▏| 287/314 [38:50<04:09,  9.23s/it]test:288/314 | accuracy 8671  0.9408637152777778:  92%|█████████▏| 288/314 [38:50<04:01,  9.30s/it]test:289/314 | accuracy 8702  0.9409602076124568:  92%|█████████▏| 288/314 [38:59<04:01,  9.30s/it]test:289/314 | accuracy 8702  0.9409602076124568:  92%|█████████▏| 289/314 [38:59<03:51,  9.28s/it]test:290/314 | accuracy 8734  0.9411637931034482:  92%|█████████▏| 289/314 [39:08<03:51,  9.28s/it]test:290/314 | accuracy 8734  0.9411637931034482:  92%|█████████▏| 290/314 [39:08<03:43,  9.30s/it]test:291/314 | accuracy 8766  0.9413659793814433:  92%|█████████▏| 290/314 [39:17<03:43,  9.30s/it]test:291/314 | accuracy 8766  0.9413659793814433:  93%|█████████▎| 291/314 [39:17<03:31,  9.19s/it]test:292/314 | accuracy 8798  0.9415667808219178:  93%|█████████▎| 291/314 [39:26<03:31,  9.19s/it]test:292/314 | accuracy 8798  0.9415667808219178:  93%|█████████▎| 292/314 [39:26<03:20,  9.11s/it]test:293/314 | accuracy 8829  0.9416595563139932:  93%|█████████▎| 292/314 [39:36<03:20,  9.11s/it]test:293/314 | accuracy 8829  0.9416595563139932:  93%|█████████▎| 293/314 [39:36<03:14,  9.25s/it]test:294/314 | accuracy 8859  0.9416454081632653:  93%|█████████▎| 293/314 [39:45<03:14,  9.25s/it]test:294/314 | accuracy 8859  0.9416454081632653:  94%|█████████▎| 294/314 [39:45<03:06,  9.34s/it]test:295/314 | accuracy 8889  0.9416313559322034:  94%|█████████▎| 294/314 [39:54<03:06,  9.34s/it]test:295/314 | accuracy 8889  0.9416313559322034:  94%|█████████▍| 295/314 [39:54<02:55,  9.23s/it]test:296/314 | accuracy 8921  0.9418285472972973:  94%|█████████▍| 295/314 [40:04<02:55,  9.23s/it]test:296/314 | accuracy 8921  0.9418285472972973:  94%|█████████▍| 296/314 [40:04<02:47,  9.31s/it]test:297/314 | accuracy 8951  0.9418139730639731:  94%|█████████▍| 296/314 [40:13<02:47,  9.31s/it]test:297/314 | accuracy 8951  0.9418139730639731:  95%|█████████▍| 297/314 [40:13<02:38,  9.35s/it]test:298/314 | accuracy 8978  0.941484899328859:  95%|█████████▍| 297/314 [40:22<02:38,  9.35s/it] test:298/314 | accuracy 8978  0.941484899328859:  95%|█████████▍| 298/314 [40:22<02:28,  9.26s/it]test:299/314 | accuracy 9010  0.941680602006689:  95%|█████████▍| 298/314 [40:31<02:28,  9.26s/it]test:299/314 | accuracy 9010  0.941680602006689:  95%|█████████▌| 299/314 [40:32<02:19,  9.27s/it]test:300/314 | accuracy 9041  0.9417708333333333:  95%|█████████▌| 299/314 [40:41<02:19,  9.27s/it]test:300/314 | accuracy 9041  0.9417708333333333:  96%|█████████▌| 300/314 [40:41<02:09,  9.23s/it]test:301/314 | accuracy 9071  0.9417566445182725:  96%|█████████▌| 300/314 [40:50<02:09,  9.23s/it]test:301/314 | accuracy 9071  0.9417566445182725:  96%|█████████▌| 301/314 [40:50<02:00,  9.24s/it]test:302/314 | accuracy 9101  0.9417425496688742:  96%|█████████▌| 301/314 [40:59<02:00,  9.24s/it]test:302/314 | accuracy 9101  0.9417425496688742:  96%|█████████▌| 302/314 [40:59<01:49,  9.15s/it]test:303/314 | accuracy 9133  0.9419348184818482:  96%|█████████▌| 302/314 [41:08<01:49,  9.15s/it]test:303/314 | accuracy 9133  0.9419348184818482:  96%|█████████▋| 303/314 [41:08<01:40,  9.17s/it]test:304/314 | accuracy 9163  0.9419202302631579:  96%|█████████▋| 303/314 [41:17<01:40,  9.17s/it]test:304/314 | accuracy 9163  0.9419202302631579:  97%|█████████▋| 304/314 [41:17<01:30,  9.07s/it]test:305/314 | accuracy 9194  0.9420081967213115:  97%|█████████▋| 304/314 [41:26<01:30,  9.07s/it]test:305/314 | accuracy 9194  0.9420081967213115:  97%|█████████▋| 305/314 [41:26<01:22,  9.15s/it]test:306/314 | accuracy 9226  0.9421977124183006:  97%|█████████▋| 305/314 [41:35<01:22,  9.15s/it]test:306/314 | accuracy 9226  0.9421977124183006:  97%|█████████▋| 306/314 [41:35<01:12,  9.04s/it]test:307/314 | accuracy 9257  0.9422842019543974:  97%|█████████▋| 306/314 [41:44<01:12,  9.04s/it]test:307/314 | accuracy 9257  0.9422842019543974:  98%|█████████▊| 307/314 [41:44<01:03,  9.12s/it]test:308/314 | accuracy 9288  0.9423701298701299:  98%|█████████▊| 307/314 [41:53<01:03,  9.12s/it]test:308/314 | accuracy 9288  0.9423701298701299:  98%|█████████▊| 308/314 [41:54<00:54,  9.16s/it]test:309/314 | accuracy 9317  0.9422532362459547:  98%|█████████▊| 308/314 [42:03<00:54,  9.16s/it]test:309/314 | accuracy 9317  0.9422532362459547:  98%|█████████▊| 309/314 [42:03<00:45,  9.20s/it]test:310/314 | accuracy 9348  0.9423387096774194:  98%|█████████▊| 309/314 [42:12<00:45,  9.20s/it]test:310/314 | accuracy 9348  0.9423387096774194:  99%|█████████▊| 310/314 [42:12<00:36,  9.20s/it]test:311/314 | accuracy 9377  0.9422226688102894:  99%|█████████▊| 310/314 [42:21<00:36,  9.20s/it]test:311/314 | accuracy 9377  0.9422226688102894:  99%|█████████▉| 311/314 [42:21<00:27,  9.24s/it]test:312/314 | accuracy 9409  0.9424078525641025:  99%|█████████▉| 311/314 [42:31<00:27,  9.24s/it]test:312/314 | accuracy 9409  0.9424078525641025:  99%|█████████▉| 312/314 [42:31<00:18,  9.25s/it]test:313/314 | accuracy 9440  0.9424920127795527:  99%|█████████▉| 312/314 [42:40<00:18,  9.25s/it]test:313/314 | accuracy 9440  0.9424920127795527: 100%|█████████▉| 313/314 [42:40<00:09,  9.22s/it]test:314/314 | accuracy 9466  0.9426409081856204: 100%|█████████▉| 313/314 [42:47<00:09,  9.22s/it]test:314/314 | accuracy 9466  0.9426409081856204: 100%|██████████| 314/314 [42:48<00:00,  8.77s/it]test:314/314 | accuracy 9466  0.9426409081856204: 100%|██████████| 314/314 [42:48<00:00,  8.18s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.91s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.91s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.91s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.91s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.91s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.91s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.91s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.91s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.91s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.90s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.90s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.92s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.96s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.93s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.92s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.92s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.91s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.91s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors #####
  0%|          | 0/40 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/40 | accuracy 27  0.84375:   0%|          | 0/40 [00:04<?, ?it/s]test:1/40 | accuracy 27  0.84375:   2%|▎         | 1/40 [00:04<02:57,  4.54s/it]test:2/40 | accuracy 51  0.796875:   2%|▎         | 1/40 [00:07<02:57,  4.54s/it]test:2/40 | accuracy 51  0.796875:   5%|▌         | 2/40 [00:07<02:22,  3.75s/it]test:3/40 | accuracy 82  0.8541666666666666:   5%|▌         | 2/40 [00:10<02:22,  3.75s/it]test:3/40 | accuracy 82  0.8541666666666666:   8%|▊         | 3/40 [00:10<02:08,  3.48s/it]test:4/40 | accuracy 112  0.875:   8%|▊         | 3/40 [00:14<02:08,  3.48s/it]            test:4/40 | accuracy 112  0.875:  10%|█         | 4/40 [00:14<02:00,  3.35s/it]test:5/40 | accuracy 139  0.86875:  10%|█         | 4/40 [00:17<02:00,  3.35s/it]test:5/40 | accuracy 139  0.86875:  12%|█▎        | 5/40 [00:17<01:55,  3.30s/it]test:6/40 | accuracy 167  0.8697916666666666:  12%|█▎        | 5/40 [00:20<01:55,  3.30s/it]test:6/40 | accuracy 167  0.8697916666666666:  15%|█▌        | 6/40 [00:20<01:49,  3.22s/it]test:7/40 | accuracy 194  0.8660714285714286:  15%|█▌        | 6/40 [00:23<01:49,  3.22s/it]test:7/40 | accuracy 194  0.8660714285714286:  18%|█▊        | 7/40 [00:23<01:44,  3.18s/it]test:8/40 | accuracy 218  0.8515625:  18%|█▊        | 7/40 [00:26<01:44,  3.18s/it]         test:8/40 | accuracy 218  0.8515625:  20%|██        | 8/40 [00:26<01:41,  3.16s/it]test:9/40 | accuracy 244  0.8472222222222222:  20%|██        | 8/40 [00:29<01:41,  3.16s/it]test:9/40 | accuracy 244  0.8472222222222222:  22%|██▎       | 9/40 [00:29<01:38,  3.17s/it]test:10/40 | accuracy 272  0.85:  22%|██▎       | 9/40 [00:32<01:38,  3.17s/it]             test:10/40 | accuracy 272  0.85:  25%|██▌       | 10/40 [00:32<01:34,  3.16s/it]test:11/40 | accuracy 297  0.84375:  25%|██▌       | 10/40 [00:36<01:34,  3.16s/it]test:11/40 | accuracy 297  0.84375:  28%|██▊       | 11/40 [00:36<01:31,  3.17s/it]test:12/40 | accuracy 327  0.8515625:  28%|██▊       | 11/40 [00:39<01:31,  3.17s/it]test:12/40 | accuracy 327  0.8515625:  30%|███       | 12/40 [00:39<01:29,  3.19s/it]test:13/40 | accuracy 356  0.8557692307692307:  30%|███       | 12/40 [00:42<01:29,  3.19s/it]test:13/40 | accuracy 356  0.8557692307692307:  32%|███▎      | 13/40 [00:42<01:26,  3.22s/it]test:14/40 | accuracy 383  0.8549107142857143:  32%|███▎      | 13/40 [00:45<01:26,  3.22s/it]test:14/40 | accuracy 383  0.8549107142857143:  35%|███▌      | 14/40 [00:45<01:23,  3.22s/it]test:15/40 | accuracy 412  0.8583333333333333:  35%|███▌      | 14/40 [00:48<01:23,  3.22s/it]test:15/40 | accuracy 412  0.8583333333333333:  38%|███▊      | 15/40 [00:48<01:20,  3.21s/it]test:16/40 | accuracy 438  0.85546875:  38%|███▊      | 15/40 [00:52<01:20,  3.21s/it]        test:16/40 | accuracy 438  0.85546875:  40%|████      | 16/40 [00:52<01:16,  3.18s/it]test:17/40 | accuracy 462  0.8492647058823529:  40%|████      | 16/40 [00:55<01:16,  3.18s/it]test:17/40 | accuracy 462  0.8492647058823529:  42%|████▎     | 17/40 [00:55<01:13,  3.18s/it]test:18/40 | accuracy 489  0.8489583333333334:  42%|████▎     | 17/40 [00:58<01:13,  3.18s/it]test:18/40 | accuracy 489  0.8489583333333334:  45%|████▌     | 18/40 [00:58<01:09,  3.18s/it]test:19/40 | accuracy 516  0.8486842105263158:  45%|████▌     | 18/40 [01:01<01:09,  3.18s/it]test:19/40 | accuracy 516  0.8486842105263158:  48%|████▊     | 19/40 [01:01<01:07,  3.19s/it]test:20/40 | accuracy 542  0.846875:  48%|████▊     | 19/40 [01:04<01:07,  3.19s/it]          test:20/40 | accuracy 542  0.846875:  50%|█████     | 20/40 [01:04<01:03,  3.20s/it]test:21/40 | accuracy 567  0.84375:  50%|█████     | 20/40 [01:08<01:03,  3.20s/it] test:21/40 | accuracy 567  0.84375:  52%|█████▎    | 21/40 [01:08<01:00,  3.20s/it]test:22/40 | accuracy 596  0.8465909090909091:  52%|█████▎    | 21/40 [01:11<01:00,  3.20s/it]test:22/40 | accuracy 596  0.8465909090909091:  55%|█████▌    | 22/40 [01:11<00:57,  3.22s/it]test:23/40 | accuracy 626  0.8505434782608695:  55%|█████▌    | 22/40 [01:14<00:57,  3.22s/it]test:23/40 | accuracy 626  0.8505434782608695:  57%|█████▊    | 23/40 [01:14<00:54,  3.22s/it]test:24/40 | accuracy 652  0.8489583333333334:  57%|█████▊    | 23/40 [01:17<00:54,  3.22s/it]test:24/40 | accuracy 652  0.8489583333333334:  60%|██████    | 24/40 [01:17<00:51,  3.21s/it]test:25/40 | accuracy 682  0.8525:  60%|██████    | 24/40 [01:20<00:51,  3.21s/it]            test:25/40 | accuracy 682  0.8525:  62%|██████▎   | 25/40 [01:21<00:48,  3.22s/it]test:26/40 | accuracy 710  0.8533653846153846:  62%|██████▎   | 25/40 [01:24<00:48,  3.22s/it]test:26/40 | accuracy 710  0.8533653846153846:  65%|██████▌   | 26/40 [01:24<00:45,  3.27s/it]test:27/40 | accuracy 739  0.8553240740740741:  65%|██████▌   | 26/40 [01:27<00:45,  3.27s/it]test:27/40 | accuracy 739  0.8553240740740741:  68%|██████▊   | 27/40 [01:27<00:41,  3.23s/it]test:28/40 | accuracy 765  0.8537946428571429:  68%|██████▊   | 27/40 [01:30<00:41,  3.23s/it]test:28/40 | accuracy 765  0.8537946428571429:  70%|███████   | 28/40 [01:30<00:38,  3.22s/it]test:29/40 | accuracy 792  0.853448275862069:  70%|███████   | 28/40 [01:33<00:38,  3.22s/it] test:29/40 | accuracy 792  0.853448275862069:  72%|███████▎  | 29/40 [01:33<00:35,  3.22s/it]test:30/40 | accuracy 820  0.8541666666666666:  72%|███████▎  | 29/40 [01:37<00:35,  3.22s/it]test:30/40 | accuracy 820  0.8541666666666666:  75%|███████▌  | 30/40 [01:37<00:32,  3.21s/it]test:31/40 | accuracy 847  0.8538306451612904:  75%|███████▌  | 30/40 [01:40<00:32,  3.21s/it]test:31/40 | accuracy 847  0.8538306451612904:  78%|███████▊  | 31/40 [01:40<00:28,  3.19s/it]test:32/40 | accuracy 872  0.8515625:  78%|███████▊  | 31/40 [01:43<00:28,  3.19s/it]         test:32/40 | accuracy 872  0.8515625:  80%|████████  | 32/40 [01:43<00:25,  3.17s/it]test:33/40 | accuracy 897  0.8494318181818182:  80%|████████  | 32/40 [01:46<00:25,  3.17s/it]test:33/40 | accuracy 897  0.8494318181818182:  82%|████████▎ | 33/40 [01:46<00:22,  3.17s/it]test:34/40 | accuracy 924  0.8492647058823529:  82%|████████▎ | 33/40 [01:49<00:22,  3.17s/it]test:34/40 | accuracy 924  0.8492647058823529:  85%|████████▌ | 34/40 [01:49<00:18,  3.15s/it]test:35/40 | accuracy 949  0.8473214285714286:  85%|████████▌ | 34/40 [01:52<00:18,  3.15s/it]test:35/40 | accuracy 949  0.8473214285714286:  88%|████████▊ | 35/40 [01:53<00:16,  3.20s/it]test:36/40 | accuracy 979  0.8498263888888888:  88%|████████▊ | 35/40 [01:56<00:16,  3.20s/it]test:36/40 | accuracy 979  0.8498263888888888:  90%|█████████ | 36/40 [01:56<00:12,  3.21s/it]test:37/40 | accuracy 1006  0.8496621621621622:  90%|█████████ | 36/40 [01:59<00:12,  3.21s/it]test:37/40 | accuracy 1006  0.8496621621621622:  92%|█████████▎| 37/40 [01:59<00:09,  3.21s/it]test:38/40 | accuracy 1031  0.8478618421052632:  92%|█████████▎| 37/40 [02:02<00:09,  3.21s/it]test:38/40 | accuracy 1031  0.8478618421052632:  95%|█████████▌| 38/40 [02:02<00:06,  3.21s/it]test:39/40 | accuracy 1060  0.8493589743589743:  95%|█████████▌| 38/40 [02:05<00:06,  3.21s/it]test:39/40 | accuracy 1060  0.8493589743589743:  98%|█████████▊| 39/40 [02:05<00:03,  3.22s/it]test:40/40 | accuracy 1075  0.8484609313338595:  98%|█████████▊| 39/40 [02:08<00:03,  3.22s/it]test:40/40 | accuracy 1075  0.8484609313338595: 100%|██████████| 40/40 [02:08<00:00,  2.90s/it]test:40/40 | accuracy 1075  0.8484609313338595: 100%|██████████| 40/40 [02:08<00:00,  3.20s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.91s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.91s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.91s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.91s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.91s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.90s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.89s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.90s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.90s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.89s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.90s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.90s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.91s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.94s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.93s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.92s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.91s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors #####
  0%|          | 0/37 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/37 | accuracy 27  0.84375:   0%|          | 0/37 [00:06<?, ?it/s]test:1/37 | accuracy 27  0.84375:   3%|▎         | 1/37 [00:06<03:41,  6.15s/it]test:2/37 | accuracy 52  0.8125:   3%|▎         | 1/37 [00:11<03:41,  6.15s/it] test:2/37 | accuracy 52  0.8125:   5%|▌         | 2/37 [00:11<03:13,  5.54s/it]test:3/37 | accuracy 78  0.8125:   5%|▌         | 2/37 [00:16<03:13,  5.54s/it]test:3/37 | accuracy 78  0.8125:   8%|▊         | 3/37 [00:16<03:01,  5.34s/it]test:4/37 | accuracy 107  0.8359375:   8%|▊         | 3/37 [00:21<03:01,  5.34s/it]test:4/37 | accuracy 107  0.8359375:  11%|█         | 4/37 [00:21<02:47,  5.09s/it]test:5/37 | accuracy 134  0.8375:  11%|█         | 4/37 [00:25<02:47,  5.09s/it]   test:5/37 | accuracy 134  0.8375:  14%|█▎        | 5/37 [00:25<02:35,  4.86s/it]test:6/37 | accuracy 161  0.8385416666666666:  14%|█▎        | 5/37 [00:31<02:35,  4.86s/it]test:6/37 | accuracy 161  0.8385416666666666:  16%|█▌        | 6/37 [00:31<02:39,  5.16s/it]test:7/37 | accuracy 186  0.8303571428571429:  16%|█▌        | 6/37 [00:35<02:39,  5.16s/it]test:7/37 | accuracy 186  0.8303571428571429:  19%|█▉        | 7/37 [00:35<02:28,  4.95s/it]test:8/37 | accuracy 211  0.82421875:  19%|█▉        | 7/37 [00:43<02:28,  4.95s/it]        test:8/37 | accuracy 211  0.82421875:  22%|██▏       | 8/37 [00:43<02:45,  5.71s/it]test:9/37 | accuracy 239  0.8298611111111112:  22%|██▏       | 8/37 [00:49<02:45,  5.71s/it]test:9/37 | accuracy 239  0.8298611111111112:  24%|██▍       | 9/37 [00:49<02:41,  5.78s/it]test:10/37 | accuracy 264  0.825:  24%|██▍       | 9/37 [00:54<02:41,  5.78s/it]            test:10/37 | accuracy 264  0.825:  27%|██▋       | 10/37 [00:54<02:32,  5.66s/it]test:11/37 | accuracy 289  0.8210227272727273:  27%|██▋       | 10/37 [00:59<02:32,  5.66s/it]test:11/37 | accuracy 289  0.8210227272727273:  30%|██▉       | 11/37 [00:59<02:22,  5.50s/it]test:12/37 | accuracy 316  0.8229166666666666:  30%|██▉       | 11/37 [01:05<02:22,  5.50s/it]test:12/37 | accuracy 316  0.8229166666666666:  32%|███▏      | 12/37 [01:05<02:21,  5.66s/it]test:13/37 | accuracy 343  0.8245192307692307:  32%|███▏      | 12/37 [01:10<02:21,  5.66s/it]test:13/37 | accuracy 343  0.8245192307692307:  35%|███▌      | 13/37 [01:10<02:08,  5.37s/it]test:14/37 | accuracy 370  0.8258928571428571:  35%|███▌      | 13/37 [01:15<02:08,  5.37s/it]test:14/37 | accuracy 370  0.8258928571428571:  38%|███▊      | 14/37 [01:15<02:05,  5.46s/it]test:15/37 | accuracy 398  0.8291666666666667:  38%|███▊      | 14/37 [01:21<02:05,  5.46s/it]test:15/37 | accuracy 398  0.8291666666666667:  41%|████      | 15/37 [01:21<01:59,  5.45s/it]test:16/37 | accuracy 422  0.82421875:  41%|████      | 15/37 [01:26<01:59,  5.45s/it]        test:16/37 | accuracy 422  0.82421875:  43%|████▎     | 16/37 [01:26<01:50,  5.28s/it]test:17/37 | accuracy 450  0.8272058823529411:  43%|████▎     | 16/37 [01:31<01:50,  5.28s/it]test:17/37 | accuracy 450  0.8272058823529411:  46%|████▌     | 17/37 [01:31<01:43,  5.19s/it]test:18/37 | accuracy 479  0.8315972222222222:  46%|████▌     | 17/37 [01:36<01:43,  5.19s/it]test:18/37 | accuracy 479  0.8315972222222222:  49%|████▊     | 18/37 [01:36<01:40,  5.30s/it]test:19/37 | accuracy 511  0.8404605263157895:  49%|████▊     | 18/37 [01:42<01:40,  5.30s/it]test:19/37 | accuracy 511  0.8404605263157895:  51%|█████▏    | 19/37 [01:42<01:38,  5.46s/it]test:20/37 | accuracy 534  0.834375:  51%|█████▏    | 19/37 [01:47<01:38,  5.46s/it]          test:20/37 | accuracy 534  0.834375:  54%|█████▍    | 20/37 [01:47<01:29,  5.25s/it]test:21/37 | accuracy 560  0.8333333333333334:  54%|█████▍    | 20/37 [01:51<01:29,  5.25s/it]test:21/37 | accuracy 560  0.8333333333333334:  57%|█████▋    | 21/37 [01:51<01:20,  5.05s/it]test:22/37 | accuracy 589  0.8366477272727273:  57%|█████▋    | 21/37 [01:56<01:20,  5.05s/it]test:22/37 | accuracy 589  0.8366477272727273:  59%|█████▉    | 22/37 [01:56<01:15,  5.02s/it]test:23/37 | accuracy 614  0.8342391304347826:  59%|█████▉    | 22/37 [02:01<01:15,  5.02s/it]test:23/37 | accuracy 614  0.8342391304347826:  62%|██████▏   | 23/37 [02:01<01:10,  5.03s/it]test:24/37 | accuracy 634  0.8255208333333334:  62%|██████▏   | 23/37 [02:06<01:10,  5.03s/it]test:24/37 | accuracy 634  0.8255208333333334:  65%|██████▍   | 24/37 [02:06<01:04,  4.92s/it]test:25/37 | accuracy 660  0.825:  65%|██████▍   | 24/37 [02:12<01:04,  4.92s/it]             test:25/37 | accuracy 660  0.825:  68%|██████▊   | 25/37 [02:12<01:00,  5.06s/it]test:26/37 | accuracy 684  0.8221153846153846:  68%|██████▊   | 25/37 [02:17<01:00,  5.06s/it]test:26/37 | accuracy 684  0.8221153846153846:  70%|███████   | 26/37 [02:17<00:58,  5.31s/it]test:27/37 | accuracy 708  0.8194444444444444:  70%|███████   | 26/37 [02:24<00:58,  5.31s/it]test:27/37 | accuracy 708  0.8194444444444444:  73%|███████▎  | 27/37 [02:24<00:55,  5.59s/it]test:28/37 | accuracy 732  0.8169642857142857:  73%|███████▎  | 27/37 [02:29<00:55,  5.59s/it]test:28/37 | accuracy 732  0.8169642857142857:  76%|███████▌  | 28/37 [02:29<00:48,  5.40s/it]test:29/37 | accuracy 757  0.8157327586206896:  76%|███████▌  | 28/37 [02:35<00:48,  5.40s/it]test:29/37 | accuracy 757  0.8157327586206896:  78%|███████▊  | 29/37 [02:35<00:44,  5.59s/it]test:30/37 | accuracy 783  0.815625:  78%|███████▊  | 29/37 [02:40<00:44,  5.59s/it]          test:30/37 | accuracy 783  0.815625:  81%|████████  | 30/37 [02:40<00:37,  5.38s/it]test:31/37 | accuracy 810  0.8165322580645161:  81%|████████  | 30/37 [02:44<00:37,  5.38s/it]test:31/37 | accuracy 810  0.8165322580645161:  84%|████████▍ | 31/37 [02:44<00:31,  5.17s/it]test:32/37 | accuracy 836  0.81640625:  84%|████████▍ | 31/37 [02:49<00:31,  5.17s/it]        test:32/37 | accuracy 836  0.81640625:  86%|████████▋ | 32/37 [02:49<00:25,  5.19s/it]test:33/37 | accuracy 863  0.8172348484848485:  86%|████████▋ | 32/37 [02:54<00:25,  5.19s/it]test:33/37 | accuracy 863  0.8172348484848485:  89%|████████▉ | 33/37 [02:54<00:20,  5.08s/it]test:34/37 | accuracy 891  0.8189338235294118:  89%|████████▉ | 33/37 [02:59<00:20,  5.08s/it]test:34/37 | accuracy 891  0.8189338235294118:  92%|█████████▏| 34/37 [02:59<00:15,  5.09s/it]test:35/37 | accuracy 915  0.8169642857142857:  92%|█████████▏| 34/37 [03:04<00:15,  5.09s/it]test:35/37 | accuracy 915  0.8169642857142857:  95%|█████████▍| 35/37 [03:04<00:10,  5.02s/it]test:36/37 | accuracy 938  0.8142361111111112:  95%|█████████▍| 35/37 [03:09<00:10,  5.02s/it]test:36/37 | accuracy 938  0.8142361111111112:  97%|█████████▋| 36/37 [03:09<00:04,  4.87s/it]test:37/37 | accuracy 955  0.8148464163822525:  97%|█████████▋| 36/37 [03:12<00:04,  4.87s/it]test:37/37 | accuracy 955  0.8148464163822525: 100%|██████████| 37/37 [03:12<00:00,  4.30s/it]test:37/37 | accuracy 955  0.8148464163822525: 100%|██████████| 37/37 [03:12<00:00,  5.20s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:35,  1.95s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.93s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.92s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.91s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.91s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.92s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.91s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.91s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.91s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.90s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:21<00:15,  1.91s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:23<00:13,  1.97s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:25<00:11,  1.95s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.92s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.92s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.91s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.92s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors #####
  0%|          | 0/75 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/75 | accuracy 27  0.84375:   0%|          | 0/75 [00:05<?, ?it/s]test:1/75 | accuracy 27  0.84375:   1%|▏         | 1/75 [00:05<06:45,  5.48s/it]test:2/75 | accuracy 57  0.890625:   1%|▏         | 1/75 [00:10<06:45,  5.48s/it]test:2/75 | accuracy 57  0.890625:   3%|▎         | 2/75 [00:10<06:25,  5.29s/it]test:3/75 | accuracy 84  0.875:   3%|▎         | 2/75 [00:15<06:25,  5.29s/it]   test:3/75 | accuracy 84  0.875:   4%|▍         | 3/75 [00:15<06:02,  5.04s/it]test:4/75 | accuracy 114  0.890625:   4%|▍         | 3/75 [00:19<06:02,  5.04s/it]test:4/75 | accuracy 114  0.890625:   5%|▌         | 4/75 [00:19<05:37,  4.75s/it]test:5/75 | accuracy 146  0.9125:   5%|▌         | 4/75 [00:26<05:37,  4.75s/it]  test:5/75 | accuracy 146  0.9125:   7%|▋         | 5/75 [00:26<06:22,  5.46s/it]test:6/75 | accuracy 176  0.9166666666666666:   7%|▋         | 5/75 [00:31<06:22,  5.46s/it]test:6/75 | accuracy 176  0.9166666666666666:   8%|▊         | 6/75 [00:31<06:03,  5.26s/it]test:7/75 | accuracy 208  0.9285714285714286:   8%|▊         | 6/75 [00:36<06:03,  5.26s/it]test:7/75 | accuracy 208  0.9285714285714286:   9%|▉         | 7/75 [00:36<05:50,  5.16s/it]test:8/75 | accuracy 239  0.93359375:   9%|▉         | 7/75 [00:40<05:50,  5.16s/it]        test:8/75 | accuracy 239  0.93359375:  11%|█         | 8/75 [00:40<05:30,  4.94s/it]test:9/75 | accuracy 269  0.9340277777777778:  11%|█         | 8/75 [00:44<05:30,  4.94s/it]test:9/75 | accuracy 269  0.9340277777777778:  12%|█▏        | 9/75 [00:44<05:12,  4.73s/it]test:10/75 | accuracy 298  0.93125:  12%|█▏        | 9/75 [00:50<05:12,  4.73s/it]          test:10/75 | accuracy 298  0.93125:  13%|█▎        | 10/75 [00:50<05:21,  4.94s/it]test:11/75 | accuracy 329  0.9346590909090909:  13%|█▎        | 10/75 [00:54<05:21,  4.94s/it]test:11/75 | accuracy 329  0.9346590909090909:  15%|█▍        | 11/75 [00:54<05:04,  4.77s/it]test:12/75 | accuracy 360  0.9375:  15%|█▍        | 11/75 [00:59<05:04,  4.77s/it]            test:12/75 | accuracy 360  0.9375:  16%|█▌        | 12/75 [00:59<05:05,  4.84s/it]test:13/75 | accuracy 391  0.9399038461538461:  16%|█▌        | 12/75 [01:04<05:05,  4.84s/it]test:13/75 | accuracy 391  0.9399038461538461:  17%|█▋        | 13/75 [01:04<04:53,  4.73s/it]test:14/75 | accuracy 422  0.9419642857142857:  17%|█▋        | 13/75 [01:09<04:53,  4.73s/it]test:14/75 | accuracy 422  0.9419642857142857:  19%|█▊        | 14/75 [01:09<05:03,  4.98s/it]test:15/75 | accuracy 451  0.9395833333333333:  19%|█▊        | 14/75 [01:14<05:03,  4.98s/it]test:15/75 | accuracy 451  0.9395833333333333:  20%|██        | 15/75 [01:14<04:52,  4.87s/it]test:16/75 | accuracy 481  0.939453125:  20%|██        | 15/75 [01:19<04:52,  4.87s/it]       test:16/75 | accuracy 481  0.939453125:  21%|██▏       | 16/75 [01:19<04:44,  4.82s/it]test:17/75 | accuracy 509  0.9356617647058824:  21%|██▏       | 16/75 [01:24<04:44,  4.82s/it]test:17/75 | accuracy 509  0.9356617647058824:  23%|██▎       | 17/75 [01:24<04:50,  5.01s/it]test:18/75 | accuracy 540  0.9375:  23%|██▎       | 17/75 [01:29<04:50,  5.01s/it]            test:18/75 | accuracy 540  0.9375:  24%|██▍       | 18/75 [01:29<04:37,  4.87s/it]test:19/75 | accuracy 572  0.9407894736842105:  24%|██▍       | 18/75 [01:33<04:37,  4.87s/it]test:19/75 | accuracy 572  0.9407894736842105:  25%|██▌       | 19/75 [01:33<04:32,  4.86s/it]test:20/75 | accuracy 602  0.940625:  25%|██▌       | 19/75 [01:38<04:32,  4.86s/it]          test:20/75 | accuracy 602  0.940625:  27%|██▋       | 20/75 [01:38<04:21,  4.75s/it]test:21/75 | accuracy 631  0.9389880952380952:  27%|██▋       | 20/75 [01:43<04:21,  4.75s/it]test:21/75 | accuracy 631  0.9389880952380952:  28%|██▊       | 21/75 [01:43<04:14,  4.72s/it]test:22/75 | accuracy 662  0.9403409090909091:  28%|██▊       | 21/75 [01:47<04:14,  4.72s/it]test:22/75 | accuracy 662  0.9403409090909091:  29%|██▉       | 22/75 [01:47<04:12,  4.76s/it]test:23/75 | accuracy 691  0.938858695652174:  29%|██▉       | 22/75 [01:53<04:12,  4.76s/it] test:23/75 | accuracy 691  0.938858695652174:  31%|███       | 23/75 [01:53<04:20,  5.01s/it]test:24/75 | accuracy 723  0.94140625:  31%|███       | 23/75 [01:58<04:20,  5.01s/it]       test:24/75 | accuracy 723  0.94140625:  32%|███▏      | 24/75 [01:58<04:10,  4.90s/it]test:25/75 | accuracy 752  0.94:  32%|███▏      | 24/75 [02:03<04:10,  4.90s/it]      test:25/75 | accuracy 752  0.94:  33%|███▎      | 25/75 [02:03<04:07,  4.94s/it]test:26/75 | accuracy 783  0.9411057692307693:  33%|███▎      | 25/75 [02:08<04:07,  4.94s/it]test:26/75 | accuracy 783  0.9411057692307693:  35%|███▍      | 26/75 [02:08<04:00,  4.90s/it]test:27/75 | accuracy 810  0.9375:  35%|███▍      | 26/75 [02:12<04:00,  4.90s/it]            test:27/75 | accuracy 810  0.9375:  36%|███▌      | 27/75 [02:12<03:55,  4.90s/it]test:28/75 | accuracy 840  0.9375:  36%|███▌      | 27/75 [02:17<03:55,  4.90s/it]test:28/75 | accuracy 840  0.9375:  37%|███▋      | 28/75 [02:18<03:52,  4.95s/it]test:29/75 | accuracy 870  0.9375:  37%|███▋      | 28/75 [02:22<03:52,  4.95s/it]test:29/75 | accuracy 870  0.9375:  39%|███▊      | 29/75 [02:22<03:42,  4.84s/it]test:30/75 | accuracy 897  0.934375:  39%|███▊      | 29/75 [02:27<03:42,  4.84s/it]test:30/75 | accuracy 897  0.934375:  40%|████      | 30/75 [02:27<03:36,  4.81s/it]test:31/75 | accuracy 929  0.936491935483871:  40%|████      | 30/75 [02:32<03:36,  4.81s/it]test:31/75 | accuracy 929  0.936491935483871:  41%|████▏     | 31/75 [02:32<03:35,  4.89s/it]test:32/75 | accuracy 960  0.9375:  41%|████▏     | 31/75 [02:38<03:35,  4.89s/it]           test:32/75 | accuracy 960  0.9375:  43%|████▎     | 32/75 [02:38<03:49,  5.33s/it]test:33/75 | accuracy 990  0.9375:  43%|████▎     | 32/75 [02:43<03:49,  5.33s/it]test:33/75 | accuracy 990  0.9375:  44%|████▍     | 33/75 [02:43<03:37,  5.18s/it]test:34/75 | accuracy 1020  0.9375:  44%|████▍     | 33/75 [02:49<03:37,  5.18s/it]test:34/75 | accuracy 1020  0.9375:  45%|████▌     | 34/75 [02:49<03:36,  5.27s/it]test:35/75 | accuracy 1048  0.9357142857142857:  45%|████▌     | 34/75 [02:53<03:36,  5.27s/it]test:35/75 | accuracy 1048  0.9357142857142857:  47%|████▋     | 35/75 [02:53<03:21,  5.03s/it]test:36/75 | accuracy 1078  0.9357638888888888:  47%|████▋     | 35/75 [02:58<03:21,  5.03s/it]test:36/75 | accuracy 1078  0.9357638888888888:  48%|████▊     | 36/75 [02:58<03:10,  4.88s/it]test:37/75 | accuracy 1108  0.9358108108108109:  48%|████▊     | 36/75 [03:02<03:10,  4.88s/it]test:37/75 | accuracy 1108  0.9358108108108109:  49%|████▉     | 37/75 [03:02<03:05,  4.88s/it]test:38/75 | accuracy 1137  0.9350328947368421:  49%|████▉     | 37/75 [03:07<03:05,  4.88s/it]test:38/75 | accuracy 1137  0.9350328947368421:  51%|█████     | 38/75 [03:07<02:57,  4.79s/it]test:39/75 | accuracy 1165  0.9334935897435898:  51%|█████     | 38/75 [03:12<02:57,  4.79s/it]test:39/75 | accuracy 1165  0.9334935897435898:  52%|█████▏    | 39/75 [03:12<02:54,  4.84s/it]test:40/75 | accuracy 1193  0.93203125:  52%|█████▏    | 39/75 [03:18<02:54,  4.84s/it]        test:40/75 | accuracy 1193  0.93203125:  53%|█████▎    | 40/75 [03:18<02:59,  5.12s/it]test:41/75 | accuracy 1221  0.930640243902439:  53%|█████▎    | 40/75 [03:24<02:59,  5.12s/it]test:41/75 | accuracy 1221  0.930640243902439:  55%|█████▍    | 41/75 [03:24<03:01,  5.33s/it]test:42/75 | accuracy 1252  0.9315476190476191:  55%|█████▍    | 41/75 [03:28<03:01,  5.33s/it]test:42/75 | accuracy 1252  0.9315476190476191:  56%|█████▌    | 42/75 [03:28<02:46,  5.04s/it]test:43/75 | accuracy 1281  0.9309593023255814:  56%|█████▌    | 42/75 [03:33<02:46,  5.04s/it]test:43/75 | accuracy 1281  0.9309593023255814:  57%|█████▋    | 43/75 [03:33<02:45,  5.19s/it]test:44/75 | accuracy 1309  0.9296875:  57%|█████▋    | 43/75 [03:38<02:45,  5.19s/it]         test:44/75 | accuracy 1309  0.9296875:  59%|█████▊    | 44/75 [03:38<02:39,  5.14s/it]test:45/75 | accuracy 1336  0.9277777777777778:  59%|█████▊    | 44/75 [03:43<02:39,  5.14s/it]test:45/75 | accuracy 1336  0.9277777777777778:  60%|██████    | 45/75 [03:43<02:30,  5.00s/it]test:46/75 | accuracy 1368  0.9293478260869565:  60%|██████    | 45/75 [03:48<02:30,  5.00s/it]test:46/75 | accuracy 1368  0.9293478260869565:  61%|██████▏   | 46/75 [03:48<02:19,  4.82s/it]test:47/75 | accuracy 1397  0.9288563829787234:  61%|██████▏   | 46/75 [03:52<02:19,  4.82s/it]test:47/75 | accuracy 1397  0.9288563829787234:  63%|██████▎   | 47/75 [03:52<02:14,  4.79s/it]test:48/75 | accuracy 1426  0.9283854166666666:  63%|██████▎   | 47/75 [03:57<02:14,  4.79s/it]test:48/75 | accuracy 1426  0.9283854166666666:  64%|██████▍   | 48/75 [03:57<02:10,  4.83s/it]test:49/75 | accuracy 1453  0.9266581632653061:  64%|██████▍   | 48/75 [04:02<02:10,  4.83s/it]test:49/75 | accuracy 1453  0.9266581632653061:  65%|██████▌   | 49/75 [04:02<02:01,  4.67s/it]test:50/75 | accuracy 1483  0.926875:  65%|██████▌   | 49/75 [04:06<02:01,  4.67s/it]          test:50/75 | accuracy 1483  0.926875:  67%|██████▋   | 50/75 [04:06<01:55,  4.63s/it]test:51/75 | accuracy 1511  0.9258578431372549:  67%|██████▋   | 50/75 [04:11<01:55,  4.63s/it]test:51/75 | accuracy 1511  0.9258578431372549:  68%|██████▊   | 51/75 [04:11<01:52,  4.70s/it]test:52/75 | accuracy 1541  0.9260817307692307:  68%|██████▊   | 51/75 [04:16<01:52,  4.70s/it]test:52/75 | accuracy 1541  0.9260817307692307:  69%|██████▉   | 52/75 [04:16<01:52,  4.87s/it]test:53/75 | accuracy 1570  0.9257075471698113:  69%|██████▉   | 52/75 [04:21<01:52,  4.87s/it]test:53/75 | accuracy 1570  0.9257075471698113:  71%|███████   | 53/75 [04:21<01:44,  4.75s/it]test:54/75 | accuracy 1599  0.9253472222222222:  71%|███████   | 53/75 [04:25<01:44,  4.75s/it]test:54/75 | accuracy 1599  0.9253472222222222:  72%|███████▏  | 54/75 [04:25<01:38,  4.70s/it]test:55/75 | accuracy 1629  0.9255681818181818:  72%|███████▏  | 54/75 [04:30<01:38,  4.70s/it]test:55/75 | accuracy 1629  0.9255681818181818:  73%|███████▎  | 55/75 [04:30<01:36,  4.83s/it]test:56/75 | accuracy 1661  0.9268973214285714:  73%|███████▎  | 55/75 [04:37<01:36,  4.83s/it]test:56/75 | accuracy 1661  0.9268973214285714:  75%|███████▍  | 56/75 [04:37<01:41,  5.35s/it]test:57/75 | accuracy 1692  0.9276315789473685:  75%|███████▍  | 56/75 [04:42<01:41,  5.35s/it]test:57/75 | accuracy 1692  0.9276315789473685:  76%|███████▌  | 57/75 [04:42<01:36,  5.35s/it]test:58/75 | accuracy 1723  0.9283405172413793:  76%|███████▌  | 57/75 [04:46<01:36,  5.35s/it]test:58/75 | accuracy 1723  0.9283405172413793:  77%|███████▋  | 58/75 [04:46<01:23,  4.90s/it]test:59/75 | accuracy 1754  0.9290254237288136:  77%|███████▋  | 58/75 [04:51<01:23,  4.90s/it]test:59/75 | accuracy 1754  0.9290254237288136:  79%|███████▊  | 59/75 [04:51<01:19,  4.98s/it]test:60/75 | accuracy 1782  0.928125:  79%|███████▊  | 59/75 [04:56<01:19,  4.98s/it]          test:60/75 | accuracy 1782  0.928125:  80%|████████  | 60/75 [04:56<01:13,  4.90s/it]test:61/75 | accuracy 1810  0.9272540983606558:  80%|████████  | 60/75 [05:00<01:13,  4.90s/it]test:61/75 | accuracy 1810  0.9272540983606558:  81%|████████▏ | 61/75 [05:00<01:06,  4.74s/it]test:62/75 | accuracy 1841  0.9279233870967742:  81%|████████▏ | 61/75 [05:05<01:06,  4.74s/it]test:62/75 | accuracy 1841  0.9279233870967742:  83%|████████▎ | 62/75 [05:05<00:59,  4.56s/it]test:63/75 | accuracy 1870  0.9275793650793651:  83%|████████▎ | 62/75 [05:09<00:59,  4.56s/it]test:63/75 | accuracy 1870  0.9275793650793651:  84%|████████▍ | 63/75 [05:09<00:54,  4.58s/it]test:64/75 | accuracy 1898  0.9267578125:  84%|████████▍ | 63/75 [05:14<00:54,  4.58s/it]      test:64/75 | accuracy 1898  0.9267578125:  85%|████████▌ | 64/75 [05:14<00:51,  4.67s/it]test:65/75 | accuracy 1929  0.9274038461538462:  85%|████████▌ | 64/75 [05:19<00:51,  4.67s/it]test:65/75 | accuracy 1929  0.9274038461538462:  87%|████████▋ | 65/75 [05:19<00:47,  4.71s/it]test:66/75 | accuracy 1959  0.9275568181818182:  87%|████████▋ | 65/75 [05:23<00:47,  4.71s/it]test:66/75 | accuracy 1959  0.9275568181818182:  88%|████████▊ | 66/75 [05:23<00:41,  4.56s/it]test:67/75 | accuracy 1991  0.9286380597014925:  88%|████████▊ | 66/75 [05:27<00:41,  4.56s/it]test:67/75 | accuracy 1991  0.9286380597014925:  89%|████████▉ | 67/75 [05:27<00:35,  4.48s/it]test:68/75 | accuracy 2020  0.9283088235294118:  89%|████████▉ | 67/75 [05:32<00:35,  4.48s/it]test:68/75 | accuracy 2020  0.9283088235294118:  91%|█████████ | 68/75 [05:32<00:30,  4.41s/it]test:69/75 | accuracy 2049  0.9279891304347826:  91%|█████████ | 68/75 [05:36<00:30,  4.41s/it]test:69/75 | accuracy 2049  0.9279891304347826:  92%|█████████▏| 69/75 [05:36<00:26,  4.49s/it]test:70/75 | accuracy 2079  0.928125:  92%|█████████▏| 69/75 [05:41<00:26,  4.49s/it]          test:70/75 | accuracy 2079  0.928125:  93%|█████████▎| 70/75 [05:41<00:22,  4.56s/it]test:71/75 | accuracy 2108  0.9278169014084507:  93%|█████████▎| 70/75 [05:46<00:22,  4.56s/it]test:71/75 | accuracy 2108  0.9278169014084507:  95%|█████████▍| 71/75 [05:46<00:18,  4.73s/it]test:72/75 | accuracy 2136  0.9270833333333334:  95%|█████████▍| 71/75 [05:51<00:18,  4.73s/it]test:72/75 | accuracy 2136  0.9270833333333334:  96%|█████████▌| 72/75 [05:51<00:14,  4.69s/it]test:73/75 | accuracy 2166  0.9272260273972602:  96%|█████████▌| 72/75 [05:55<00:14,  4.69s/it]test:73/75 | accuracy 2166  0.9272260273972602:  97%|█████████▋| 73/75 [05:55<00:09,  4.61s/it]test:74/75 | accuracy 2198  0.9282094594594594:  97%|█████████▋| 73/75 [06:00<00:09,  4.61s/it]test:74/75 | accuracy 2198  0.9282094594594594:  99%|█████████▊| 74/75 [06:00<00:04,  4.75s/it]test:75/75 | accuracy 2206  0.9284511784511784:  99%|█████████▊| 74/75 [06:02<00:04,  4.75s/it]test:75/75 | accuracy 2206  0.9284511784511784: 100%|██████████| 75/75 [06:02<00:00,  3.74s/it]test:75/75 | accuracy 2206  0.9284511784511784: 100%|██████████| 75/75 [06:02<00:00,  4.83s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.91s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.92s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.91s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.91s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.91s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.91s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.89s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.90s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:18,  1.90s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.90s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.90s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.90s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.90s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.89s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.91s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.95s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.93s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.89s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.1/model.safetensors #####
  0%|          | 0/16 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/16 | accuracy 29  0.90625:   0%|          | 0/16 [00:04<?, ?it/s]test:1/16 | accuracy 29  0.90625:   6%|▋         | 1/16 [00:04<01:14,  4.99s/it]test:2/16 | accuracy 56  0.875:   6%|▋         | 1/16 [00:09<01:14,  4.99s/it]  test:2/16 | accuracy 56  0.875:  12%|█▎        | 2/16 [00:09<01:05,  4.70s/it]test:3/16 | accuracy 84  0.875:  12%|█▎        | 2/16 [00:12<01:05,  4.70s/it]test:3/16 | accuracy 84  0.875:  19%|█▉        | 3/16 [00:12<00:53,  4.14s/it]test:4/16 | accuracy 112  0.875:  19%|█▉        | 3/16 [00:16<00:53,  4.14s/it]test:4/16 | accuracy 112  0.875:  25%|██▌       | 4/16 [00:16<00:49,  4.10s/it]test:5/16 | accuracy 138  0.8625:  25%|██▌       | 4/16 [00:21<00:49,  4.10s/it]test:5/16 | accuracy 138  0.8625:  31%|███▏      | 5/16 [00:21<00:44,  4.07s/it]test:6/16 | accuracy 167  0.8697916666666666:  31%|███▏      | 5/16 [00:25<00:44,  4.07s/it]test:6/16 | accuracy 167  0.8697916666666666:  38%|███▊      | 6/16 [00:25<00:40,  4.05s/it]test:7/16 | accuracy 195  0.8705357142857143:  38%|███▊      | 6/16 [00:28<00:40,  4.05s/it]test:7/16 | accuracy 195  0.8705357142857143:  44%|████▍     | 7/16 [00:28<00:35,  3.99s/it]test:8/16 | accuracy 220  0.859375:  44%|████▍     | 7/16 [00:32<00:35,  3.99s/it]          test:8/16 | accuracy 220  0.859375:  50%|█████     | 8/16 [00:32<00:31,  3.92s/it]test:9/16 | accuracy 248  0.8611111111111112:  50%|█████     | 8/16 [00:36<00:31,  3.92s/it]test:9/16 | accuracy 248  0.8611111111111112:  56%|█████▋    | 9/16 [00:36<00:26,  3.82s/it]test:10/16 | accuracy 277  0.865625:  56%|█████▋    | 9/16 [00:39<00:26,  3.82s/it]         test:10/16 | accuracy 277  0.865625:  62%|██████▎   | 10/16 [00:39<00:22,  3.73s/it]test:11/16 | accuracy 307  0.8721590909090909:  62%|██████▎   | 10/16 [00:43<00:22,  3.73s/it]test:11/16 | accuracy 307  0.8721590909090909:  69%|██████▉   | 11/16 [00:43<00:18,  3.70s/it]test:12/16 | accuracy 332  0.8645833333333334:  69%|██████▉   | 11/16 [00:47<00:18,  3.70s/it]test:12/16 | accuracy 332  0.8645833333333334:  75%|███████▌  | 12/16 [00:47<00:15,  3.76s/it]test:13/16 | accuracy 364  0.875:  75%|███████▌  | 12/16 [00:50<00:15,  3.76s/it]             test:13/16 | accuracy 364  0.875:  81%|████████▏ | 13/16 [00:50<00:11,  3.69s/it]test:14/16 | accuracy 392  0.875:  81%|████████▏ | 13/16 [00:54<00:11,  3.69s/it]test:14/16 | accuracy 392  0.875:  88%|████████▊ | 14/16 [00:54<00:07,  3.75s/it]test:15/16 | accuracy 422  0.8791666666666667:  88%|████████▊ | 14/16 [00:58<00:07,  3.75s/it]test:15/16 | accuracy 422  0.8791666666666667:  94%|█████████▍| 15/16 [00:58<00:03,  3.73s/it]test:16/16 | accuracy 438  0.876:  94%|█████████▍| 15/16 [01:01<00:03,  3.73s/it]             test:16/16 | accuracy 438  0.876: 100%|██████████| 16/16 [01:01<00:00,  3.47s/it]test:16/16 | accuracy 438  0.876: 100%|██████████| 16/16 [01:01<00:00,  3.83s/it]


test finished

============================= JOB FEEDBACK =============================

Job ID: 2629161
Cluster: hk
User/Group: hgf_mxv5488/hk-project-p0022189
Account: hk-project-p0022189
State: COMPLETED (exit code 0)
Partition: accelerated
Nodes: 1
Cores per node: 152
Nodelist: hkn0819
CPU Utilized: 01:14:05
CPU Efficiency: 0.67% of 7-17:18:48 core-walltime
Job Wall-clock time: 01:13:09
Starttime: Sun Sep  8 07:53:58 2024
Endtime: Sun Sep  8 09:07:07 2024
Memory Utilized: 7.50 GB
Memory Efficiency: 0.00% of 0.00 MB
Energy Consumed: 3512231 Joule / 975.619722222222 Watthours
Average node power draw: 800.234905445432 Watt
