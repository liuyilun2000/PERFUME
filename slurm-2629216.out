MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_64.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 128,
    "lora_dropout": 0.0,
    "r": 64
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:05<01:30,  5.05s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:10<01:25,  5.02s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:15<01:20,  5.02s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:20<01:14,  5.00s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:25<01:10,  5.02s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:30<01:05,  5.02s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:35<00:59,  4.98s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:40<00:54,  5.00s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:45<00:49,  5.00s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:50<00:44,  4.99s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:55<00:39,  5.00s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [01:00<00:35,  5.01s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [01:05<00:30,  5.02s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [01:10<00:24,  5.00s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [01:15<00:20,  5.01s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [01:20<00:15,  5.04s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [01:25<00:10,  5.01s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [01:30<00:05,  5.01s/it]Loading checkpoint shards: 100%|██████████| 19/19 [01:34<00:00,  4.81s/it]Loading checkpoint shards: 100%|██████████| 19/19 [01:34<00:00,  4.97s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_64.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors #####
  0%|          | 0/103 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/103 | accuracy 22  0.6875:   0%|          | 0/103 [00:04<?, ?it/s]test:1/103 | accuracy 22  0.6875:   1%|          | 1/103 [00:04<08:15,  4.85s/it]test:2/103 | accuracy 46  0.71875:   1%|          | 1/103 [00:07<08:15,  4.85s/it]test:2/103 | accuracy 46  0.71875:   2%|▏         | 2/103 [00:07<05:57,  3.54s/it]test:3/103 | accuracy 65  0.6770833333333334:   2%|▏         | 2/103 [00:10<05:57,  3.54s/it]test:3/103 | accuracy 65  0.6770833333333334:   3%|▎         | 3/103 [00:10<05:13,  3.13s/it]test:4/103 | accuracy 88  0.6875:   3%|▎         | 3/103 [00:12<05:13,  3.13s/it]            test:4/103 | accuracy 88  0.6875:   4%|▍         | 4/103 [00:12<04:47,  2.90s/it]test:5/103 | accuracy 107  0.66875:   4%|▍         | 4/103 [00:15<04:47,  2.90s/it]test:5/103 | accuracy 107  0.66875:   5%|▍         | 5/103 [00:15<04:34,  2.80s/it]test:6/103 | accuracy 133  0.6927083333333334:   5%|▍         | 5/103 [00:17<04:34,  2.80s/it]test:6/103 | accuracy 133  0.6927083333333334:   6%|▌         | 6/103 [00:17<04:27,  2.75s/it]test:7/103 | accuracy 159  0.7098214285714286:   6%|▌         | 6/103 [00:20<04:27,  2.75s/it]test:7/103 | accuracy 159  0.7098214285714286:   7%|▋         | 7/103 [00:20<04:22,  2.74s/it]test:8/103 | accuracy 183  0.71484375:   7%|▋         | 7/103 [00:23<04:22,  2.74s/it]        test:8/103 | accuracy 183  0.71484375:   8%|▊         | 8/103 [00:23<04:15,  2.69s/it]test:9/103 | accuracy 209  0.7256944444444444:   8%|▊         | 8/103 [00:25<04:15,  2.69s/it]test:9/103 | accuracy 209  0.7256944444444444:   9%|▊         | 9/103 [00:25<04:09,  2.65s/it]test:10/103 | accuracy 235  0.734375:   9%|▊         | 9/103 [00:28<04:09,  2.65s/it]         test:10/103 | accuracy 235  0.734375:  10%|▉         | 10/103 [00:28<04:05,  2.64s/it]test:11/103 | accuracy 261  0.7414772727272727:  10%|▉         | 10/103 [00:31<04:05,  2.64s/it]test:11/103 | accuracy 261  0.7414772727272727:  11%|█         | 11/103 [00:31<04:03,  2.65s/it]test:12/103 | accuracy 287  0.7473958333333334:  11%|█         | 11/103 [00:33<04:03,  2.65s/it]test:12/103 | accuracy 287  0.7473958333333334:  12%|█▏        | 12/103 [00:33<04:00,  2.65s/it]test:13/103 | accuracy 309  0.7427884615384616:  12%|█▏        | 12/103 [00:36<04:00,  2.65s/it]test:13/103 | accuracy 309  0.7427884615384616:  13%|█▎        | 13/103 [00:36<03:58,  2.65s/it]test:14/103 | accuracy 329  0.734375:  13%|█▎        | 13/103 [00:39<03:58,  2.65s/it]          test:14/103 | accuracy 329  0.734375:  14%|█▎        | 14/103 [00:39<03:56,  2.65s/it]test:15/103 | accuracy 354  0.7375:  14%|█▎        | 14/103 [00:41<03:56,  2.65s/it]  test:15/103 | accuracy 354  0.7375:  15%|█▍        | 15/103 [00:41<03:55,  2.67s/it]test:16/103 | accuracy 379  0.740234375:  15%|█▍        | 15/103 [00:44<03:55,  2.67s/it]test:16/103 | accuracy 379  0.740234375:  16%|█▌        | 16/103 [00:44<03:50,  2.65s/it]test:17/103 | accuracy 403  0.7408088235294118:  16%|█▌        | 16/103 [00:47<03:50,  2.65s/it]test:17/103 | accuracy 403  0.7408088235294118:  17%|█▋        | 17/103 [00:47<03:48,  2.65s/it]test:18/103 | accuracy 425  0.7378472222222222:  17%|█▋        | 17/103 [00:49<03:48,  2.65s/it]test:18/103 | accuracy 425  0.7378472222222222:  17%|█▋        | 18/103 [00:49<03:41,  2.60s/it]test:19/103 | accuracy 444  0.7302631578947368:  17%|█▋        | 18/103 [00:52<03:41,  2.60s/it]test:19/103 | accuracy 444  0.7302631578947368:  18%|█▊        | 19/103 [00:52<03:40,  2.62s/it]test:20/103 | accuracy 470  0.734375:  18%|█▊        | 19/103 [00:54<03:40,  2.62s/it]          test:20/103 | accuracy 470  0.734375:  19%|█▉        | 20/103 [00:54<03:35,  2.59s/it]test:21/103 | accuracy 496  0.7380952380952381:  19%|█▉        | 20/103 [00:57<03:35,  2.59s/it]test:21/103 | accuracy 496  0.7380952380952381:  20%|██        | 21/103 [00:57<03:33,  2.61s/it]test:22/103 | accuracy 523  0.7428977272727273:  20%|██        | 21/103 [00:59<03:33,  2.61s/it]test:22/103 | accuracy 523  0.7428977272727273:  21%|██▏       | 22/103 [00:59<03:31,  2.61s/it]test:23/103 | accuracy 545  0.7404891304347826:  21%|██▏       | 22/103 [01:02<03:31,  2.61s/it]test:23/103 | accuracy 545  0.7404891304347826:  22%|██▏       | 23/103 [01:02<03:27,  2.59s/it]test:24/103 | accuracy 564  0.734375:  22%|██▏       | 23/103 [01:05<03:27,  2.59s/it]          test:24/103 | accuracy 564  0.734375:  23%|██▎       | 24/103 [01:05<03:24,  2.59s/it]test:25/103 | accuracy 586  0.7325:  23%|██▎       | 24/103 [01:07<03:24,  2.59s/it]  test:25/103 | accuracy 586  0.7325:  24%|██▍       | 25/103 [01:07<03:23,  2.61s/it]test:26/103 | accuracy 613  0.7367788461538461:  24%|██▍       | 25/103 [01:10<03:23,  2.61s/it]test:26/103 | accuracy 613  0.7367788461538461:  25%|██▌       | 26/103 [01:10<03:21,  2.61s/it]test:27/103 | accuracy 634  0.7337962962962963:  25%|██▌       | 26/103 [01:12<03:21,  2.61s/it]test:27/103 | accuracy 634  0.7337962962962963:  26%|██▌       | 27/103 [01:12<03:18,  2.61s/it]test:28/103 | accuracy 659  0.7354910714285714:  26%|██▌       | 27/103 [01:15<03:18,  2.61s/it]test:28/103 | accuracy 659  0.7354910714285714:  27%|██▋       | 28/103 [01:15<03:16,  2.62s/it]test:29/103 | accuracy 681  0.7338362068965517:  27%|██▋       | 28/103 [01:18<03:16,  2.62s/it]test:29/103 | accuracy 681  0.7338362068965517:  28%|██▊       | 29/103 [01:18<03:14,  2.63s/it]test:30/103 | accuracy 703  0.7322916666666667:  28%|██▊       | 29/103 [01:20<03:14,  2.63s/it]test:30/103 | accuracy 703  0.7322916666666667:  29%|██▉       | 30/103 [01:20<03:12,  2.63s/it]test:31/103 | accuracy 722  0.7278225806451613:  29%|██▉       | 30/103 [01:23<03:12,  2.63s/it]test:31/103 | accuracy 722  0.7278225806451613:  30%|███       | 31/103 [01:23<03:08,  2.62s/it]test:32/103 | accuracy 746  0.728515625:  30%|███       | 31/103 [01:26<03:08,  2.62s/it]       test:32/103 | accuracy 746  0.728515625:  31%|███       | 32/103 [01:26<03:04,  2.60s/it]test:33/103 | accuracy 768  0.7272727272727273:  31%|███       | 32/103 [01:28<03:04,  2.60s/it]test:33/103 | accuracy 768  0.7272727272727273:  32%|███▏      | 33/103 [01:28<03:01,  2.60s/it]test:34/103 | accuracy 797  0.7325367647058824:  32%|███▏      | 33/103 [01:31<03:01,  2.60s/it]test:34/103 | accuracy 797  0.7325367647058824:  33%|███▎      | 34/103 [01:31<02:58,  2.59s/it]test:35/103 | accuracy 820  0.7321428571428571:  33%|███▎      | 34/103 [01:33<02:58,  2.59s/it]test:35/103 | accuracy 820  0.7321428571428571:  34%|███▍      | 35/103 [01:33<02:56,  2.60s/it]test:36/103 | accuracy 842  0.7309027777777778:  34%|███▍      | 35/103 [01:36<02:56,  2.60s/it]test:36/103 | accuracy 842  0.7309027777777778:  35%|███▍      | 36/103 [01:36<02:53,  2.59s/it]test:37/103 | accuracy 868  0.7331081081081081:  35%|███▍      | 36/103 [01:39<02:53,  2.59s/it]test:37/103 | accuracy 868  0.7331081081081081:  36%|███▌      | 37/103 [01:39<02:52,  2.61s/it]test:38/103 | accuracy 892  0.7335526315789473:  36%|███▌      | 37/103 [01:41<02:52,  2.61s/it]test:38/103 | accuracy 892  0.7335526315789473:  37%|███▋      | 38/103 [01:41<02:50,  2.62s/it]test:39/103 | accuracy 910  0.7291666666666666:  37%|███▋      | 38/103 [01:44<02:50,  2.62s/it]test:39/103 | accuracy 910  0.7291666666666666:  38%|███▊      | 39/103 [01:44<02:46,  2.61s/it]test:40/103 | accuracy 930  0.7265625:  38%|███▊      | 39/103 [01:47<02:46,  2.61s/it]         test:40/103 | accuracy 930  0.7265625:  39%|███▉      | 40/103 [01:47<02:46,  2.64s/it]test:41/103 | accuracy 957  0.729420731707317:  39%|███▉      | 40/103 [01:49<02:46,  2.64s/it]test:41/103 | accuracy 957  0.729420731707317:  40%|███▉      | 41/103 [01:49<02:43,  2.63s/it]test:42/103 | accuracy 982  0.7306547619047619:  40%|███▉      | 41/103 [01:52<02:43,  2.63s/it]test:42/103 | accuracy 982  0.7306547619047619:  41%|████      | 42/103 [01:52<02:41,  2.64s/it]test:43/103 | accuracy 1008  0.7325581395348837:  41%|████      | 42/103 [01:54<02:41,  2.64s/it]test:43/103 | accuracy 1008  0.7325581395348837:  42%|████▏     | 43/103 [01:54<02:38,  2.63s/it]test:44/103 | accuracy 1031  0.7322443181818182:  42%|████▏     | 43/103 [01:57<02:38,  2.63s/it]test:44/103 | accuracy 1031  0.7322443181818182:  43%|████▎     | 44/103 [01:57<02:35,  2.63s/it]test:45/103 | accuracy 1054  0.7319444444444444:  43%|████▎     | 44/103 [02:00<02:35,  2.63s/it]test:45/103 | accuracy 1054  0.7319444444444444:  44%|████▎     | 45/103 [02:00<02:32,  2.62s/it]test:46/103 | accuracy 1081  0.734375:  44%|████▎     | 45/103 [02:02<02:32,  2.62s/it]          test:46/103 | accuracy 1081  0.734375:  45%|████▍     | 46/103 [02:02<02:30,  2.64s/it]test:47/103 | accuracy 1104  0.7340425531914894:  45%|████▍     | 46/103 [02:05<02:30,  2.64s/it]test:47/103 | accuracy 1104  0.7340425531914894:  46%|████▌     | 47/103 [02:05<02:30,  2.69s/it]test:48/103 | accuracy 1129  0.7350260416666666:  46%|████▌     | 47/103 [02:08<02:30,  2.69s/it]test:48/103 | accuracy 1129  0.7350260416666666:  47%|████▋     | 48/103 [02:08<02:26,  2.67s/it]test:49/103 | accuracy 1151  0.7340561224489796:  47%|████▋     | 48/103 [02:10<02:26,  2.67s/it]test:49/103 | accuracy 1151  0.7340561224489796:  48%|████▊     | 49/103 [02:10<02:22,  2.64s/it]test:50/103 | accuracy 1173  0.733125:  48%|████▊     | 49/103 [02:13<02:22,  2.64s/it]          test:50/103 | accuracy 1173  0.733125:  49%|████▊     | 50/103 [02:13<02:19,  2.64s/it]test:51/103 | accuracy 1195  0.7322303921568627:  49%|████▊     | 50/103 [02:16<02:19,  2.64s/it]test:51/103 | accuracy 1195  0.7322303921568627:  50%|████▉     | 51/103 [02:16<02:17,  2.64s/it]test:52/103 | accuracy 1219  0.7325721153846154:  50%|████▉     | 51/103 [02:18<02:17,  2.64s/it]test:52/103 | accuracy 1219  0.7325721153846154:  50%|█████     | 52/103 [02:18<02:14,  2.63s/it]test:53/103 | accuracy 1238  0.7299528301886793:  50%|█████     | 52/103 [02:21<02:14,  2.63s/it]test:53/103 | accuracy 1238  0.7299528301886793:  51%|█████▏    | 53/103 [02:21<02:11,  2.63s/it]test:54/103 | accuracy 1257  0.7274305555555556:  51%|█████▏    | 53/103 [02:23<02:11,  2.63s/it]test:54/103 | accuracy 1257  0.7274305555555556:  52%|█████▏    | 54/103 [02:24<02:09,  2.64s/it]test:55/103 | accuracy 1282  0.7284090909090909:  52%|█████▏    | 54/103 [02:26<02:09,  2.64s/it]test:55/103 | accuracy 1282  0.7284090909090909:  53%|█████▎    | 55/103 [02:26<02:06,  2.64s/it]test:56/103 | accuracy 1308  0.7299107142857143:  53%|█████▎    | 55/103 [02:29<02:06,  2.64s/it]test:56/103 | accuracy 1308  0.7299107142857143:  54%|█████▍    | 56/103 [02:29<02:04,  2.64s/it]test:57/103 | accuracy 1334  0.731359649122807:  54%|█████▍    | 56/103 [02:31<02:04,  2.64s/it] test:57/103 | accuracy 1334  0.731359649122807:  55%|█████▌    | 57/103 [02:31<02:00,  2.63s/it]test:58/103 | accuracy 1357  0.7311422413793104:  55%|█████▌    | 57/103 [02:34<02:00,  2.63s/it]test:58/103 | accuracy 1357  0.7311422413793104:  56%|█████▋    | 58/103 [02:34<01:59,  2.66s/it]test:59/103 | accuracy 1384  0.7330508474576272:  56%|█████▋    | 58/103 [02:37<01:59,  2.66s/it]test:59/103 | accuracy 1384  0.7330508474576272:  57%|█████▋    | 59/103 [02:37<01:56,  2.64s/it]test:60/103 | accuracy 1410  0.734375:  57%|█████▋    | 59/103 [02:39<01:56,  2.64s/it]          test:60/103 | accuracy 1410  0.734375:  58%|█████▊    | 60/103 [02:39<01:52,  2.62s/it]test:61/103 | accuracy 1436  0.735655737704918:  58%|█████▊    | 60/103 [02:42<01:52,  2.62s/it]test:61/103 | accuracy 1436  0.735655737704918:  59%|█████▉    | 61/103 [02:42<01:50,  2.62s/it]test:62/103 | accuracy 1464  0.7379032258064516:  59%|█████▉    | 61/103 [02:45<01:50,  2.62s/it]test:62/103 | accuracy 1464  0.7379032258064516:  60%|██████    | 62/103 [02:45<01:47,  2.62s/it]test:63/103 | accuracy 1487  0.7375992063492064:  60%|██████    | 62/103 [02:47<01:47,  2.62s/it]test:63/103 | accuracy 1487  0.7375992063492064:  61%|██████    | 63/103 [02:47<01:46,  2.66s/it]test:64/103 | accuracy 1515  0.73974609375:  61%|██████    | 63/103 [02:50<01:46,  2.66s/it]     test:64/103 | accuracy 1515  0.73974609375:  62%|██████▏   | 64/103 [02:50<01:43,  2.65s/it]test:65/103 | accuracy 1537  0.7389423076923077:  62%|██████▏   | 64/103 [02:53<01:43,  2.65s/it]test:65/103 | accuracy 1537  0.7389423076923077:  63%|██████▎   | 65/103 [02:53<01:40,  2.66s/it]test:66/103 | accuracy 1557  0.7372159090909091:  63%|██████▎   | 65/103 [02:55<01:40,  2.66s/it]test:66/103 | accuracy 1557  0.7372159090909091:  64%|██████▍   | 66/103 [02:55<01:39,  2.69s/it]test:67/103 | accuracy 1580  0.7369402985074627:  64%|██████▍   | 66/103 [02:58<01:39,  2.69s/it]test:67/103 | accuracy 1580  0.7369402985074627:  65%|██████▌   | 67/103 [02:58<01:36,  2.69s/it]test:68/103 | accuracy 1602  0.7362132352941176:  65%|██████▌   | 67/103 [03:01<01:36,  2.69s/it]test:68/103 | accuracy 1602  0.7362132352941176:  66%|██████▌   | 68/103 [03:01<01:33,  2.68s/it]test:69/103 | accuracy 1626  0.7364130434782609:  66%|██████▌   | 68/103 [03:03<01:33,  2.68s/it]test:69/103 | accuracy 1626  0.7364130434782609:  67%|██████▋   | 69/103 [03:03<01:31,  2.68s/it]test:70/103 | accuracy 1646  0.7348214285714286:  67%|██████▋   | 69/103 [03:06<01:31,  2.68s/it]test:70/103 | accuracy 1646  0.7348214285714286:  68%|██████▊   | 70/103 [03:06<01:27,  2.65s/it]test:71/103 | accuracy 1675  0.7372359154929577:  68%|██████▊   | 70/103 [03:09<01:27,  2.65s/it]test:71/103 | accuracy 1675  0.7372359154929577:  69%|██████▉   | 71/103 [03:09<01:23,  2.62s/it]test:72/103 | accuracy 1699  0.7374131944444444:  69%|██████▉   | 71/103 [03:11<01:23,  2.62s/it]test:72/103 | accuracy 1699  0.7374131944444444:  70%|██████▉   | 72/103 [03:11<01:21,  2.62s/it]test:73/103 | accuracy 1718  0.735445205479452:  70%|██████▉   | 72/103 [03:14<01:21,  2.62s/it] test:73/103 | accuracy 1718  0.735445205479452:  71%|███████   | 73/103 [03:14<01:18,  2.63s/it]test:74/103 | accuracy 1744  0.7364864864864865:  71%|███████   | 73/103 [03:16<01:18,  2.63s/it]test:74/103 | accuracy 1744  0.7364864864864865:  72%|███████▏  | 74/103 [03:16<01:16,  2.63s/it]test:75/103 | accuracy 1770  0.7375:  72%|███████▏  | 74/103 [03:19<01:16,  2.63s/it]            test:75/103 | accuracy 1770  0.7375:  73%|███████▎  | 75/103 [03:19<01:13,  2.63s/it]test:76/103 | accuracy 1789  0.735608552631579:  73%|███████▎  | 75/103 [03:22<01:13,  2.63s/it]test:76/103 | accuracy 1789  0.735608552631579:  74%|███████▍  | 76/103 [03:22<01:11,  2.64s/it]test:77/103 | accuracy 1812  0.7353896103896104:  74%|███████▍  | 76/103 [03:24<01:11,  2.64s/it]test:77/103 | accuracy 1812  0.7353896103896104:  75%|███████▍  | 77/103 [03:24<01:08,  2.63s/it]test:78/103 | accuracy 1834  0.7347756410256411:  75%|███████▍  | 77/103 [03:27<01:08,  2.63s/it]test:78/103 | accuracy 1834  0.7347756410256411:  76%|███████▌  | 78/103 [03:27<01:07,  2.69s/it]test:79/103 | accuracy 1856  0.7341772151898734:  76%|███████▌  | 78/103 [03:30<01:07,  2.69s/it]test:79/103 | accuracy 1856  0.7341772151898734:  77%|███████▋  | 79/103 [03:30<01:04,  2.69s/it]test:80/103 | accuracy 1880  0.734375:  77%|███████▋  | 79/103 [03:32<01:04,  2.69s/it]          test:80/103 | accuracy 1880  0.734375:  78%|███████▊  | 80/103 [03:32<01:01,  2.66s/it]test:81/103 | accuracy 1903  0.7341820987654321:  78%|███████▊  | 80/103 [03:35<01:01,  2.66s/it]test:81/103 | accuracy 1903  0.7341820987654321:  79%|███████▊  | 81/103 [03:35<00:58,  2.65s/it]test:82/103 | accuracy 1929  0.7351371951219512:  79%|███████▊  | 81/103 [03:38<00:58,  2.65s/it]test:82/103 | accuracy 1929  0.7351371951219512:  80%|███████▉  | 82/103 [03:38<00:55,  2.66s/it]test:83/103 | accuracy 1952  0.7349397590361446:  80%|███████▉  | 82/103 [03:40<00:55,  2.66s/it]test:83/103 | accuracy 1952  0.7349397590361446:  81%|████████  | 83/103 [03:40<00:52,  2.62s/it]test:84/103 | accuracy 1975  0.7347470238095238:  81%|████████  | 83/103 [03:43<00:52,  2.62s/it]test:84/103 | accuracy 1975  0.7347470238095238:  82%|████████▏ | 84/103 [03:43<00:50,  2.66s/it]test:85/103 | accuracy 2000  0.7352941176470589:  82%|████████▏ | 84/103 [03:46<00:50,  2.66s/it]test:85/103 | accuracy 2000  0.7352941176470589:  83%|████████▎ | 85/103 [03:46<00:48,  2.67s/it]test:86/103 | accuracy 2030  0.7376453488372093:  83%|████████▎ | 85/103 [03:48<00:48,  2.67s/it]test:86/103 | accuracy 2030  0.7376453488372093:  83%|████████▎ | 86/103 [03:48<00:44,  2.64s/it]test:87/103 | accuracy 2048  0.735632183908046:  83%|████████▎ | 86/103 [03:51<00:44,  2.64s/it] test:87/103 | accuracy 2048  0.735632183908046:  84%|████████▍ | 87/103 [03:51<00:41,  2.62s/it]test:88/103 | accuracy 2071  0.7354403409090909:  84%|████████▍ | 87/103 [03:54<00:41,  2.62s/it]test:88/103 | accuracy 2071  0.7354403409090909:  85%|████████▌ | 88/103 [03:54<00:39,  2.64s/it]test:89/103 | accuracy 2094  0.735252808988764:  85%|████████▌ | 88/103 [03:56<00:39,  2.64s/it] test:89/103 | accuracy 2094  0.735252808988764:  86%|████████▋ | 89/103 [03:56<00:36,  2.63s/it]test:90/103 | accuracy 2118  0.7354166666666667:  86%|████████▋ | 89/103 [03:59<00:36,  2.63s/it]test:90/103 | accuracy 2118  0.7354166666666667:  87%|████████▋ | 90/103 [03:59<00:34,  2.65s/it]test:91/103 | accuracy 2140  0.7348901098901099:  87%|████████▋ | 90/103 [04:01<00:34,  2.65s/it]test:91/103 | accuracy 2140  0.7348901098901099:  88%|████████▊ | 91/103 [04:02<00:31,  2.65s/it]test:92/103 | accuracy 2166  0.735733695652174:  88%|████████▊ | 91/103 [04:04<00:31,  2.65s/it] test:92/103 | accuracy 2166  0.735733695652174:  89%|████████▉ | 92/103 [04:04<00:29,  2.69s/it]test:93/103 | accuracy 2193  0.7368951612903226:  89%|████████▉ | 92/103 [04:07<00:29,  2.69s/it]test:93/103 | accuracy 2193  0.7368951612903226:  90%|█████████ | 93/103 [04:07<00:26,  2.68s/it]test:94/103 | accuracy 2219  0.7376994680851063:  90%|█████████ | 93/103 [04:10<00:26,  2.68s/it]test:94/103 | accuracy 2219  0.7376994680851063:  91%|█████████▏| 94/103 [04:10<00:24,  2.67s/it]test:95/103 | accuracy 2239  0.7365131578947368:  91%|█████████▏| 94/103 [04:12<00:24,  2.67s/it]test:95/103 | accuracy 2239  0.7365131578947368:  92%|█████████▏| 95/103 [04:12<00:21,  2.65s/it]test:96/103 | accuracy 2264  0.7369791666666666:  92%|█████████▏| 95/103 [04:15<00:21,  2.65s/it]test:96/103 | accuracy 2264  0.7369791666666666:  93%|█████████▎| 96/103 [04:15<00:18,  2.66s/it]test:97/103 | accuracy 2292  0.7384020618556701:  93%|█████████▎| 96/103 [04:17<00:18,  2.66s/it]test:97/103 | accuracy 2292  0.7384020618556701:  94%|█████████▍| 97/103 [04:18<00:15,  2.65s/it]test:98/103 | accuracy 2320  0.7397959183673469:  94%|█████████▍| 97/103 [04:20<00:15,  2.65s/it]test:98/103 | accuracy 2320  0.7397959183673469:  95%|█████████▌| 98/103 [04:20<00:13,  2.65s/it]test:99/103 | accuracy 2349  0.7414772727272727:  95%|█████████▌| 98/103 [04:23<00:13,  2.65s/it]test:99/103 | accuracy 2349  0.7414772727272727:  96%|█████████▌| 99/103 [04:23<00:10,  2.65s/it]test:100/103 | accuracy 2370  0.740625:  96%|█████████▌| 99/103 [04:25<00:10,  2.65s/it]         test:100/103 | accuracy 2370  0.740625:  97%|█████████▋| 100/103 [04:25<00:07,  2.65s/it]test:101/103 | accuracy 2394  0.7407178217821783:  97%|█████████▋| 100/103 [04:28<00:07,  2.65s/it]test:101/103 | accuracy 2394  0.7407178217821783:  98%|█████████▊| 101/103 [04:28<00:05,  2.66s/it]test:102/103 | accuracy 2417  0.7405024509803921:  98%|█████████▊| 101/103 [04:31<00:05,  2.66s/it]test:102/103 | accuracy 2417  0.7405024509803921:  99%|█████████▉| 102/103 [04:31<00:02,  2.65s/it]test:103/103 | accuracy 2422  0.7406727828746177:  99%|█████████▉| 102/103 [04:32<00:02,  2.65s/it]test:103/103 | accuracy 2422  0.7406727828746177: 100%|██████████| 103/103 [04:32<00:00,  2.23s/it]test:103/103 | accuracy 2422  0.7406727828746177: 100%|██████████| 103/103 [04:32<00:00,  2.65s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_64.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 128,
    "lora_dropout": 0.0,
    "r": 64
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:35,  1.95s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.93s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.93s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.91s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.90s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.90s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.89s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.90s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:18,  1.90s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:18,  2.09s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:22<00:17,  2.17s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:23<00:14,  2.09s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:25<00:12,  2.05s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:27<00:09,  1.99s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:29<00:07,  1.97s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:31<00:05,  1.95s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:33<00:03,  1.93s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:35<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.94s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_64.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors #####
  0%|          | 0/58 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/58 | accuracy 26  0.8125:   0%|          | 0/58 [00:10<?, ?it/s]test:1/58 | accuracy 26  0.8125:   2%|▏         | 1/58 [00:10<09:51, 10.38s/it]test:2/58 | accuracy 54  0.84375:   2%|▏         | 1/58 [00:16<09:51, 10.38s/it]test:2/58 | accuracy 54  0.84375:   3%|▎         | 2/58 [00:16<07:13,  7.74s/it]test:3/58 | accuracy 81  0.84375:   3%|▎         | 2/58 [00:22<07:13,  7.74s/it]test:3/58 | accuracy 81  0.84375:   5%|▌         | 3/58 [00:22<06:27,  7.05s/it]test:4/58 | accuracy 111  0.8671875:   5%|▌         | 3/58 [00:28<06:27,  7.05s/it]test:4/58 | accuracy 111  0.8671875:   7%|▋         | 4/58 [00:28<05:56,  6.59s/it]test:5/58 | accuracy 140  0.875:   7%|▋         | 4/58 [00:38<05:56,  6.59s/it]    test:5/58 | accuracy 140  0.875:   9%|▊         | 5/58 [00:38<06:54,  7.82s/it]test:6/58 | accuracy 167  0.8697916666666666:   9%|▊         | 5/58 [00:45<06:54,  7.82s/it]test:6/58 | accuracy 167  0.8697916666666666:  10%|█         | 6/58 [00:45<06:31,  7.53s/it]test:7/58 | accuracy 197  0.8794642857142857:  10%|█         | 6/58 [00:52<06:31,  7.53s/it]test:7/58 | accuracy 197  0.8794642857142857:  12%|█▏        | 7/58 [00:52<06:15,  7.35s/it]test:8/58 | accuracy 227  0.88671875:  12%|█▏        | 7/58 [01:00<06:15,  7.35s/it]        test:8/58 | accuracy 227  0.88671875:  14%|█▍        | 8/58 [01:00<06:15,  7.50s/it]test:9/58 | accuracy 257  0.8923611111111112:  14%|█▍        | 8/58 [01:08<06:15,  7.50s/it]test:9/58 | accuracy 257  0.8923611111111112:  16%|█▌        | 9/58 [01:08<06:17,  7.70s/it]Traceback (most recent call last):
  File "commonsense_evaluate.py", line 346, in <module>
    main()
  File "commonsense_evaluate.py", line 124, in main
    prompts,outputs = evaluate(instructions)
  File "commonsense_evaluate.py", line 94, in evaluate
    generation_output = model.generate(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py", line 2063, in generate
    result = self._beam_search(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py", line 3238, in _beam_search
    outputs = self(**model_inputs, return_dict=True)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 1535, in forward
    outputs = self.model(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 1395, in forward
    layer_outputs = decoder_layer(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 1113, in forward
    hidden_states, router_logits, shared_routing_adapter_router_logits = self.block_sparse_moe(hidden_states)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 994, in forward
    current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list, None]
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 882, in forward
    current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.70 GiB. GPU 1 has a total capacity of 39.38 GiB of which 1.64 GiB is free. Including non-PyTorch memory, this process has 37.70 GiB memory in use. Of the allocated memory 34.29 GiB is allocated by PyTorch, and 2.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
test:9/58 | accuracy 257  0.8923611111111112:  16%|█▌        | 9/58 [01:14<06:47,  8.32s/it]Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_64.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 128,
    "lora_dropout": 0.0,
    "r": 64
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.92s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.91s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.91s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.90s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.92s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.92s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.90s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.90s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.90s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.89s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.89s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.90s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.90s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.91s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.90s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.90s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.89s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.90s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.89s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_64.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors #####
  0%|          | 0/62 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/62 | accuracy 23  0.71875:   0%|          | 0/62 [00:05<?, ?it/s]test:1/62 | accuracy 23  0.71875:   2%|▏         | 1/62 [00:05<05:43,  5.63s/it]test:2/62 | accuracy 48  0.75:   2%|▏         | 1/62 [00:09<05:43,  5.63s/it]   test:2/62 | accuracy 48  0.75:   3%|▎         | 2/62 [00:09<04:29,  4.48s/it]test:3/62 | accuracy 74  0.7708333333333334:   3%|▎         | 2/62 [00:12<04:29,  4.48s/it]test:3/62 | accuracy 74  0.7708333333333334:   5%|▍         | 3/62 [00:12<04:03,  4.12s/it]test:4/62 | accuracy 100  0.78125:   5%|▍         | 3/62 [00:16<04:03,  4.12s/it]          test:4/62 | accuracy 100  0.78125:   6%|▋         | 4/62 [00:16<03:50,  3.98s/it]test:5/62 | accuracy 129  0.80625:   6%|▋         | 4/62 [00:20<03:50,  3.98s/it]test:5/62 | accuracy 129  0.80625:   8%|▊         | 5/62 [00:20<03:42,  3.90s/it]test:6/62 | accuracy 159  0.828125:   8%|▊         | 5/62 [00:24<03:42,  3.90s/it]test:6/62 | accuracy 159  0.828125:  10%|▉         | 6/62 [00:24<03:34,  3.84s/it]test:7/62 | accuracy 188  0.8392857142857143:  10%|▉         | 6/62 [00:28<03:34,  3.84s/it]test:7/62 | accuracy 188  0.8392857142857143:  11%|█▏        | 7/62 [00:28<03:36,  3.93s/it]test:8/62 | accuracy 213  0.83203125:  11%|█▏        | 7/62 [00:32<03:36,  3.93s/it]        test:8/62 | accuracy 213  0.83203125:  13%|█▎        | 8/62 [00:32<03:30,  3.89s/it]test:9/62 | accuracy 240  0.8333333333333334:  13%|█▎        | 8/62 [00:36<03:30,  3.89s/it]test:9/62 | accuracy 240  0.8333333333333334:  15%|█▍        | 9/62 [00:36<03:31,  4.00s/it]test:10/62 | accuracy 265  0.828125:  15%|█▍        | 9/62 [00:40<03:31,  4.00s/it]         test:10/62 | accuracy 265  0.828125:  16%|█▌        | 10/62 [00:40<03:25,  3.95s/it]test:11/62 | accuracy 291  0.8267045454545454:  16%|█▌        | 10/62 [00:44<03:25,  3.95s/it]test:11/62 | accuracy 291  0.8267045454545454:  18%|█▊        | 11/62 [00:44<03:21,  3.95s/it]test:12/62 | accuracy 319  0.8307291666666666:  18%|█▊        | 11/62 [00:48<03:21,  3.95s/it]test:12/62 | accuracy 319  0.8307291666666666:  19%|█▉        | 12/62 [00:48<03:22,  4.05s/it]test:13/62 | accuracy 346  0.8317307692307693:  19%|█▉        | 12/62 [00:52<03:22,  4.05s/it]test:13/62 | accuracy 346  0.8317307692307693:  21%|██        | 13/62 [00:52<03:22,  4.12s/it]test:14/62 | accuracy 369  0.8236607142857143:  21%|██        | 13/62 [00:56<03:22,  4.12s/it]test:14/62 | accuracy 369  0.8236607142857143:  23%|██▎       | 14/62 [00:56<03:15,  4.07s/it]test:15/62 | accuracy 391  0.8145833333333333:  23%|██▎       | 14/62 [01:00<03:15,  4.07s/it]test:15/62 | accuracy 391  0.8145833333333333:  24%|██▍       | 15/62 [01:00<03:11,  4.07s/it]test:16/62 | accuracy 419  0.818359375:  24%|██▍       | 15/62 [01:04<03:11,  4.07s/it]       test:16/62 | accuracy 419  0.818359375:  26%|██▌       | 16/62 [01:04<03:08,  4.09s/it]test:17/62 | accuracy 444  0.8161764705882353:  26%|██▌       | 16/62 [01:08<03:08,  4.09s/it]test:17/62 | accuracy 444  0.8161764705882353:  27%|██▋       | 17/62 [01:08<03:00,  4.01s/it]test:18/62 | accuracy 468  0.8125:  27%|██▋       | 17/62 [01:12<03:00,  4.01s/it]            test:18/62 | accuracy 468  0.8125:  29%|██▉       | 18/62 [01:12<02:51,  3.90s/it]test:19/62 | accuracy 487  0.8009868421052632:  29%|██▉       | 18/62 [01:16<02:51,  3.90s/it]test:19/62 | accuracy 487  0.8009868421052632:  31%|███       | 19/62 [01:16<02:45,  3.85s/it]test:20/62 | accuracy 515  0.8046875:  31%|███       | 19/62 [01:20<02:45,  3.85s/it]         test:20/62 | accuracy 515  0.8046875:  32%|███▏      | 20/62 [01:20<02:42,  3.87s/it]test:21/62 | accuracy 541  0.8050595238095238:  32%|███▏      | 20/62 [01:23<02:42,  3.87s/it]test:21/62 | accuracy 541  0.8050595238095238:  34%|███▍      | 21/62 [01:23<02:37,  3.85s/it]test:22/62 | accuracy 566  0.8039772727272727:  34%|███▍      | 21/62 [01:27<02:37,  3.85s/it]test:22/62 | accuracy 566  0.8039772727272727:  35%|███▌      | 22/62 [01:27<02:31,  3.78s/it]test:23/62 | accuracy 593  0.8057065217391305:  35%|███▌      | 22/62 [01:31<02:31,  3.78s/it]test:23/62 | accuracy 593  0.8057065217391305:  37%|███▋      | 23/62 [01:31<02:32,  3.91s/it]test:24/62 | accuracy 620  0.8072916666666666:  37%|███▋      | 23/62 [01:36<02:32,  3.91s/it]test:24/62 | accuracy 620  0.8072916666666666:  39%|███▊      | 24/62 [01:36<02:33,  4.04s/it]test:25/62 | accuracy 643  0.80375:  39%|███▊      | 24/62 [01:39<02:33,  4.04s/it]           test:25/62 | accuracy 643  0.80375:  40%|████      | 25/62 [01:39<02:27,  3.99s/it]test:26/62 | accuracy 665  0.7992788461538461:  40%|████      | 25/62 [01:43<02:27,  3.99s/it]test:26/62 | accuracy 665  0.7992788461538461:  42%|████▏     | 26/62 [01:43<02:21,  3.93s/it]test:27/62 | accuracy 689  0.7974537037037037:  42%|████▏     | 26/62 [01:47<02:21,  3.93s/it]test:27/62 | accuracy 689  0.7974537037037037:  44%|████▎     | 27/62 [01:47<02:18,  3.95s/it]test:28/62 | accuracy 713  0.7957589285714286:  44%|████▎     | 27/62 [01:51<02:18,  3.95s/it]test:28/62 | accuracy 713  0.7957589285714286:  45%|████▌     | 28/62 [01:51<02:13,  3.92s/it]test:29/62 | accuracy 741  0.7984913793103449:  45%|████▌     | 28/62 [01:55<02:13,  3.92s/it]test:29/62 | accuracy 741  0.7984913793103449:  47%|████▋     | 29/62 [01:55<02:10,  3.96s/it]test:30/62 | accuracy 766  0.7979166666666667:  47%|████▋     | 29/62 [01:59<02:10,  3.96s/it]test:30/62 | accuracy 766  0.7979166666666667:  48%|████▊     | 30/62 [01:59<02:05,  3.92s/it]test:31/62 | accuracy 793  0.7993951612903226:  48%|████▊     | 30/62 [02:02<02:05,  3.92s/it]test:31/62 | accuracy 793  0.7993951612903226:  50%|█████     | 31/62 [02:02<01:57,  3.80s/it]test:32/62 | accuracy 821  0.8017578125:  50%|█████     | 31/62 [02:06<01:57,  3.80s/it]      test:32/62 | accuracy 821  0.8017578125:  52%|█████▏    | 32/62 [02:06<01:53,  3.79s/it]test:33/62 | accuracy 844  0.7992424242424242:  52%|█████▏    | 32/62 [02:10<01:53,  3.79s/it]test:33/62 | accuracy 844  0.7992424242424242:  53%|█████▎    | 33/62 [02:10<01:49,  3.79s/it]test:34/62 | accuracy 867  0.796875:  53%|█████▎    | 33/62 [02:14<01:49,  3.79s/it]          test:34/62 | accuracy 867  0.796875:  55%|█████▍    | 34/62 [02:14<01:47,  3.84s/it]test:35/62 | accuracy 893  0.7973214285714286:  55%|█████▍    | 34/62 [02:18<01:47,  3.84s/it]test:35/62 | accuracy 893  0.7973214285714286:  56%|█████▋    | 35/62 [02:18<01:47,  3.98s/it]test:36/62 | accuracy 922  0.8003472222222222:  56%|█████▋    | 35/62 [02:22<01:47,  3.98s/it]test:36/62 | accuracy 922  0.8003472222222222:  58%|█████▊    | 36/62 [02:22<01:40,  3.87s/it]test:37/62 | accuracy 948  0.8006756756756757:  58%|█████▊    | 36/62 [02:26<01:40,  3.87s/it]test:37/62 | accuracy 948  0.8006756756756757:  60%|█████▉    | 37/62 [02:26<01:36,  3.88s/it]test:38/62 | accuracy 977  0.803453947368421:  60%|█████▉    | 37/62 [02:30<01:36,  3.88s/it] test:38/62 | accuracy 977  0.803453947368421:  61%|██████▏   | 38/62 [02:30<01:32,  3.84s/it]test:39/62 | accuracy 1001  0.8020833333333334:  61%|██████▏   | 38/62 [02:33<01:32,  3.84s/it]test:39/62 | accuracy 1001  0.8020833333333334:  63%|██████▎   | 39/62 [02:33<01:28,  3.84s/it]test:40/62 | accuracy 1028  0.803125:  63%|██████▎   | 39/62 [02:37<01:28,  3.84s/it]          test:40/62 | accuracy 1028  0.803125:  65%|██████▍   | 40/62 [02:37<01:24,  3.84s/it]test:41/62 | accuracy 1054  0.8033536585365854:  65%|██████▍   | 40/62 [02:41<01:24,  3.84s/it]test:41/62 | accuracy 1054  0.8033536585365854:  66%|██████▌   | 41/62 [02:41<01:19,  3.80s/it]test:42/62 | accuracy 1077  0.8013392857142857:  66%|██████▌   | 41/62 [02:45<01:19,  3.80s/it]test:42/62 | accuracy 1077  0.8013392857142857:  68%|██████▊   | 42/62 [02:45<01:15,  3.78s/it]test:43/62 | accuracy 1105  0.8030523255813954:  68%|██████▊   | 42/62 [02:48<01:15,  3.78s/it]test:43/62 | accuracy 1105  0.8030523255813954:  69%|██████▉   | 43/62 [02:49<01:12,  3.80s/it]test:44/62 | accuracy 1128  0.8011363636363636:  69%|██████▉   | 43/62 [02:52<01:12,  3.80s/it]test:44/62 | accuracy 1128  0.8011363636363636:  71%|███████   | 44/62 [02:52<01:07,  3.77s/it]test:45/62 | accuracy 1154  0.8013888888888889:  71%|███████   | 44/62 [02:56<01:07,  3.77s/it]test:45/62 | accuracy 1154  0.8013888888888889:  73%|███████▎  | 45/62 [02:56<01:05,  3.84s/it]test:46/62 | accuracy 1177  0.7995923913043478:  73%|███████▎  | 45/62 [03:00<01:05,  3.84s/it]test:46/62 | accuracy 1177  0.7995923913043478:  74%|███████▍  | 46/62 [03:00<01:02,  3.89s/it]test:47/62 | accuracy 1203  0.7998670212765957:  74%|███████▍  | 46/62 [03:04<01:02,  3.89s/it]test:47/62 | accuracy 1203  0.7998670212765957:  76%|███████▌  | 47/62 [03:04<00:59,  3.94s/it]test:48/62 | accuracy 1226  0.7981770833333334:  76%|███████▌  | 47/62 [03:08<00:59,  3.94s/it]test:48/62 | accuracy 1226  0.7981770833333334:  77%|███████▋  | 48/62 [03:08<00:54,  3.89s/it]test:49/62 | accuracy 1251  0.7978316326530612:  77%|███████▋  | 48/62 [03:12<00:54,  3.89s/it]test:49/62 | accuracy 1251  0.7978316326530612:  79%|███████▉  | 49/62 [03:12<00:49,  3.79s/it]test:50/62 | accuracy 1275  0.796875:  79%|███████▉  | 49/62 [03:16<00:49,  3.79s/it]          test:50/62 | accuracy 1275  0.796875:  81%|████████  | 50/62 [03:16<00:46,  3.85s/it]test:51/62 | accuracy 1301  0.7971813725490197:  81%|████████  | 50/62 [03:19<00:46,  3.85s/it]test:51/62 | accuracy 1301  0.7971813725490197:  82%|████████▏ | 51/62 [03:20<00:42,  3.87s/it]test:52/62 | accuracy 1329  0.7986778846153846:  82%|████████▏ | 51/62 [03:23<00:42,  3.87s/it]test:52/62 | accuracy 1329  0.7986778846153846:  84%|████████▍ | 52/62 [03:23<00:38,  3.87s/it]test:53/62 | accuracy 1355  0.7989386792452831:  84%|████████▍ | 52/62 [03:27<00:38,  3.87s/it]test:53/62 | accuracy 1355  0.7989386792452831:  85%|████████▌ | 53/62 [03:27<00:34,  3.88s/it]test:54/62 | accuracy 1379  0.7980324074074074:  85%|████████▌ | 53/62 [03:31<00:34,  3.88s/it]test:54/62 | accuracy 1379  0.7980324074074074:  87%|████████▋ | 54/62 [03:31<00:30,  3.87s/it]test:55/62 | accuracy 1408  0.8:  87%|████████▋ | 54/62 [03:35<00:30,  3.87s/it]               test:55/62 | accuracy 1408  0.8:  89%|████████▊ | 55/62 [03:35<00:26,  3.83s/it]test:56/62 | accuracy 1435  0.80078125:  89%|████████▊ | 55/62 [03:38<00:26,  3.83s/it]test:56/62 | accuracy 1435  0.80078125:  90%|█████████ | 56/62 [03:39<00:22,  3.78s/it]test:57/62 | accuracy 1461  0.8009868421052632:  90%|█████████ | 56/62 [03:42<00:22,  3.78s/it]test:57/62 | accuracy 1461  0.8009868421052632:  92%|█████████▏| 57/62 [03:42<00:18,  3.78s/it]test:58/62 | accuracy 1485  0.8001077586206896:  92%|█████████▏| 57/62 [03:46<00:18,  3.78s/it]test:58/62 | accuracy 1485  0.8001077586206896:  94%|█████████▎| 58/62 [03:46<00:15,  3.87s/it]test:59/62 | accuracy 1511  0.8003177966101694:  94%|█████████▎| 58/62 [03:50<00:15,  3.87s/it]test:59/62 | accuracy 1511  0.8003177966101694:  95%|█████████▌| 59/62 [03:50<00:11,  3.84s/it]test:60/62 | accuracy 1540  0.8020833333333334:  95%|█████████▌| 59/62 [03:54<00:11,  3.84s/it]test:60/62 | accuracy 1540  0.8020833333333334:  97%|█████████▋| 60/62 [03:54<00:07,  3.79s/it]test:61/62 | accuracy 1563  0.8007172131147541:  97%|█████████▋| 60/62 [03:58<00:07,  3.79s/it]test:61/62 | accuracy 1563  0.8007172131147541:  98%|█████████▊| 61/62 [03:58<00:03,  3.78s/it]test:62/62 | accuracy 1565  0.8009211873080859:  98%|█████████▊| 61/62 [03:58<00:03,  3.78s/it]test:62/62 | accuracy 1565  0.8009211873080859: 100%|██████████| 62/62 [03:58<00:00,  2.86s/it]test:62/62 | accuracy 1565  0.8009211873080859: 100%|██████████| 62/62 [03:58<00:00,  3.85s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_64.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 128,
    "lora_dropout": 0.0,
    "r": 64
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.93s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.93s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.92s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.90s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.90s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.90s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.90s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:21,  1.94s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.93s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.92s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:21<00:15,  1.92s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:23<00:13,  1.93s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.92s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.91s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.91s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.91s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.92s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_64.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors #####
  0%|          | 0/314 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/314 | accuracy 27  0.84375:   0%|          | 0/314 [00:06<?, ?it/s]test:1/314 | accuracy 27  0.84375:   0%|          | 1/314 [00:06<35:44,  6.85s/it]test:2/314 | accuracy 56  0.875:   0%|          | 1/314 [00:12<35:44,  6.85s/it]  test:2/314 | accuracy 56  0.875:   1%|          | 2/314 [00:12<32:22,  6.23s/it]test:3/314 | accuracy 83  0.8645833333333334:   1%|          | 2/314 [00:18<32:22,  6.23s/it]test:3/314 | accuracy 83  0.8645833333333334:   1%|          | 3/314 [00:18<31:39,  6.11s/it]test:4/314 | accuracy 111  0.8671875:   1%|          | 3/314 [00:25<31:39,  6.11s/it]        test:4/314 | accuracy 111  0.8671875:   1%|▏         | 4/314 [00:25<32:16,  6.25s/it]test:5/314 | accuracy 138  0.8625:   1%|▏         | 4/314 [00:31<32:16,  6.25s/it]   test:5/314 | accuracy 138  0.8625:   2%|▏         | 5/314 [00:31<33:01,  6.41s/it]test:6/314 | accuracy 164  0.8541666666666666:   2%|▏         | 5/314 [00:37<33:01,  6.41s/it]test:6/314 | accuracy 164  0.8541666666666666:   2%|▏         | 6/314 [00:37<31:30,  6.14s/it]test:7/314 | accuracy 190  0.8482142857142857:   2%|▏         | 6/314 [00:43<31:30,  6.14s/it]test:7/314 | accuracy 190  0.8482142857142857:   2%|▏         | 7/314 [00:43<31:08,  6.09s/it]test:8/314 | accuracy 216  0.84375:   2%|▏         | 7/314 [00:49<31:08,  6.09s/it]           test:8/314 | accuracy 216  0.84375:   3%|▎         | 8/314 [00:49<30:20,  5.95s/it]test:9/314 | accuracy 246  0.8541666666666666:   3%|▎         | 8/314 [00:54<30:20,  5.95s/it]test:9/314 | accuracy 246  0.8541666666666666:   3%|▎         | 9/314 [00:54<29:50,  5.87s/it]test:10/314 | accuracy 277  0.865625:   3%|▎         | 9/314 [01:00<29:50,  5.87s/it]         test:10/314 | accuracy 277  0.865625:   3%|▎         | 10/314 [01:00<30:23,  6.00s/it]test:11/314 | accuracy 307  0.8721590909090909:   3%|▎         | 10/314 [01:06<30:23,  6.00s/it]test:11/314 | accuracy 307  0.8721590909090909:   4%|▎         | 11/314 [01:07<30:18,  6.00s/it]test:12/314 | accuracy 335  0.8723958333333334:   4%|▎         | 11/314 [01:12<30:18,  6.00s/it]test:12/314 | accuracy 335  0.8723958333333334:   4%|▍         | 12/314 [01:12<29:28,  5.86s/it]test:13/314 | accuracy 362  0.8701923076923077:   4%|▍         | 12/314 [01:18<29:28,  5.86s/it]test:13/314 | accuracy 362  0.8701923076923077:   4%|▍         | 13/314 [01:18<30:09,  6.01s/it]test:14/314 | accuracy 389  0.8683035714285714:   4%|▍         | 13/314 [01:24<30:09,  6.01s/it]test:14/314 | accuracy 389  0.8683035714285714:   4%|▍         | 14/314 [01:25<30:11,  6.04s/it]test:15/314 | accuracy 418  0.8708333333333333:   4%|▍         | 14/314 [01:31<30:11,  6.04s/it]test:15/314 | accuracy 418  0.8708333333333333:   5%|▍         | 15/314 [01:31<30:39,  6.15s/it]test:16/314 | accuracy 446  0.87109375:   5%|▍         | 15/314 [01:37<30:39,  6.15s/it]        test:16/314 | accuracy 446  0.87109375:   5%|▌         | 16/314 [01:37<30:40,  6.18s/it]test:17/314 | accuracy 475  0.8731617647058824:   5%|▌         | 16/314 [01:44<30:40,  6.18s/it]test:17/314 | accuracy 475  0.8731617647058824:   5%|▌         | 17/314 [01:44<31:49,  6.43s/it]test:18/314 | accuracy 501  0.8697916666666666:   5%|▌         | 17/314 [01:50<31:49,  6.43s/it]test:18/314 | accuracy 501  0.8697916666666666:   6%|▌         | 18/314 [01:50<31:07,  6.31s/it]test:19/314 | accuracy 527  0.8667763157894737:   6%|▌         | 18/314 [01:56<31:07,  6.31s/it]test:19/314 | accuracy 527  0.8667763157894737:   6%|▌         | 19/314 [01:56<29:33,  6.01s/it]test:20/314 | accuracy 553  0.8640625:   6%|▌         | 19/314 [02:01<29:33,  6.01s/it]         test:20/314 | accuracy 553  0.8640625:   6%|▋         | 20/314 [02:01<28:52,  5.89s/it]test:21/314 | accuracy 583  0.8675595238095238:   6%|▋         | 20/314 [02:07<28:52,  5.89s/it]test:21/314 | accuracy 583  0.8675595238095238:   7%|▋         | 21/314 [02:07<29:05,  5.96s/it]test:22/314 | accuracy 614  0.8721590909090909:   7%|▋         | 21/314 [02:14<29:05,  5.96s/it]test:22/314 | accuracy 614  0.8721590909090909:   7%|▋         | 22/314 [02:14<30:06,  6.19s/it]test:23/314 | accuracy 644  0.875:   7%|▋         | 22/314 [02:20<30:06,  6.19s/it]             test:23/314 | accuracy 644  0.875:   7%|▋         | 23/314 [02:20<29:15,  6.03s/it]test:24/314 | accuracy 672  0.875:   7%|▋         | 23/314 [02:25<29:15,  6.03s/it]test:24/314 | accuracy 672  0.875:   8%|▊         | 24/314 [02:25<28:07,  5.82s/it]test:25/314 | accuracy 701  0.87625:   8%|▊         | 24/314 [02:32<28:07,  5.82s/it]test:25/314 | accuracy 701  0.87625:   8%|▊         | 25/314 [02:32<29:56,  6.21s/it]test:26/314 | accuracy 732  0.8798076923076923:   8%|▊         | 25/314 [02:38<29:56,  6.21s/it]test:26/314 | accuracy 732  0.8798076923076923:   8%|▊         | 26/314 [02:38<29:53,  6.23s/it]test:27/314 | accuracy 759  0.8784722222222222:   8%|▊         | 26/314 [02:45<29:53,  6.23s/it]test:27/314 | accuracy 759  0.8784722222222222:   9%|▊         | 27/314 [02:45<29:52,  6.24s/it]test:28/314 | accuracy 788  0.8794642857142857:   9%|▊         | 27/314 [02:51<29:52,  6.24s/it]test:28/314 | accuracy 788  0.8794642857142857:   9%|▉         | 28/314 [02:51<30:20,  6.36s/it]test:29/314 | accuracy 818  0.8814655172413793:   9%|▉         | 28/314 [02:57<30:20,  6.36s/it]test:29/314 | accuracy 818  0.8814655172413793:   9%|▉         | 29/314 [02:57<29:16,  6.16s/it]test:30/314 | accuracy 844  0.8791666666666667:   9%|▉         | 29/314 [03:03<29:16,  6.16s/it]test:30/314 | accuracy 844  0.8791666666666667:  10%|▉         | 30/314 [03:03<28:35,  6.04s/it]test:31/314 | accuracy 874  0.8810483870967742:  10%|▉         | 30/314 [03:09<28:35,  6.04s/it]test:31/314 | accuracy 874  0.8810483870967742:  10%|▉         | 31/314 [03:09<28:13,  5.98s/it]test:32/314 | accuracy 903  0.8818359375:  10%|▉         | 31/314 [03:15<28:13,  5.98s/it]      test:32/314 | accuracy 903  0.8818359375:  10%|█         | 32/314 [03:15<28:59,  6.17s/it]test:33/314 | accuracy 933  0.8835227272727273:  10%|█         | 32/314 [03:21<28:59,  6.17s/it]test:33/314 | accuracy 933  0.8835227272727273:  11%|█         | 33/314 [03:21<28:05,  6.00s/it]test:34/314 | accuracy 961  0.8832720588235294:  11%|█         | 33/314 [03:26<28:05,  6.00s/it]test:34/314 | accuracy 961  0.8832720588235294:  11%|█         | 34/314 [03:26<27:25,  5.88s/it]test:35/314 | accuracy 991  0.8848214285714285:  11%|█         | 34/314 [03:32<27:25,  5.88s/it]test:35/314 | accuracy 991  0.8848214285714285:  11%|█         | 35/314 [03:32<26:39,  5.73s/it]test:36/314 | accuracy 1018  0.8836805555555556:  11%|█         | 35/314 [03:37<26:39,  5.73s/it]test:36/314 | accuracy 1018  0.8836805555555556:  11%|█▏        | 36/314 [03:37<26:27,  5.71s/it]test:37/314 | accuracy 1046  0.8834459459459459:  11%|█▏        | 36/314 [03:45<26:27,  5.71s/it]test:37/314 | accuracy 1046  0.8834459459459459:  12%|█▏        | 37/314 [03:45<28:33,  6.18s/it]test:38/314 | accuracy 1076  0.8848684210526315:  12%|█▏        | 37/314 [03:50<28:33,  6.18s/it]test:38/314 | accuracy 1076  0.8848684210526315:  12%|█▏        | 38/314 [03:50<27:08,  5.90s/it]test:39/314 | accuracy 1104  0.8846153846153846:  12%|█▏        | 38/314 [03:56<27:08,  5.90s/it]test:39/314 | accuracy 1104  0.8846153846153846:  12%|█▏        | 39/314 [03:56<26:43,  5.83s/it]test:40/314 | accuracy 1134  0.8859375:  12%|█▏        | 39/314 [04:02<26:43,  5.83s/it]         test:40/314 | accuracy 1134  0.8859375:  13%|█▎        | 40/314 [04:02<27:15,  5.97s/it]test:41/314 | accuracy 1160  0.8841463414634146:  13%|█▎        | 40/314 [04:08<27:15,  5.97s/it]test:41/314 | accuracy 1160  0.8841463414634146:  13%|█▎        | 41/314 [04:08<27:21,  6.01s/it]test:42/314 | accuracy 1189  0.8846726190476191:  13%|█▎        | 41/314 [04:14<27:21,  6.01s/it]test:42/314 | accuracy 1189  0.8846726190476191:  13%|█▎        | 42/314 [04:14<27:09,  5.99s/it]test:43/314 | accuracy 1220  0.8866279069767442:  13%|█▎        | 42/314 [04:21<27:09,  5.99s/it]test:43/314 | accuracy 1220  0.8866279069767442:  14%|█▎        | 43/314 [04:21<28:02,  6.21s/it]test:44/314 | accuracy 1245  0.8842329545454546:  14%|█▎        | 43/314 [04:27<28:02,  6.21s/it]test:44/314 | accuracy 1245  0.8842329545454546:  14%|█▍        | 44/314 [04:27<28:01,  6.23s/it]test:45/314 | accuracy 1274  0.8847222222222222:  14%|█▍        | 44/314 [04:33<28:01,  6.23s/it]test:45/314 | accuracy 1274  0.8847222222222222:  14%|█▍        | 45/314 [04:33<28:04,  6.26s/it]test:46/314 | accuracy 1302  0.8845108695652174:  14%|█▍        | 45/314 [04:40<28:04,  6.26s/it]test:46/314 | accuracy 1302  0.8845108695652174:  15%|█▍        | 46/314 [04:40<28:23,  6.36s/it]test:47/314 | accuracy 1330  0.8843085106382979:  15%|█▍        | 46/314 [04:45<28:23,  6.36s/it]test:47/314 | accuracy 1330  0.8843085106382979:  15%|█▍        | 47/314 [04:45<26:41,  6.00s/it]test:48/314 | accuracy 1357  0.8834635416666666:  15%|█▍        | 47/314 [04:51<26:41,  6.00s/it]test:48/314 | accuracy 1357  0.8834635416666666:  15%|█▌        | 48/314 [04:51<26:18,  5.93s/it]test:49/314 | accuracy 1382  0.8813775510204082:  15%|█▌        | 48/314 [04:57<26:18,  5.93s/it]test:49/314 | accuracy 1382  0.8813775510204082:  16%|█▌        | 49/314 [04:57<26:59,  6.11s/it]test:50/314 | accuracy 1409  0.880625:  16%|█▌        | 49/314 [05:04<26:59,  6.11s/it]          test:50/314 | accuracy 1409  0.880625:  16%|█▌        | 50/314 [05:04<27:45,  6.31s/it]test:51/314 | accuracy 1436  0.8799019607843137:  16%|█▌        | 50/314 [05:10<27:45,  6.31s/it]test:51/314 | accuracy 1436  0.8799019607843137:  16%|█▌        | 51/314 [05:10<26:37,  6.07s/it]test:52/314 | accuracy 1464  0.8798076923076923:  16%|█▌        | 51/314 [05:15<26:37,  6.07s/it]test:52/314 | accuracy 1464  0.8798076923076923:  17%|█▋        | 52/314 [05:15<25:38,  5.87s/it]test:53/314 | accuracy 1490  0.8785377358490566:  17%|█▋        | 52/314 [05:22<25:38,  5.87s/it]test:53/314 | accuracy 1490  0.8785377358490566:  17%|█▋        | 53/314 [05:22<26:48,  6.16s/it]test:54/314 | accuracy 1520  0.8796296296296297:  17%|█▋        | 53/314 [05:29<26:48,  6.16s/it]test:54/314 | accuracy 1520  0.8796296296296297:  17%|█▋        | 54/314 [05:29<27:38,  6.38s/it]test:55/314 | accuracy 1545  0.8778409090909091:  17%|█▋        | 54/314 [05:35<27:38,  6.38s/it]test:55/314 | accuracy 1545  0.8778409090909091:  18%|█▊        | 55/314 [05:35<26:45,  6.20s/it]test:56/314 | accuracy 1575  0.87890625:  18%|█▊        | 55/314 [05:40<26:45,  6.20s/it]        test:56/314 | accuracy 1575  0.87890625:  18%|█▊        | 56/314 [05:40<26:18,  6.12s/it]test:57/314 | accuracy 1603  0.8788377192982456:  18%|█▊        | 56/314 [05:46<26:18,  6.12s/it]test:57/314 | accuracy 1603  0.8788377192982456:  18%|█▊        | 57/314 [05:46<25:28,  5.95s/it]test:58/314 | accuracy 1629  0.8776939655172413:  18%|█▊        | 57/314 [05:52<25:28,  5.95s/it]test:58/314 | accuracy 1629  0.8776939655172413:  18%|█▊        | 58/314 [05:52<25:30,  5.98s/it]test:59/314 | accuracy 1659  0.878707627118644:  18%|█▊        | 58/314 [05:59<25:30,  5.98s/it] test:59/314 | accuracy 1659  0.878707627118644:  19%|█▉        | 59/314 [05:59<26:53,  6.33s/it]test:60/314 | accuracy 1691  0.8807291666666667:  19%|█▉        | 59/314 [06:05<26:53,  6.33s/it]test:60/314 | accuracy 1691  0.8807291666666667:  19%|█▉        | 60/314 [06:05<26:05,  6.16s/it]test:61/314 | accuracy 1722  0.882172131147541:  19%|█▉        | 60/314 [06:11<26:05,  6.16s/it] test:61/314 | accuracy 1722  0.882172131147541:  19%|█▉        | 61/314 [06:11<26:23,  6.26s/it]test:62/314 | accuracy 1749  0.8815524193548387:  19%|█▉        | 61/314 [06:18<26:23,  6.26s/it]test:62/314 | accuracy 1749  0.8815524193548387:  20%|█▉        | 62/314 [06:18<27:04,  6.45s/it]test:63/314 | accuracy 1775  0.8804563492063492:  20%|█▉        | 62/314 [06:24<27:04,  6.45s/it]test:63/314 | accuracy 1775  0.8804563492063492:  20%|██        | 63/314 [06:24<25:41,  6.14s/it]test:64/314 | accuracy 1803  0.88037109375:  20%|██        | 63/314 [06:29<25:41,  6.14s/it]     test:64/314 | accuracy 1803  0.88037109375:  20%|██        | 64/314 [06:29<25:00,  6.00s/it]test:65/314 | accuracy 1827  0.8783653846153846:  20%|██        | 64/314 [06:36<25:00,  6.00s/it]test:65/314 | accuracy 1827  0.8783653846153846:  21%|██        | 65/314 [06:36<25:33,  6.16s/it]test:66/314 | accuracy 1853  0.8773674242424242:  21%|██        | 65/314 [06:41<25:33,  6.16s/it]test:66/314 | accuracy 1853  0.8773674242424242:  21%|██        | 66/314 [06:41<24:34,  5.94s/it]test:67/314 | accuracy 1883  0.8782649253731343:  21%|██        | 66/314 [06:47<24:34,  5.94s/it]test:67/314 | accuracy 1883  0.8782649253731343:  21%|██▏       | 67/314 [06:47<24:08,  5.87s/it]test:68/314 | accuracy 1914  0.8795955882352942:  21%|██▏       | 67/314 [06:55<24:08,  5.87s/it]test:68/314 | accuracy 1914  0.8795955882352942:  22%|██▏       | 68/314 [06:55<26:06,  6.37s/it]test:69/314 | accuracy 1943  0.8799818840579711:  22%|██▏       | 68/314 [07:00<26:06,  6.37s/it]test:69/314 | accuracy 1943  0.8799818840579711:  22%|██▏       | 69/314 [07:00<25:12,  6.18s/it]test:70/314 | accuracy 1967  0.878125:  22%|██▏       | 69/314 [07:06<25:12,  6.18s/it]          test:70/314 | accuracy 1967  0.878125:  22%|██▏       | 70/314 [07:06<24:07,  5.93s/it]test:71/314 | accuracy 1997  0.8789612676056338:  22%|██▏       | 70/314 [07:11<24:07,  5.93s/it]test:71/314 | accuracy 1997  0.8789612676056338:  23%|██▎       | 71/314 [07:11<23:13,  5.73s/it]test:72/314 | accuracy 2026  0.8793402777777778:  23%|██▎       | 71/314 [07:16<23:13,  5.73s/it]test:72/314 | accuracy 2026  0.8793402777777778:  23%|██▎       | 72/314 [07:17<22:49,  5.66s/it]test:73/314 | accuracy 2054  0.8792808219178082:  23%|██▎       | 72/314 [07:23<22:49,  5.66s/it]test:73/314 | accuracy 2054  0.8792808219178082:  23%|██▎       | 73/314 [07:23<23:28,  5.85s/it]test:74/314 | accuracy 2082  0.879222972972973:  23%|██▎       | 73/314 [07:28<23:28,  5.85s/it] test:74/314 | accuracy 2082  0.879222972972973:  24%|██▎       | 74/314 [07:28<22:22,  5.59s/it]test:75/314 | accuracy 2110  0.8791666666666667:  24%|██▎       | 74/314 [07:34<22:22,  5.59s/it]test:75/314 | accuracy 2110  0.8791666666666667:  24%|██▍       | 75/314 [07:34<22:37,  5.68s/it]test:76/314 | accuracy 2138  0.8791118421052632:  24%|██▍       | 75/314 [07:39<22:37,  5.68s/it]test:76/314 | accuracy 2138  0.8791118421052632:  24%|██▍       | 76/314 [07:39<22:25,  5.65s/it]test:77/314 | accuracy 2165  0.8786525974025974:  24%|██▍       | 76/314 [07:44<22:25,  5.65s/it]test:77/314 | accuracy 2165  0.8786525974025974:  25%|██▍       | 77/314 [07:44<21:42,  5.50s/it]test:78/314 | accuracy 2195  0.8794070512820513:  25%|██▍       | 77/314 [07:50<21:42,  5.50s/it]test:78/314 | accuracy 2195  0.8794070512820513:  25%|██▍       | 78/314 [07:50<22:06,  5.62s/it]test:79/314 | accuracy 2225  0.8801424050632911:  25%|██▍       | 78/314 [07:57<22:06,  5.62s/it]test:79/314 | accuracy 2225  0.8801424050632911:  25%|██▌       | 79/314 [07:57<23:18,  5.95s/it]test:80/314 | accuracy 2255  0.880859375:  25%|██▌       | 79/314 [08:03<23:18,  5.95s/it]       test:80/314 | accuracy 2255  0.880859375:  25%|██▌       | 80/314 [08:03<23:07,  5.93s/it]test:81/314 | accuracy 2283  0.8807870370370371:  25%|██▌       | 80/314 [08:09<23:07,  5.93s/it]test:81/314 | accuracy 2283  0.8807870370370371:  26%|██▌       | 81/314 [08:09<22:59,  5.92s/it]test:82/314 | accuracy 2314  0.881859756097561:  26%|██▌       | 81/314 [08:15<22:59,  5.92s/it] test:82/314 | accuracy 2314  0.881859756097561:  26%|██▌       | 82/314 [08:15<22:47,  5.89s/it]test:83/314 | accuracy 2345  0.8829066265060241:  26%|██▌       | 82/314 [08:21<22:47,  5.89s/it]test:83/314 | accuracy 2345  0.8829066265060241:  26%|██▋       | 83/314 [08:22<23:49,  6.19s/it]test:84/314 | accuracy 2375  0.8835565476190477:  26%|██▋       | 83/314 [08:28<23:49,  6.19s/it]test:84/314 | accuracy 2375  0.8835565476190477:  27%|██▋       | 84/314 [08:28<24:36,  6.42s/it]test:85/314 | accuracy 2404  0.8838235294117647:  27%|██▋       | 84/314 [08:35<24:36,  6.42s/it]test:85/314 | accuracy 2404  0.8838235294117647:  27%|██▋       | 85/314 [08:35<24:19,  6.37s/it]test:86/314 | accuracy 2433  0.8840843023255814:  27%|██▋       | 85/314 [08:41<24:19,  6.37s/it]test:86/314 | accuracy 2433  0.8840843023255814:  27%|██▋       | 86/314 [08:41<24:15,  6.38s/it]test:87/314 | accuracy 2461  0.8839798850574713:  27%|██▋       | 86/314 [08:47<24:15,  6.38s/it]test:87/314 | accuracy 2461  0.8839798850574713:  28%|██▊       | 87/314 [08:47<23:57,  6.33s/it]test:88/314 | accuracy 2490  0.8842329545454546:  28%|██▊       | 87/314 [08:53<23:57,  6.33s/it]test:88/314 | accuracy 2490  0.8842329545454546:  28%|██▊       | 88/314 [08:53<22:46,  6.04s/it]test:89/314 | accuracy 2516  0.8834269662921348:  28%|██▊       | 88/314 [08:59<22:46,  6.04s/it]test:89/314 | accuracy 2516  0.8834269662921348:  28%|██▊       | 89/314 [08:59<22:32,  6.01s/it]test:90/314 | accuracy 2545  0.8836805555555556:  28%|██▊       | 89/314 [09:05<22:32,  6.01s/it]test:90/314 | accuracy 2545  0.8836805555555556:  29%|██▊       | 90/314 [09:05<22:25,  6.01s/it]test:91/314 | accuracy 2572  0.8832417582417582:  29%|██▊       | 90/314 [09:11<22:25,  6.01s/it]test:91/314 | accuracy 2572  0.8832417582417582:  29%|██▉       | 91/314 [09:11<22:17,  6.00s/it]test:92/314 | accuracy 2601  0.8834918478260869:  29%|██▉       | 91/314 [09:16<22:17,  6.00s/it]test:92/314 | accuracy 2601  0.8834918478260869:  29%|██▉       | 92/314 [09:16<21:42,  5.87s/it]test:93/314 | accuracy 2631  0.8840725806451613:  29%|██▉       | 92/314 [09:23<21:42,  5.87s/it]test:93/314 | accuracy 2631  0.8840725806451613:  30%|██▉       | 93/314 [09:23<22:14,  6.04s/it]test:94/314 | accuracy 2661  0.8846409574468085:  30%|██▉       | 93/314 [09:29<22:14,  6.04s/it]test:94/314 | accuracy 2661  0.8846409574468085:  30%|██▉       | 94/314 [09:29<22:05,  6.02s/it]test:95/314 | accuracy 2688  0.8842105263157894:  30%|██▉       | 94/314 [09:34<22:05,  6.02s/it]test:95/314 | accuracy 2688  0.8842105263157894:  30%|███       | 95/314 [09:34<21:11,  5.81s/it]test:96/314 | accuracy 2717  0.8844401041666666:  30%|███       | 95/314 [09:39<21:11,  5.81s/it]test:96/314 | accuracy 2717  0.8844401041666666:  31%|███       | 96/314 [09:40<20:53,  5.75s/it]test:97/314 | accuracy 2745  0.8843427835051546:  31%|███       | 96/314 [09:45<20:53,  5.75s/it]test:97/314 | accuracy 2745  0.8843427835051546:  31%|███       | 97/314 [09:45<20:40,  5.72s/it]test:98/314 | accuracy 2767  0.8823341836734694:  31%|███       | 97/314 [09:51<20:40,  5.72s/it]test:98/314 | accuracy 2767  0.8823341836734694:  31%|███       | 98/314 [09:52<21:13,  5.89s/it]test:99/314 | accuracy 2796  0.8825757575757576:  31%|███       | 98/314 [09:58<21:13,  5.89s/it]test:99/314 | accuracy 2796  0.8825757575757576:  32%|███▏      | 99/314 [09:58<21:18,  5.95s/it]test:100/314 | accuracy 2823  0.8821875:  32%|███▏      | 99/314 [10:04<21:18,  5.95s/it]        test:100/314 | accuracy 2823  0.8821875:  32%|███▏      | 100/314 [10:04<21:56,  6.15s/it]test:101/314 | accuracy 2852  0.8824257425742574:  32%|███▏      | 100/314 [10:10<21:56,  6.15s/it]test:101/314 | accuracy 2852  0.8824257425742574:  32%|███▏      | 101/314 [10:10<21:28,  6.05s/it]test:102/314 | accuracy 2881  0.8826593137254902:  32%|███▏      | 101/314 [10:19<21:28,  6.05s/it]test:102/314 | accuracy 2881  0.8826593137254902:  32%|███▏      | 102/314 [10:19<24:56,  7.06s/it]test:103/314 | accuracy 2910  0.8828883495145631:  32%|███▏      | 102/314 [10:29<24:56,  7.06s/it]test:103/314 | accuracy 2910  0.8828883495145631:  33%|███▎      | 103/314 [10:29<27:40,  7.87s/it]test:104/314 | accuracy 2942  0.8840144230769231:  33%|███▎      | 103/314 [10:38<27:40,  7.87s/it]test:104/314 | accuracy 2942  0.8840144230769231:  33%|███▎      | 104/314 [10:38<28:57,  8.27s/it]test:105/314 | accuracy 2974  0.8851190476190476:  33%|███▎      | 104/314 [10:47<28:57,  8.27s/it]test:105/314 | accuracy 2974  0.8851190476190476:  33%|███▎      | 105/314 [10:48<29:42,  8.53s/it]test:106/314 | accuracy 3006  0.8862028301886793:  33%|███▎      | 105/314 [10:57<29:42,  8.53s/it]test:106/314 | accuracy 3006  0.8862028301886793:  34%|███▍      | 106/314 [10:57<30:11,  8.71s/it]test:107/314 | accuracy 3037  0.8869742990654206:  34%|███▍      | 106/314 [11:06<30:11,  8.71s/it]test:107/314 | accuracy 3037  0.8869742990654206:  34%|███▍      | 107/314 [11:06<30:21,  8.80s/it]test:108/314 | accuracy 3069  0.8880208333333334:  34%|███▍      | 107/314 [11:14<30:21,  8.80s/it]test:108/314 | accuracy 3069  0.8880208333333334:  34%|███▍      | 108/314 [11:14<30:10,  8.79s/it]test:109/314 | accuracy 3101  0.8890481651376146:  34%|███▍      | 108/314 [11:23<30:10,  8.79s/it]test:109/314 | accuracy 3101  0.8890481651376146:  35%|███▍      | 109/314 [11:24<30:21,  8.89s/it]test:110/314 | accuracy 3132  0.8897727272727273:  35%|███▍      | 109/314 [11:33<30:21,  8.89s/it]test:110/314 | accuracy 3132  0.8897727272727273:  35%|███▌      | 110/314 [11:33<30:37,  9.01s/it]test:111/314 | accuracy 3162  0.8902027027027027:  35%|███▌      | 110/314 [11:42<30:37,  9.01s/it]test:111/314 | accuracy 3162  0.8902027027027027:  35%|███▌      | 111/314 [11:42<31:02,  9.17s/it]test:112/314 | accuracy 3192  0.890625:  35%|███▌      | 111/314 [11:52<31:02,  9.17s/it]          test:112/314 | accuracy 3192  0.890625:  36%|███▌      | 112/314 [11:52<30:52,  9.17s/it]test:113/314 | accuracy 3223  0.891316371681416:  36%|███▌      | 112/314 [12:02<30:52,  9.17s/it]test:113/314 | accuracy 3223  0.891316371681416:  36%|███▌      | 113/314 [12:02<31:55,  9.53s/it]test:114/314 | accuracy 3252  0.8914473684210527:  36%|███▌      | 113/314 [12:11<31:55,  9.53s/it]test:114/314 | accuracy 3252  0.8914473684210527:  36%|███▋      | 114/314 [12:11<31:27,  9.44s/it]test:115/314 | accuracy 3283  0.8921195652173913:  36%|███▋      | 114/314 [12:20<31:27,  9.44s/it]test:115/314 | accuracy 3283  0.8921195652173913:  37%|███▋      | 115/314 [12:21<31:14,  9.42s/it]test:116/314 | accuracy 3314  0.8927801724137931:  37%|███▋      | 115/314 [12:29<31:14,  9.42s/it]test:116/314 | accuracy 3314  0.8927801724137931:  37%|███▋      | 116/314 [12:29<30:24,  9.21s/it]test:117/314 | accuracy 3345  0.8934294871794872:  37%|███▋      | 116/314 [12:38<30:24,  9.21s/it]test:117/314 | accuracy 3345  0.8934294871794872:  37%|███▋      | 117/314 [12:38<29:55,  9.11s/it]test:118/314 | accuracy 3375  0.893802966101695:  37%|███▋      | 117/314 [12:47<29:55,  9.11s/it] test:118/314 | accuracy 3375  0.893802966101695:  38%|███▊      | 118/314 [12:48<30:04,  9.21s/it]test:119/314 | accuracy 3406  0.8944327731092437:  38%|███▊      | 118/314 [12:57<30:04,  9.21s/it]test:119/314 | accuracy 3406  0.8944327731092437:  38%|███▊      | 119/314 [12:57<30:18,  9.32s/it]test:120/314 | accuracy 3436  0.8947916666666667:  38%|███▊      | 119/314 [13:06<30:18,  9.32s/it]test:120/314 | accuracy 3436  0.8947916666666667:  38%|███▊      | 120/314 [13:06<30:00,  9.28s/it]test:121/314 | accuracy 3465  0.8948863636363636:  38%|███▊      | 120/314 [13:15<30:00,  9.28s/it]test:121/314 | accuracy 3465  0.8948863636363636:  39%|███▊      | 121/314 [13:15<29:34,  9.20s/it]test:122/314 | accuracy 3496  0.8954918032786885:  39%|███▊      | 121/314 [13:25<29:34,  9.20s/it]test:122/314 | accuracy 3496  0.8954918032786885:  39%|███▉      | 122/314 [13:25<29:33,  9.23s/it]test:123/314 | accuracy 3527  0.8960873983739838:  39%|███▉      | 122/314 [13:34<29:33,  9.23s/it]test:123/314 | accuracy 3527  0.8960873983739838:  39%|███▉      | 123/314 [13:34<29:37,  9.31s/it]test:124/314 | accuracy 3558  0.8966733870967742:  39%|███▉      | 123/314 [13:43<29:37,  9.31s/it]test:124/314 | accuracy 3558  0.8966733870967742:  39%|███▉      | 124/314 [13:43<29:07,  9.20s/it]test:125/314 | accuracy 3589  0.89725:  39%|███▉      | 124/314 [13:53<29:07,  9.20s/it]           test:125/314 | accuracy 3589  0.89725:  40%|███▉      | 125/314 [13:53<29:18,  9.30s/it]test:126/314 | accuracy 3617  0.8970734126984127:  40%|███▉      | 125/314 [14:02<29:18,  9.30s/it]test:126/314 | accuracy 3617  0.8970734126984127:  40%|████      | 126/314 [14:02<29:06,  9.29s/it]test:127/314 | accuracy 3648  0.8976377952755905:  40%|████      | 126/314 [14:12<29:06,  9.29s/it]test:127/314 | accuracy 3648  0.8976377952755905:  40%|████      | 127/314 [14:12<29:19,  9.41s/it]test:128/314 | accuracy 3678  0.89794921875:  40%|████      | 127/314 [14:22<29:19,  9.41s/it]     test:128/314 | accuracy 3678  0.89794921875:  41%|████      | 128/314 [14:22<30:18,  9.78s/it]test:129/314 | accuracy 3709  0.8984980620155039:  41%|████      | 128/314 [14:32<30:18,  9.78s/it]test:129/314 | accuracy 3709  0.8984980620155039:  41%|████      | 129/314 [14:32<29:47,  9.66s/it]test:130/314 | accuracy 3739  0.8987980769230769:  41%|████      | 129/314 [14:40<29:47,  9.66s/it]test:130/314 | accuracy 3739  0.8987980769230769:  41%|████▏     | 130/314 [14:40<28:48,  9.40s/it]test:131/314 | accuracy 3770  0.8993320610687023:  41%|████▏     | 130/314 [14:49<28:48,  9.40s/it]test:131/314 | accuracy 3770  0.8993320610687023:  42%|████▏     | 131/314 [14:50<28:25,  9.32s/it]test:132/314 | accuracy 3800  0.8996212121212122:  42%|████▏     | 131/314 [14:58<28:25,  9.32s/it]test:132/314 | accuracy 3800  0.8996212121212122:  42%|████▏     | 132/314 [14:58<27:52,  9.19s/it]test:133/314 | accuracy 3832  0.900375939849624:  42%|████▏     | 132/314 [15:08<27:52,  9.19s/it] test:133/314 | accuracy 3832  0.900375939849624:  42%|████▏     | 133/314 [15:08<28:07,  9.32s/it]test:134/314 | accuracy 3862  0.9006529850746269:  42%|████▏     | 133/314 [15:17<28:07,  9.32s/it]test:134/314 | accuracy 3862  0.9006529850746269:  43%|████▎     | 134/314 [15:17<27:49,  9.28s/it]test:135/314 | accuracy 3892  0.9009259259259259:  43%|████▎     | 134/314 [15:26<27:49,  9.28s/it]test:135/314 | accuracy 3892  0.9009259259259259:  43%|████▎     | 135/314 [15:26<27:29,  9.22s/it]test:136/314 | accuracy 3924  0.9016544117647058:  43%|████▎     | 135/314 [15:36<27:29,  9.22s/it]test:136/314 | accuracy 3924  0.9016544117647058:  43%|████▎     | 136/314 [15:36<27:49,  9.38s/it]test:137/314 | accuracy 3956  0.9023722627737226:  43%|████▎     | 136/314 [15:45<27:49,  9.38s/it]test:137/314 | accuracy 3956  0.9023722627737226:  44%|████▎     | 137/314 [15:45<27:02,  9.17s/it]test:138/314 | accuracy 3987  0.9028532608695652:  44%|████▎     | 137/314 [15:54<27:02,  9.17s/it]test:138/314 | accuracy 3987  0.9028532608695652:  44%|████▍     | 138/314 [15:54<26:58,  9.20s/it]test:139/314 | accuracy 4017  0.9031025179856115:  44%|████▍     | 138/314 [16:03<26:58,  9.20s/it]test:139/314 | accuracy 4017  0.9031025179856115:  44%|████▍     | 139/314 [16:03<26:28,  9.08s/it]test:140/314 | accuracy 4049  0.9037946428571428:  44%|████▍     | 139/314 [16:11<26:28,  9.08s/it]test:140/314 | accuracy 4049  0.9037946428571428:  45%|████▍     | 140/314 [16:11<25:58,  8.96s/it]test:141/314 | accuracy 4079  0.9040336879432624:  45%|████▍     | 140/314 [16:21<25:58,  8.96s/it]test:141/314 | accuracy 4079  0.9040336879432624:  45%|████▍     | 141/314 [16:21<26:03,  9.04s/it]test:142/314 | accuracy 4110  0.9044894366197183:  45%|████▍     | 141/314 [16:30<26:03,  9.04s/it]test:142/314 | accuracy 4110  0.9044894366197183:  45%|████▌     | 142/314 [16:30<26:15,  9.16s/it]test:143/314 | accuracy 4138  0.9042832167832168:  45%|████▌     | 142/314 [16:39<26:15,  9.16s/it]test:143/314 | accuracy 4138  0.9042832167832168:  46%|████▌     | 143/314 [16:39<26:01,  9.13s/it]test:144/314 | accuracy 4170  0.9049479166666666:  46%|████▌     | 143/314 [16:49<26:01,  9.13s/it]test:144/314 | accuracy 4170  0.9049479166666666:  46%|████▌     | 144/314 [16:49<26:05,  9.21s/it]test:145/314 | accuracy 4202  0.905603448275862:  46%|████▌     | 144/314 [16:58<26:05,  9.21s/it] test:145/314 | accuracy 4202  0.905603448275862:  46%|████▌     | 145/314 [16:58<25:56,  9.21s/it]test:146/314 | accuracy 4232  0.9058219178082192:  46%|████▌     | 145/314 [17:07<25:56,  9.21s/it]test:146/314 | accuracy 4232  0.9058219178082192:  46%|████▋     | 146/314 [17:07<25:47,  9.21s/it]test:147/314 | accuracy 4262  0.9060374149659864:  46%|████▋     | 146/314 [17:18<25:47,  9.21s/it]test:147/314 | accuracy 4262  0.9060374149659864:  47%|████▋     | 147/314 [17:18<27:13,  9.78s/it]test:148/314 | accuracy 4291  0.9060388513513513:  47%|████▋     | 147/314 [17:27<27:13,  9.78s/it]test:148/314 | accuracy 4291  0.9060388513513513:  47%|████▋     | 148/314 [17:27<26:22,  9.53s/it]test:149/314 | accuracy 4321  0.90625:  47%|████▋     | 148/314 [17:36<26:22,  9.53s/it]           test:149/314 | accuracy 4321  0.90625:  47%|████▋     | 149/314 [17:36<25:39,  9.33s/it]test:150/314 | accuracy 4352  0.9066666666666666:  47%|████▋     | 149/314 [17:46<25:39,  9.33s/it]test:150/314 | accuracy 4352  0.9066666666666666:  48%|████▊     | 150/314 [17:46<26:29,  9.69s/it]test:151/314 | accuracy 4382  0.9068708609271523:  48%|████▊     | 150/314 [17:56<26:29,  9.69s/it]test:151/314 | accuracy 4382  0.9068708609271523:  48%|████▊     | 151/314 [17:56<26:07,  9.62s/it]test:152/314 | accuracy 4413  0.9072779605263158:  48%|████▊     | 151/314 [18:05<26:07,  9.62s/it]test:152/314 | accuracy 4413  0.9072779605263158:  48%|████▊     | 152/314 [18:05<25:40,  9.51s/it]test:153/314 | accuracy 4444  0.9076797385620915:  48%|████▊     | 152/314 [18:15<25:40,  9.51s/it]test:153/314 | accuracy 4444  0.9076797385620915:  49%|████▊     | 153/314 [18:15<25:32,  9.52s/it]test:154/314 | accuracy 4476  0.9082792207792207:  49%|████▊     | 153/314 [18:24<25:32,  9.52s/it]test:154/314 | accuracy 4476  0.9082792207792207:  49%|████▉     | 154/314 [18:24<24:56,  9.35s/it]test:155/314 | accuracy 4507  0.9086693548387097:  49%|████▉     | 154/314 [18:33<24:56,  9.35s/it]test:155/314 | accuracy 4507  0.9086693548387097:  49%|████▉     | 155/314 [18:33<24:39,  9.31s/it]test:156/314 | accuracy 4538  0.9090544871794872:  49%|████▉     | 155/314 [18:42<24:39,  9.31s/it]test:156/314 | accuracy 4538  0.9090544871794872:  50%|████▉     | 156/314 [18:42<24:41,  9.38s/it]test:157/314 | accuracy 4569  0.9094347133757962:  50%|████▉     | 156/314 [18:52<24:41,  9.38s/it]test:157/314 | accuracy 4569  0.9094347133757962:  50%|█████     | 157/314 [18:52<24:32,  9.38s/it]test:158/314 | accuracy 4600  0.9098101265822784:  50%|█████     | 157/314 [19:01<24:32,  9.38s/it]test:158/314 | accuracy 4600  0.9098101265822784:  50%|█████     | 158/314 [19:01<24:06,  9.27s/it]test:159/314 | accuracy 4631  0.9101808176100629:  50%|█████     | 158/314 [19:10<24:06,  9.27s/it]test:159/314 | accuracy 4631  0.9101808176100629:  51%|█████     | 159/314 [19:11<24:15,  9.39s/it]test:160/314 | accuracy 4663  0.9107421875:  51%|█████     | 159/314 [19:20<24:15,  9.39s/it]      test:160/314 | accuracy 4663  0.9107421875:  51%|█████     | 160/314 [19:20<24:06,  9.40s/it]test:161/314 | accuracy 4694  0.9111024844720497:  51%|█████     | 160/314 [19:29<24:06,  9.40s/it]test:161/314 | accuracy 4694  0.9111024844720497:  51%|█████▏    | 161/314 [19:30<24:07,  9.46s/it]test:162/314 | accuracy 4726  0.9116512345679012:  51%|█████▏    | 161/314 [19:38<24:07,  9.46s/it]test:162/314 | accuracy 4726  0.9116512345679012:  52%|█████▏    | 162/314 [19:38<23:33,  9.30s/it]test:163/314 | accuracy 4757  0.9120015337423313:  52%|█████▏    | 162/314 [19:48<23:33,  9.30s/it]test:163/314 | accuracy 4757  0.9120015337423313:  52%|█████▏    | 163/314 [19:48<23:24,  9.30s/it]test:164/314 | accuracy 4787  0.9121570121951219:  52%|█████▏    | 163/314 [19:57<23:24,  9.30s/it]test:164/314 | accuracy 4787  0.9121570121951219:  52%|█████▏    | 164/314 [19:57<23:12,  9.28s/it]test:165/314 | accuracy 4817  0.9123106060606061:  52%|█████▏    | 164/314 [20:07<23:12,  9.28s/it]test:165/314 | accuracy 4817  0.9123106060606061:  53%|█████▎    | 165/314 [20:07<23:24,  9.42s/it]test:166/314 | accuracy 4846  0.9122740963855421:  53%|█████▎    | 165/314 [20:16<23:24,  9.42s/it]test:166/314 | accuracy 4846  0.9122740963855421:  53%|█████▎    | 166/314 [20:16<23:11,  9.40s/it]test:167/314 | accuracy 4878  0.9127994011976048:  53%|█████▎    | 166/314 [20:25<23:11,  9.40s/it]test:167/314 | accuracy 4878  0.9127994011976048:  53%|█████▎    | 167/314 [20:25<22:55,  9.36s/it]test:168/314 | accuracy 4908  0.9129464285714286:  53%|█████▎    | 167/314 [20:34<22:55,  9.36s/it]test:168/314 | accuracy 4908  0.9129464285714286:  54%|█████▎    | 168/314 [20:34<22:26,  9.22s/it]test:169/314 | accuracy 4939  0.9132766272189349:  54%|█████▎    | 168/314 [20:44<22:26,  9.22s/it]test:169/314 | accuracy 4939  0.9132766272189349:  54%|█████▍    | 169/314 [20:44<22:24,  9.27s/it]test:170/314 | accuracy 4970  0.9136029411764706:  54%|█████▍    | 169/314 [20:54<22:24,  9.27s/it]test:170/314 | accuracy 4970  0.9136029411764706:  54%|█████▍    | 170/314 [20:54<22:50,  9.52s/it]test:171/314 | accuracy 5001  0.9139254385964912:  54%|█████▍    | 170/314 [21:03<22:50,  9.52s/it]test:171/314 | accuracy 5001  0.9139254385964912:  54%|█████▍    | 171/314 [21:03<22:32,  9.46s/it]test:172/314 | accuracy 5032  0.9142441860465116:  54%|█████▍    | 171/314 [21:12<22:32,  9.46s/it]test:172/314 | accuracy 5032  0.9142441860465116:  55%|█████▍    | 172/314 [21:12<22:13,  9.39s/it]test:173/314 | accuracy 5063  0.9145592485549133:  55%|█████▍    | 172/314 [21:21<22:13,  9.39s/it]test:173/314 | accuracy 5063  0.9145592485549133:  55%|█████▌    | 173/314 [21:21<21:53,  9.32s/it]test:174/314 | accuracy 5095  0.9150502873563219:  55%|█████▌    | 173/314 [21:30<21:53,  9.32s/it]test:174/314 | accuracy 5095  0.9150502873563219:  55%|█████▌    | 174/314 [21:30<21:28,  9.20s/it]test:175/314 | accuracy 5125  0.9151785714285714:  55%|█████▌    | 174/314 [21:40<21:28,  9.20s/it]test:175/314 | accuracy 5125  0.9151785714285714:  56%|█████▌    | 175/314 [21:40<21:26,  9.26s/it]test:176/314 | accuracy 5156  0.9154829545454546:  56%|█████▌    | 175/314 [21:49<21:26,  9.26s/it]test:176/314 | accuracy 5156  0.9154829545454546:  56%|█████▌    | 176/314 [21:49<21:25,  9.32s/it]test:177/314 | accuracy 5187  0.9157838983050848:  56%|█████▌    | 176/314 [21:58<21:25,  9.32s/it]test:177/314 | accuracy 5187  0.9157838983050848:  56%|█████▋    | 177/314 [21:59<21:16,  9.32s/it]test:178/314 | accuracy 5218  0.9160814606741573:  56%|█████▋    | 177/314 [22:08<21:16,  9.32s/it]test:178/314 | accuracy 5218  0.9160814606741573:  57%|█████▋    | 178/314 [22:08<21:03,  9.29s/it]test:179/314 | accuracy 5248  0.9162011173184358:  57%|█████▋    | 178/314 [22:17<21:03,  9.29s/it]test:179/314 | accuracy 5248  0.9162011173184358:  57%|█████▋    | 179/314 [22:17<21:01,  9.34s/it]test:180/314 | accuracy 5279  0.9164930555555556:  57%|█████▋    | 179/314 [22:27<21:01,  9.34s/it]test:180/314 | accuracy 5279  0.9164930555555556:  57%|█████▋    | 180/314 [22:27<21:05,  9.44s/it]test:181/314 | accuracy 5311  0.9169544198895028:  57%|█████▋    | 180/314 [22:36<21:05,  9.44s/it]test:181/314 | accuracy 5311  0.9169544198895028:  58%|█████▊    | 181/314 [22:36<20:57,  9.46s/it]test:182/314 | accuracy 5342  0.917239010989011:  58%|█████▊    | 181/314 [22:46<20:57,  9.46s/it] test:182/314 | accuracy 5342  0.917239010989011:  58%|█████▊    | 182/314 [22:46<20:45,  9.44s/it]test:183/314 | accuracy 5374  0.917691256830601:  58%|█████▊    | 182/314 [22:55<20:45,  9.44s/it]test:183/314 | accuracy 5374  0.917691256830601:  58%|█████▊    | 183/314 [22:55<20:15,  9.28s/it]test:184/314 | accuracy 5406  0.9181385869565217:  58%|█████▊    | 183/314 [23:04<20:15,  9.28s/it]test:184/314 | accuracy 5406  0.9181385869565217:  59%|█████▊    | 184/314 [23:04<20:21,  9.39s/it]test:185/314 | accuracy 5438  0.918581081081081:  59%|█████▊    | 184/314 [23:14<20:21,  9.39s/it] test:185/314 | accuracy 5438  0.918581081081081:  59%|█████▉    | 185/314 [23:14<20:15,  9.43s/it]test:186/314 | accuracy 5469  0.9188508064516129:  59%|█████▉    | 185/314 [23:23<20:15,  9.43s/it]test:186/314 | accuracy 5469  0.9188508064516129:  59%|█████▉    | 186/314 [23:23<19:51,  9.31s/it]test:187/314 | accuracy 5499  0.9189505347593583:  59%|█████▉    | 186/314 [23:32<19:51,  9.31s/it]test:187/314 | accuracy 5499  0.9189505347593583:  60%|█████▉    | 187/314 [23:32<19:45,  9.33s/it]test:188/314 | accuracy 5530  0.9192154255319149:  60%|█████▉    | 187/314 [23:41<19:45,  9.33s/it]test:188/314 | accuracy 5530  0.9192154255319149:  60%|█████▉    | 188/314 [23:41<19:25,  9.25s/it]test:189/314 | accuracy 5561  0.9194775132275133:  60%|█████▉    | 188/314 [23:50<19:25,  9.25s/it]test:189/314 | accuracy 5561  0.9194775132275133:  60%|██████    | 189/314 [23:51<19:14,  9.24s/it]test:190/314 | accuracy 5593  0.9199013157894737:  60%|██████    | 189/314 [23:59<19:14,  9.24s/it]test:190/314 | accuracy 5593  0.9199013157894737:  61%|██████    | 190/314 [24:00<18:59,  9.19s/it]test:191/314 | accuracy 5623  0.9199934554973822:  61%|██████    | 190/314 [24:09<18:59,  9.19s/it]test:191/314 | accuracy 5623  0.9199934554973822:  61%|██████    | 191/314 [24:09<18:52,  9.21s/it]test:192/314 | accuracy 5653  0.9200846354166666:  61%|██████    | 191/314 [24:18<18:52,  9.21s/it]test:192/314 | accuracy 5653  0.9200846354166666:  61%|██████    | 192/314 [24:18<18:33,  9.13s/it]test:193/314 | accuracy 5684  0.9203367875647669:  61%|██████    | 192/314 [24:27<18:33,  9.13s/it]test:193/314 | accuracy 5684  0.9203367875647669:  61%|██████▏   | 193/314 [24:27<18:27,  9.15s/it]test:194/314 | accuracy 5715  0.9205863402061856:  61%|██████▏   | 193/314 [24:36<18:27,  9.15s/it]test:194/314 | accuracy 5715  0.9205863402061856:  62%|██████▏   | 194/314 [24:36<18:28,  9.23s/it]test:195/314 | accuracy 5746  0.9208333333333333:  62%|██████▏   | 194/314 [24:45<18:28,  9.23s/it]test:195/314 | accuracy 5746  0.9208333333333333:  62%|██████▏   | 195/314 [24:45<18:09,  9.16s/it]test:196/314 | accuracy 5778  0.9212372448979592:  62%|██████▏   | 195/314 [24:55<18:09,  9.16s/it]test:196/314 | accuracy 5778  0.9212372448979592:  62%|██████▏   | 196/314 [24:55<18:11,  9.25s/it]test:197/314 | accuracy 5808  0.9213197969543148:  62%|██████▏   | 196/314 [25:04<18:11,  9.25s/it]test:197/314 | accuracy 5808  0.9213197969543148:  63%|██████▎   | 197/314 [25:04<17:45,  9.11s/it]test:198/314 | accuracy 5838  0.9214015151515151:  63%|██████▎   | 197/314 [25:13<17:45,  9.11s/it]test:198/314 | accuracy 5838  0.9214015151515151:  63%|██████▎   | 198/314 [25:13<17:34,  9.09s/it]test:199/314 | accuracy 5869  0.9216394472361809:  63%|██████▎   | 198/314 [25:22<17:34,  9.09s/it]test:199/314 | accuracy 5869  0.9216394472361809:  63%|██████▎   | 199/314 [25:22<17:23,  9.08s/it]test:200/314 | accuracy 5900  0.921875:  63%|██████▎   | 199/314 [25:31<17:23,  9.08s/it]          test:200/314 | accuracy 5900  0.921875:  64%|██████▎   | 200/314 [25:31<17:20,  9.12s/it]test:201/314 | accuracy 5931  0.9221082089552238:  64%|██████▎   | 200/314 [25:40<17:20,  9.12s/it]test:201/314 | accuracy 5931  0.9221082089552238:  64%|██████▍   | 201/314 [25:40<17:04,  9.06s/it]test:202/314 | accuracy 5961  0.922184405940594:  64%|██████▍   | 201/314 [25:49<17:04,  9.06s/it] test:202/314 | accuracy 5961  0.922184405940594:  64%|██████▍   | 202/314 [25:49<16:53,  9.05s/it]test:203/314 | accuracy 5992  0.9224137931034483:  64%|██████▍   | 202/314 [25:58<16:53,  9.05s/it]test:203/314 | accuracy 5992  0.9224137931034483:  65%|██████▍   | 203/314 [25:58<16:54,  9.14s/it]test:204/314 | accuracy 6023  0.922640931372549:  65%|██████▍   | 203/314 [26:07<16:54,  9.14s/it] test:204/314 | accuracy 6023  0.922640931372549:  65%|██████▍   | 204/314 [26:07<16:40,  9.10s/it]test:205/314 | accuracy 6055  0.9230182926829268:  65%|██████▍   | 204/314 [26:17<16:40,  9.10s/it]test:205/314 | accuracy 6055  0.9230182926829268:  65%|██████▌   | 205/314 [26:17<16:47,  9.24s/it]test:206/314 | accuracy 6087  0.9233919902912622:  65%|██████▌   | 205/314 [26:26<16:47,  9.24s/it]test:206/314 | accuracy 6087  0.9233919902912622:  66%|██████▌   | 206/314 [26:26<16:44,  9.30s/it]test:207/314 | accuracy 6118  0.9236111111111112:  66%|██████▌   | 206/314 [26:36<16:44,  9.30s/it]test:207/314 | accuracy 6118  0.9236111111111112:  66%|██████▌   | 207/314 [26:36<16:41,  9.36s/it]test:208/314 | accuracy 6145  0.9232271634615384:  66%|██████▌   | 207/314 [26:45<16:41,  9.36s/it]test:208/314 | accuracy 6145  0.9232271634615384:  66%|██████▌   | 208/314 [26:45<16:22,  9.27s/it]test:209/314 | accuracy 6177  0.9235944976076556:  66%|██████▌   | 208/314 [26:54<16:22,  9.27s/it]test:209/314 | accuracy 6177  0.9235944976076556:  67%|██████▋   | 209/314 [26:54<16:13,  9.27s/it]test:210/314 | accuracy 6209  0.9239583333333333:  67%|██████▋   | 209/314 [27:03<16:13,  9.27s/it]test:210/314 | accuracy 6209  0.9239583333333333:  67%|██████▋   | 210/314 [27:03<16:03,  9.27s/it]test:211/314 | accuracy 6241  0.924318720379147:  67%|██████▋   | 210/314 [27:13<16:03,  9.27s/it] test:211/314 | accuracy 6241  0.924318720379147:  67%|██████▋   | 211/314 [27:13<16:09,  9.41s/it]test:212/314 | accuracy 6270  0.9242334905660378:  67%|██████▋   | 211/314 [27:22<16:09,  9.41s/it]test:212/314 | accuracy 6270  0.9242334905660378:  68%|██████▊   | 212/314 [27:22<15:41,  9.23s/it]test:213/314 | accuracy 6302  0.9245892018779343:  68%|██████▊   | 212/314 [27:32<15:41,  9.23s/it]test:213/314 | accuracy 6302  0.9245892018779343:  68%|██████▊   | 213/314 [27:32<15:54,  9.45s/it]test:214/314 | accuracy 6332  0.9246495327102804:  68%|██████▊   | 213/314 [27:41<15:54,  9.45s/it]test:214/314 | accuracy 6332  0.9246495327102804:  68%|██████▊   | 214/314 [27:41<15:26,  9.26s/it]test:215/314 | accuracy 6364  0.925:  68%|██████▊   | 214/314 [27:50<15:26,  9.26s/it]             test:215/314 | accuracy 6364  0.925:  68%|██████▊   | 215/314 [27:50<15:26,  9.36s/it]test:216/314 | accuracy 6394  0.9250578703703703:  68%|██████▊   | 215/314 [27:59<15:26,  9.36s/it]test:216/314 | accuracy 6394  0.9250578703703703:  69%|██████▉   | 216/314 [27:59<15:11,  9.30s/it]test:217/314 | accuracy 6423  0.924971198156682:  69%|██████▉   | 216/314 [28:09<15:11,  9.30s/it] test:217/314 | accuracy 6423  0.924971198156682:  69%|██████▉   | 217/314 [28:09<15:01,  9.29s/it]test:218/314 | accuracy 6455  0.9253153669724771:  69%|██████▉   | 217/314 [28:18<15:01,  9.29s/it]test:218/314 | accuracy 6455  0.9253153669724771:  69%|██████▉   | 218/314 [28:18<14:57,  9.35s/it]test:219/314 | accuracy 6486  0.925513698630137:  69%|██████▉   | 218/314 [28:28<14:57,  9.35s/it] test:219/314 | accuracy 6486  0.925513698630137:  70%|██████▉   | 219/314 [28:28<15:13,  9.62s/it]test:220/314 | accuracy 6517  0.9257102272727272:  70%|██████▉   | 219/314 [28:38<15:13,  9.62s/it]test:220/314 | accuracy 6517  0.9257102272727272:  70%|███████   | 220/314 [28:38<14:54,  9.52s/it]test:221/314 | accuracy 6549  0.9260463800904978:  70%|███████   | 220/314 [28:47<14:54,  9.52s/it]test:221/314 | accuracy 6549  0.9260463800904978:  70%|███████   | 221/314 [28:47<14:40,  9.46s/it]test:222/314 | accuracy 6581  0.9263795045045045:  70%|███████   | 221/314 [28:56<14:40,  9.46s/it]test:222/314 | accuracy 6581  0.9263795045045045:  71%|███████   | 222/314 [28:56<14:18,  9.33s/it]test:223/314 | accuracy 6612  0.9265695067264574:  71%|███████   | 222/314 [29:05<14:18,  9.33s/it]test:223/314 | accuracy 6612  0.9265695067264574:  71%|███████   | 223/314 [29:05<14:09,  9.34s/it]test:224/314 | accuracy 6644  0.9268973214285714:  71%|███████   | 223/314 [29:14<14:09,  9.34s/it]test:224/314 | accuracy 6644  0.9268973214285714:  71%|███████▏  | 224/314 [29:14<13:45,  9.18s/it]test:225/314 | accuracy 6674  0.9269444444444445:  71%|███████▏  | 224/314 [29:23<13:45,  9.18s/it]test:225/314 | accuracy 6674  0.9269444444444445:  72%|███████▏  | 225/314 [29:23<13:34,  9.16s/it]test:226/314 | accuracy 6706  0.9272676991150443:  72%|███████▏  | 225/314 [29:33<13:34,  9.16s/it]test:226/314 | accuracy 6706  0.9272676991150443:  72%|███████▏  | 226/314 [29:33<13:43,  9.36s/it]test:227/314 | accuracy 6736  0.9273127753303965:  72%|███████▏  | 226/314 [29:43<13:43,  9.36s/it]test:227/314 | accuracy 6736  0.9273127753303965:  72%|███████▏  | 227/314 [29:43<13:40,  9.44s/it]test:228/314 | accuracy 6768  0.9276315789473685:  72%|███████▏  | 227/314 [29:52<13:40,  9.44s/it]test:228/314 | accuracy 6768  0.9276315789473685:  73%|███████▎  | 228/314 [29:52<13:24,  9.36s/it]test:229/314 | accuracy 6800  0.9279475982532751:  73%|███████▎  | 228/314 [30:02<13:24,  9.36s/it]test:229/314 | accuracy 6800  0.9279475982532751:  73%|███████▎  | 229/314 [30:02<13:24,  9.46s/it]test:230/314 | accuracy 6832  0.9282608695652174:  73%|███████▎  | 229/314 [30:11<13:24,  9.46s/it]test:230/314 | accuracy 6832  0.9282608695652174:  73%|███████▎  | 230/314 [30:11<13:16,  9.48s/it]test:231/314 | accuracy 6861  0.9281655844155844:  73%|███████▎  | 230/314 [30:20<13:16,  9.48s/it]test:231/314 | accuracy 6861  0.9281655844155844:  74%|███████▎  | 231/314 [30:20<12:59,  9.39s/it]test:232/314 | accuracy 6892  0.9283405172413793:  74%|███████▎  | 231/314 [30:29<12:59,  9.39s/it]test:232/314 | accuracy 6892  0.9283405172413793:  74%|███████▍  | 232/314 [30:29<12:42,  9.29s/it]test:233/314 | accuracy 6923  0.9285139484978541:  74%|███████▍  | 232/314 [30:39<12:42,  9.29s/it]test:233/314 | accuracy 6923  0.9285139484978541:  74%|███████▍  | 233/314 [30:39<12:38,  9.36s/it]test:234/314 | accuracy 6954  0.9286858974358975:  74%|███████▍  | 233/314 [30:48<12:38,  9.36s/it]test:234/314 | accuracy 6954  0.9286858974358975:  75%|███████▍  | 234/314 [30:48<12:29,  9.37s/it]test:235/314 | accuracy 6985  0.9288563829787234:  75%|███████▍  | 234/314 [30:58<12:29,  9.37s/it]test:235/314 | accuracy 6985  0.9288563829787234:  75%|███████▍  | 235/314 [30:58<12:20,  9.37s/it]test:236/314 | accuracy 7017  0.9291578389830508:  75%|███████▍  | 235/314 [31:07<12:20,  9.37s/it]test:236/314 | accuracy 7017  0.9291578389830508:  75%|███████▌  | 236/314 [31:07<12:01,  9.25s/it]test:237/314 | accuracy 7049  0.9294567510548524:  75%|███████▌  | 236/314 [31:16<12:01,  9.25s/it]test:237/314 | accuracy 7049  0.9294567510548524:  75%|███████▌  | 237/314 [31:16<11:54,  9.27s/it]test:238/314 | accuracy 7080  0.9296218487394958:  75%|███████▌  | 237/314 [31:25<11:54,  9.27s/it]test:238/314 | accuracy 7080  0.9296218487394958:  76%|███████▌  | 238/314 [31:26<11:49,  9.33s/it]test:239/314 | accuracy 7112  0.9299163179916318:  76%|███████▌  | 238/314 [31:35<11:49,  9.33s/it]test:239/314 | accuracy 7112  0.9299163179916318:  76%|███████▌  | 239/314 [31:35<11:50,  9.47s/it]test:240/314 | accuracy 7144  0.9302083333333333:  76%|███████▌  | 239/314 [31:45<11:50,  9.47s/it]test:240/314 | accuracy 7144  0.9302083333333333:  76%|███████▋  | 240/314 [31:45<11:40,  9.47s/it]test:241/314 | accuracy 7176  0.9304979253112033:  76%|███████▋  | 240/314 [31:54<11:40,  9.47s/it]test:241/314 | accuracy 7176  0.9304979253112033:  77%|███████▋  | 241/314 [31:54<11:29,  9.45s/it]test:242/314 | accuracy 7208  0.9307851239669421:  77%|███████▋  | 241/314 [32:04<11:29,  9.45s/it]test:242/314 | accuracy 7208  0.9307851239669421:  77%|███████▋  | 242/314 [32:04<11:20,  9.46s/it]test:243/314 | accuracy 7239  0.9309413580246914:  77%|███████▋  | 242/314 [32:13<11:20,  9.46s/it]test:243/314 | accuracy 7239  0.9309413580246914:  77%|███████▋  | 243/314 [32:13<11:08,  9.42s/it]test:244/314 | accuracy 7270  0.9310963114754098:  77%|███████▋  | 243/314 [32:22<11:08,  9.42s/it]test:244/314 | accuracy 7270  0.9310963114754098:  78%|███████▊  | 244/314 [32:22<10:52,  9.32s/it]test:245/314 | accuracy 7302  0.9313775510204082:  78%|███████▊  | 244/314 [32:31<10:52,  9.32s/it]test:245/314 | accuracy 7302  0.9313775510204082:  78%|███████▊  | 245/314 [32:31<10:41,  9.30s/it]test:246/314 | accuracy 7334  0.9316565040650406:  78%|███████▊  | 245/314 [32:40<10:41,  9.30s/it]test:246/314 | accuracy 7334  0.9316565040650406:  78%|███████▊  | 246/314 [32:41<10:32,  9.30s/it]test:247/314 | accuracy 7365  0.9318066801619433:  78%|███████▊  | 246/314 [32:50<10:32,  9.30s/it]test:247/314 | accuracy 7365  0.9318066801619433:  79%|███████▊  | 247/314 [32:50<10:31,  9.42s/it]test:248/314 | accuracy 7394  0.9317036290322581:  79%|███████▊  | 247/314 [32:59<10:31,  9.42s/it]test:248/314 | accuracy 7394  0.9317036290322581:  79%|███████▉  | 248/314 [33:00<10:18,  9.37s/it]test:249/314 | accuracy 7424  0.9317269076305221:  79%|███████▉  | 248/314 [33:09<10:18,  9.37s/it]test:249/314 | accuracy 7424  0.9317269076305221:  79%|███████▉  | 249/314 [33:09<10:07,  9.34s/it]test:250/314 | accuracy 7456  0.932:  79%|███████▉  | 249/314 [33:18<10:07,  9.34s/it]             test:250/314 | accuracy 7456  0.932:  80%|███████▉  | 250/314 [33:19<10:05,  9.47s/it]test:251/314 | accuracy 7488  0.9322709163346613:  80%|███████▉  | 250/314 [33:28<10:05,  9.47s/it]test:251/314 | accuracy 7488  0.9322709163346613:  80%|███████▉  | 251/314 [33:28<09:52,  9.40s/it]test:252/314 | accuracy 7520  0.9325396825396826:  80%|███████▉  | 251/314 [33:37<09:52,  9.40s/it]test:252/314 | accuracy 7520  0.9325396825396826:  80%|████████  | 252/314 [33:37<09:42,  9.39s/it]test:253/314 | accuracy 7551  0.9326828063241107:  80%|████████  | 252/314 [33:47<09:42,  9.39s/it]test:253/314 | accuracy 7551  0.9326828063241107:  81%|████████  | 253/314 [33:47<09:37,  9.47s/it]test:254/314 | accuracy 7582  0.9328248031496063:  81%|████████  | 253/314 [33:56<09:37,  9.47s/it]test:254/314 | accuracy 7582  0.9328248031496063:  81%|████████  | 254/314 [33:56<09:26,  9.45s/it]test:255/314 | accuracy 7613  0.9329656862745098:  81%|████████  | 254/314 [34:05<09:26,  9.45s/it]test:255/314 | accuracy 7613  0.9329656862745098:  81%|████████  | 255/314 [34:06<09:14,  9.39s/it]test:256/314 | accuracy 7644  0.93310546875:  81%|████████  | 255/314 [34:15<09:14,  9.39s/it]     test:256/314 | accuracy 7644  0.93310546875:  82%|████████▏ | 256/314 [34:15<09:05,  9.40s/it]test:257/314 | accuracy 7675  0.9332441634241245:  82%|████████▏ | 256/314 [34:24<09:05,  9.40s/it]test:257/314 | accuracy 7675  0.9332441634241245:  82%|████████▏ | 257/314 [34:24<08:55,  9.39s/it]test:258/314 | accuracy 7706  0.9333817829457365:  82%|████████▏ | 257/314 [34:34<08:55,  9.39s/it]test:258/314 | accuracy 7706  0.9333817829457365:  82%|████████▏ | 258/314 [34:34<08:44,  9.37s/it]test:259/314 | accuracy 7735  0.933277027027027:  82%|████████▏ | 258/314 [34:43<08:44,  9.37s/it] test:259/314 | accuracy 7735  0.933277027027027:  82%|████████▏ | 259/314 [34:43<08:40,  9.47s/it]test:260/314 | accuracy 7767  0.9335336538461538:  82%|████████▏ | 259/314 [34:52<08:40,  9.47s/it]test:260/314 | accuracy 7767  0.9335336538461538:  83%|████████▎ | 260/314 [34:53<08:27,  9.40s/it]test:261/314 | accuracy 7796  0.9334291187739464:  83%|████████▎ | 260/314 [35:02<08:27,  9.40s/it]test:261/314 | accuracy 7796  0.9334291187739464:  83%|████████▎ | 261/314 [35:02<08:17,  9.39s/it]test:262/314 | accuracy 7828  0.9336832061068703:  83%|████████▎ | 261/314 [35:11<08:17,  9.39s/it]test:262/314 | accuracy 7828  0.9336832061068703:  83%|████████▎ | 262/314 [35:11<08:06,  9.35s/it]test:263/314 | accuracy 7859  0.9338165399239544:  83%|████████▎ | 262/314 [35:20<08:06,  9.35s/it]test:263/314 | accuracy 7859  0.9338165399239544:  84%|████████▍ | 263/314 [35:20<07:53,  9.29s/it]test:264/314 | accuracy 7891  0.9340672348484849:  84%|████████▍ | 263/314 [35:30<07:53,  9.29s/it]test:264/314 | accuracy 7891  0.9340672348484849:  84%|████████▍ | 264/314 [35:30<07:48,  9.37s/it]test:265/314 | accuracy 7921  0.9340801886792452:  84%|████████▍ | 264/314 [35:39<07:48,  9.37s/it]test:265/314 | accuracy 7921  0.9340801886792452:  84%|████████▍ | 265/314 [35:39<07:35,  9.30s/it]test:266/314 | accuracy 7953  0.934328007518797:  84%|████████▍ | 265/314 [35:49<07:35,  9.30s/it] test:266/314 | accuracy 7953  0.934328007518797:  85%|████████▍ | 266/314 [35:49<07:31,  9.41s/it]test:267/314 | accuracy 7985  0.9345739700374532:  85%|████████▍ | 266/314 [35:58<07:31,  9.41s/it]test:267/314 | accuracy 7985  0.9345739700374532:  85%|████████▌ | 267/314 [35:58<07:20,  9.38s/it]test:268/314 | accuracy 8014  0.9344682835820896:  85%|████████▌ | 267/314 [36:07<07:20,  9.38s/it]test:268/314 | accuracy 8014  0.9344682835820896:  85%|████████▌ | 268/314 [36:08<07:13,  9.42s/it]test:269/314 | accuracy 8045  0.9345957249070632:  85%|████████▌ | 268/314 [36:17<07:13,  9.42s/it]test:269/314 | accuracy 8045  0.9345957249070632:  86%|████████▌ | 269/314 [36:17<07:05,  9.46s/it]test:270/314 | accuracy 8077  0.934837962962963:  86%|████████▌ | 269/314 [36:26<07:05,  9.46s/it] test:270/314 | accuracy 8077  0.934837962962963:  86%|████████▌ | 270/314 [36:26<06:54,  9.42s/it]test:271/314 | accuracy 8107  0.9348477859778598:  86%|████████▌ | 270/314 [36:35<06:54,  9.42s/it]test:271/314 | accuracy 8107  0.9348477859778598:  86%|████████▋ | 271/314 [36:35<06:40,  9.31s/it]test:272/314 | accuracy 8138  0.9349724264705882:  86%|████████▋ | 271/314 [36:45<06:40,  9.31s/it]test:272/314 | accuracy 8138  0.9349724264705882:  87%|████████▋ | 272/314 [36:45<06:37,  9.47s/it]test:273/314 | accuracy 8169  0.9350961538461539:  87%|████████▋ | 272/314 [36:54<06:37,  9.47s/it]test:273/314 | accuracy 8169  0.9350961538461539:  87%|████████▋ | 273/314 [36:54<06:23,  9.35s/it]test:274/314 | accuracy 8199  0.9351049270072993:  87%|████████▋ | 273/314 [37:04<06:23,  9.35s/it]test:274/314 | accuracy 8199  0.9351049270072993:  87%|████████▋ | 274/314 [37:04<06:13,  9.35s/it]test:275/314 | accuracy 8230  0.9352272727272727:  87%|████████▋ | 274/314 [37:13<06:13,  9.35s/it]test:275/314 | accuracy 8230  0.9352272727272727:  88%|████████▊ | 275/314 [37:13<06:04,  9.34s/it]test:276/314 | accuracy 8261  0.935348731884058:  88%|████████▊ | 275/314 [37:22<06:04,  9.34s/it] test:276/314 | accuracy 8261  0.935348731884058:  88%|████████▊ | 276/314 [37:22<05:54,  9.34s/it]test:277/314 | accuracy 8292  0.9354693140794224:  88%|████████▊ | 276/314 [37:32<05:54,  9.34s/it]test:277/314 | accuracy 8292  0.9354693140794224:  88%|████████▊ | 277/314 [37:32<05:44,  9.32s/it]test:278/314 | accuracy 8324  0.9357014388489209:  88%|████████▊ | 277/314 [37:41<05:44,  9.32s/it]test:278/314 | accuracy 8324  0.9357014388489209:  89%|████████▊ | 278/314 [37:41<05:35,  9.33s/it]test:279/314 | accuracy 8353  0.9355958781362007:  89%|████████▊ | 278/314 [37:50<05:35,  9.33s/it]test:279/314 | accuracy 8353  0.9355958781362007:  89%|████████▉ | 279/314 [37:50<05:21,  9.20s/it]test:280/314 | accuracy 8384  0.9357142857142857:  89%|████████▉ | 279/314 [38:00<05:21,  9.20s/it]test:280/314 | accuracy 8384  0.9357142857142857:  89%|████████▉ | 280/314 [38:00<05:19,  9.39s/it]test:281/314 | accuracy 8414  0.935720640569395:  89%|████████▉ | 280/314 [38:09<05:19,  9.39s/it] test:281/314 | accuracy 8414  0.935720640569395:  89%|████████▉ | 281/314 [38:09<05:10,  9.42s/it]test:282/314 | accuracy 8445  0.9358377659574468:  89%|████████▉ | 281/314 [38:18<05:10,  9.42s/it]test:282/314 | accuracy 8445  0.9358377659574468:  90%|████████▉ | 282/314 [38:19<05:00,  9.38s/it]test:283/314 | accuracy 8476  0.9359540636042403:  90%|████████▉ | 282/314 [38:28<05:00,  9.38s/it]test:283/314 | accuracy 8476  0.9359540636042403:  90%|█████████ | 283/314 [38:28<04:49,  9.35s/it]test:284/314 | accuracy 8505  0.9358494718309859:  90%|█████████ | 283/314 [38:37<04:49,  9.35s/it]test:284/314 | accuracy 8505  0.9358494718309859:  90%|█████████ | 284/314 [38:37<04:39,  9.31s/it]test:285/314 | accuracy 8536  0.9359649122807018:  90%|█████████ | 284/314 [38:46<04:39,  9.31s/it]test:285/314 | accuracy 8536  0.9359649122807018:  91%|█████████ | 285/314 [38:46<04:31,  9.35s/it]test:286/314 | accuracy 8565  0.935861013986014:  91%|█████████ | 285/314 [38:56<04:31,  9.35s/it] test:286/314 | accuracy 8565  0.935861013986014:  91%|█████████ | 286/314 [38:56<04:21,  9.33s/it]test:287/314 | accuracy 8597  0.9360844947735192:  91%|█████████ | 286/314 [39:05<04:21,  9.33s/it]test:287/314 | accuracy 8597  0.9360844947735192:  91%|█████████▏| 287/314 [39:05<04:11,  9.32s/it]test:288/314 | accuracy 8627  0.9360894097222222:  91%|█████████▏| 287/314 [39:14<04:11,  9.32s/it]test:288/314 | accuracy 8627  0.9360894097222222:  92%|█████████▏| 288/314 [39:15<04:04,  9.40s/it]test:289/314 | accuracy 8659  0.936310553633218:  92%|█████████▏| 288/314 [39:24<04:04,  9.40s/it] test:289/314 | accuracy 8659  0.936310553633218:  92%|█████████▏| 289/314 [39:24<03:54,  9.39s/it]test:290/314 | accuracy 8691  0.9365301724137931:  92%|█████████▏| 289/314 [39:33<03:54,  9.39s/it]test:290/314 | accuracy 8691  0.9365301724137931:  92%|█████████▏| 290/314 [39:33<03:45,  9.41s/it]test:291/314 | accuracy 8723  0.9367482817869416:  92%|█████████▏| 290/314 [39:42<03:45,  9.41s/it]test:291/314 | accuracy 8723  0.9367482817869416:  93%|█████████▎| 291/314 [39:42<03:33,  9.28s/it]test:292/314 | accuracy 8755  0.936964897260274:  93%|█████████▎| 291/314 [39:51<03:33,  9.28s/it] test:292/314 | accuracy 8755  0.936964897260274:  93%|█████████▎| 292/314 [39:52<03:22,  9.22s/it]test:293/314 | accuracy 8786  0.9370733788395904:  93%|█████████▎| 292/314 [40:01<03:22,  9.22s/it]test:293/314 | accuracy 8786  0.9370733788395904:  93%|█████████▎| 293/314 [40:01<03:16,  9.38s/it]test:294/314 | accuracy 8816  0.9370748299319728:  93%|█████████▎| 293/314 [40:11<03:16,  9.38s/it]test:294/314 | accuracy 8816  0.9370748299319728:  94%|█████████▎| 294/314 [40:11<03:09,  9.45s/it]test:295/314 | accuracy 8847  0.9371822033898305:  94%|█████████▎| 294/314 [40:20<03:09,  9.45s/it]test:295/314 | accuracy 8847  0.9371822033898305:  94%|█████████▍| 295/314 [40:20<02:57,  9.36s/it]test:296/314 | accuracy 8878  0.9372888513513513:  94%|█████████▍| 295/314 [40:30<02:57,  9.36s/it]test:296/314 | accuracy 8878  0.9372888513513513:  94%|█████████▍| 296/314 [40:30<02:50,  9.48s/it]test:297/314 | accuracy 8909  0.9373947811447811:  94%|█████████▍| 296/314 [40:39<02:50,  9.48s/it]test:297/314 | accuracy 8909  0.9373947811447811:  95%|█████████▍| 297/314 [40:39<02:41,  9.47s/it]test:298/314 | accuracy 8941  0.9376048657718121:  95%|█████████▍| 297/314 [40:48<02:41,  9.47s/it]test:298/314 | accuracy 8941  0.9376048657718121:  95%|█████████▍| 298/314 [40:48<02:29,  9.37s/it]test:299/314 | accuracy 8972  0.9377090301003345:  95%|█████████▍| 298/314 [40:58<02:29,  9.37s/it]test:299/314 | accuracy 8972  0.9377090301003345:  95%|█████████▌| 299/314 [40:58<02:20,  9.38s/it]test:300/314 | accuracy 9004  0.9379166666666666:  95%|█████████▌| 299/314 [41:07<02:20,  9.38s/it]test:300/314 | accuracy 9004  0.9379166666666666:  96%|█████████▌| 300/314 [41:07<02:10,  9.33s/it]test:301/314 | accuracy 9035  0.9380191029900332:  96%|█████████▌| 300/314 [41:16<02:10,  9.33s/it]test:301/314 | accuracy 9035  0.9380191029900332:  96%|█████████▌| 301/314 [41:16<02:01,  9.36s/it]test:302/314 | accuracy 9066  0.9381208609271523:  96%|█████████▌| 301/314 [41:25<02:01,  9.36s/it]test:302/314 | accuracy 9066  0.9381208609271523:  96%|█████████▌| 302/314 [41:26<01:51,  9.28s/it]test:303/314 | accuracy 9098  0.9383250825082509:  96%|█████████▌| 302/314 [41:35<01:51,  9.28s/it]test:303/314 | accuracy 9098  0.9383250825082509:  96%|█████████▋| 303/314 [41:35<01:42,  9.29s/it]test:304/314 | accuracy 9127  0.938219572368421:  96%|█████████▋| 303/314 [41:44<01:42,  9.29s/it] test:304/314 | accuracy 9127  0.938219572368421:  97%|█████████▋| 304/314 [41:44<01:31,  9.18s/it]test:305/314 | accuracy 9157  0.9382172131147541:  97%|█████████▋| 304/314 [41:53<01:31,  9.18s/it]test:305/314 | accuracy 9157  0.9382172131147541:  97%|█████████▋| 305/314 [41:53<01:23,  9.26s/it]test:306/314 | accuracy 9189  0.9384191176470589:  97%|█████████▋| 305/314 [42:02<01:23,  9.26s/it]test:306/314 | accuracy 9189  0.9384191176470589:  97%|█████████▋| 306/314 [42:02<01:13,  9.16s/it]test:307/314 | accuracy 9221  0.9386197068403909:  97%|█████████▋| 306/314 [42:11<01:13,  9.16s/it]test:307/314 | accuracy 9221  0.9386197068403909:  98%|█████████▊| 307/314 [42:12<01:04,  9.24s/it]test:308/314 | accuracy 9249  0.9384131493506493:  98%|█████████▊| 307/314 [42:21<01:04,  9.24s/it]test:308/314 | accuracy 9249  0.9384131493506493:  98%|█████████▊| 308/314 [42:21<00:55,  9.28s/it]test:309/314 | accuracy 9279  0.9384101941747572:  98%|█████████▊| 308/314 [42:30<00:55,  9.28s/it]test:309/314 | accuracy 9279  0.9384101941747572:  98%|█████████▊| 309/314 [42:30<00:46,  9.31s/it]test:310/314 | accuracy 9309  0.9384072580645161:  98%|█████████▊| 309/314 [42:39<00:46,  9.31s/it]test:310/314 | accuracy 9309  0.9384072580645161:  99%|█████████▊| 310/314 [42:40<00:37,  9.33s/it]test:311/314 | accuracy 9340  0.9385048231511254:  99%|█████████▊| 310/314 [42:49<00:37,  9.33s/it]test:311/314 | accuracy 9340  0.9385048231511254:  99%|█████████▉| 311/314 [42:49<00:28,  9.35s/it]test:312/314 | accuracy 9372  0.9387019230769231:  99%|█████████▉| 311/314 [42:58<00:28,  9.35s/it]test:312/314 | accuracy 9372  0.9387019230769231:  99%|█████████▉| 312/314 [42:59<00:18,  9.38s/it]test:313/314 | accuracy 9402  0.9386980830670927:  99%|█████████▉| 312/314 [43:08<00:18,  9.38s/it]test:313/314 | accuracy 9402  0.9386980830670927: 100%|█████████▉| 313/314 [43:08<00:09,  9.33s/it]test:314/314 | accuracy 9428  0.9388568014339773: 100%|█████████▉| 313/314 [43:15<00:09,  9.33s/it]test:314/314 | accuracy 9428  0.9388568014339773: 100%|██████████| 314/314 [43:16<00:00,  8.85s/it]test:314/314 | accuracy 9428  0.9388568014339773: 100%|██████████| 314/314 [43:16<00:00,  8.27s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_64.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 128,
    "lora_dropout": 0.0,
    "r": 64
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:35,  1.95s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.93s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:31,  1.97s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:29,  1.94s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.92s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:25,  1.93s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.91s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:21,  1.91s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.92s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.90s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:21<00:15,  1.90s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:23<00:13,  1.91s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.91s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.90s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.90s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.90s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.90s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.91s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_64.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors #####
  0%|          | 0/40 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/40 | accuracy 24  0.75:   0%|          | 0/40 [00:04<?, ?it/s]test:1/40 | accuracy 24  0.75:   2%|▎         | 1/40 [00:04<03:05,  4.76s/it]test:2/40 | accuracy 51  0.796875:   2%|▎         | 1/40 [00:07<03:05,  4.76s/it]test:2/40 | accuracy 51  0.796875:   5%|▌         | 2/40 [00:07<02:25,  3.84s/it]test:3/40 | accuracy 81  0.84375:   5%|▌         | 2/40 [00:11<02:25,  3.84s/it] test:3/40 | accuracy 81  0.84375:   8%|▊         | 3/40 [00:11<02:12,  3.58s/it]test:4/40 | accuracy 110  0.859375:   8%|▊         | 3/40 [00:14<02:12,  3.58s/it]test:4/40 | accuracy 110  0.859375:  10%|█         | 4/40 [00:14<02:02,  3.41s/it]test:5/40 | accuracy 136  0.85:  10%|█         | 4/40 [00:17<02:02,  3.41s/it]    test:5/40 | accuracy 136  0.85:  12%|█▎        | 5/40 [00:17<01:56,  3.34s/it]test:6/40 | accuracy 163  0.8489583333333334:  12%|█▎        | 5/40 [00:20<01:56,  3.34s/it]test:6/40 | accuracy 163  0.8489583333333334:  15%|█▌        | 6/40 [00:20<01:52,  3.31s/it]test:7/40 | accuracy 191  0.8526785714285714:  15%|█▌        | 6/40 [00:23<01:52,  3.31s/it]test:7/40 | accuracy 191  0.8526785714285714:  18%|█▊        | 7/40 [00:23<01:46,  3.24s/it]test:8/40 | accuracy 218  0.8515625:  18%|█▊        | 7/40 [00:27<01:46,  3.24s/it]         test:8/40 | accuracy 218  0.8515625:  20%|██        | 8/40 [00:27<01:42,  3.20s/it]test:9/40 | accuracy 245  0.8506944444444444:  20%|██        | 8/40 [00:30<01:42,  3.20s/it]test:9/40 | accuracy 245  0.8506944444444444:  22%|██▎       | 9/40 [00:30<01:39,  3.21s/it]test:10/40 | accuracy 275  0.859375:  22%|██▎       | 9/40 [00:33<01:39,  3.21s/it]         test:10/40 | accuracy 275  0.859375:  25%|██▌       | 10/40 [00:33<01:36,  3.21s/it]test:11/40 | accuracy 301  0.8551136363636364:  25%|██▌       | 10/40 [00:36<01:36,  3.21s/it]test:11/40 | accuracy 301  0.8551136363636364:  28%|██▊       | 11/40 [00:36<01:32,  3.21s/it]test:12/40 | accuracy 331  0.8619791666666666:  28%|██▊       | 11/40 [00:39<01:32,  3.21s/it]test:12/40 | accuracy 331  0.8619791666666666:  30%|███       | 12/40 [00:39<01:30,  3.23s/it]test:13/40 | accuracy 360  0.8653846153846154:  30%|███       | 12/40 [00:43<01:30,  3.23s/it]test:13/40 | accuracy 360  0.8653846153846154:  32%|███▎      | 13/40 [00:43<01:27,  3.25s/it]test:14/40 | accuracy 388  0.8660714285714286:  32%|███▎      | 13/40 [00:46<01:27,  3.25s/it]test:14/40 | accuracy 388  0.8660714285714286:  35%|███▌      | 14/40 [00:46<01:24,  3.27s/it]test:15/40 | accuracy 416  0.8666666666666667:  35%|███▌      | 14/40 [00:49<01:24,  3.27s/it]test:15/40 | accuracy 416  0.8666666666666667:  38%|███▊      | 15/40 [00:49<01:21,  3.24s/it]test:16/40 | accuracy 440  0.859375:  38%|███▊      | 15/40 [00:52<01:21,  3.24s/it]          test:16/40 | accuracy 440  0.859375:  40%|████      | 16/40 [00:52<01:16,  3.20s/it]test:17/40 | accuracy 465  0.8547794117647058:  40%|████      | 16/40 [00:56<01:16,  3.20s/it]test:17/40 | accuracy 465  0.8547794117647058:  42%|████▎     | 17/40 [00:56<01:13,  3.19s/it]test:18/40 | accuracy 494  0.8576388888888888:  42%|████▎     | 17/40 [00:59<01:13,  3.19s/it]test:18/40 | accuracy 494  0.8576388888888888:  45%|████▌     | 18/40 [00:59<01:09,  3.17s/it]test:19/40 | accuracy 521  0.8569078947368421:  45%|████▌     | 18/40 [01:02<01:09,  3.17s/it]test:19/40 | accuracy 521  0.8569078947368421:  48%|████▊     | 19/40 [01:02<01:07,  3.22s/it]test:20/40 | accuracy 548  0.85625:  48%|████▊     | 19/40 [01:05<01:07,  3.22s/it]           test:20/40 | accuracy 548  0.85625:  50%|█████     | 20/40 [01:05<01:05,  3.26s/it]test:21/40 | accuracy 572  0.8511904761904762:  50%|█████     | 20/40 [01:09<01:05,  3.26s/it]test:21/40 | accuracy 572  0.8511904761904762:  52%|█████▎    | 21/40 [01:09<01:02,  3.27s/it]test:22/40 | accuracy 600  0.8522727272727273:  52%|█████▎    | 21/40 [01:12<01:02,  3.27s/it]test:22/40 | accuracy 600  0.8522727272727273:  55%|█████▌    | 22/40 [01:12<00:59,  3.28s/it]test:23/40 | accuracy 628  0.8532608695652174:  55%|█████▌    | 22/40 [01:15<00:59,  3.28s/it]test:23/40 | accuracy 628  0.8532608695652174:  57%|█████▊    | 23/40 [01:15<00:55,  3.28s/it]test:24/40 | accuracy 653  0.8502604166666666:  57%|█████▊    | 23/40 [01:18<00:55,  3.28s/it]test:24/40 | accuracy 653  0.8502604166666666:  60%|██████    | 24/40 [01:18<00:52,  3.25s/it]test:25/40 | accuracy 682  0.8525:  60%|██████    | 24/40 [01:22<00:52,  3.25s/it]            test:25/40 | accuracy 682  0.8525:  62%|██████▎   | 25/40 [01:22<00:48,  3.27s/it]test:26/40 | accuracy 708  0.8509615384615384:  62%|██████▎   | 25/40 [01:25<00:48,  3.27s/it]test:26/40 | accuracy 708  0.8509615384615384:  65%|██████▌   | 26/40 [01:25<00:46,  3.31s/it]test:27/40 | accuracy 737  0.8530092592592593:  65%|██████▌   | 26/40 [01:28<00:46,  3.31s/it]test:27/40 | accuracy 737  0.8530092592592593:  68%|██████▊   | 27/40 [01:28<00:42,  3.26s/it]test:28/40 | accuracy 761  0.8493303571428571:  68%|██████▊   | 27/40 [01:31<00:42,  3.26s/it]test:28/40 | accuracy 761  0.8493303571428571:  70%|███████   | 28/40 [01:31<00:38,  3.23s/it]test:29/40 | accuracy 789  0.8502155172413793:  70%|███████   | 28/40 [01:35<00:38,  3.23s/it]test:29/40 | accuracy 789  0.8502155172413793:  72%|███████▎  | 29/40 [01:35<00:35,  3.25s/it]test:30/40 | accuracy 818  0.8520833333333333:  72%|███████▎  | 29/40 [01:38<00:35,  3.25s/it]test:30/40 | accuracy 818  0.8520833333333333:  75%|███████▌  | 30/40 [01:38<00:32,  3.24s/it]test:31/40 | accuracy 846  0.8528225806451613:  75%|███████▌  | 30/40 [01:41<00:32,  3.24s/it]test:31/40 | accuracy 846  0.8528225806451613:  78%|███████▊  | 31/40 [01:41<00:28,  3.22s/it]test:32/40 | accuracy 869  0.8486328125:  78%|███████▊  | 31/40 [01:44<00:28,  3.22s/it]      test:32/40 | accuracy 869  0.8486328125:  80%|████████  | 32/40 [01:44<00:25,  3.20s/it]test:33/40 | accuracy 896  0.8484848484848485:  80%|████████  | 32/40 [01:47<00:25,  3.20s/it]test:33/40 | accuracy 896  0.8484848484848485:  82%|████████▎ | 33/40 [01:47<00:22,  3.19s/it]test:34/40 | accuracy 926  0.8511029411764706:  82%|████████▎ | 33/40 [01:51<00:22,  3.19s/it]test:34/40 | accuracy 926  0.8511029411764706:  85%|████████▌ | 34/40 [01:51<00:19,  3.19s/it]test:35/40 | accuracy 952  0.85:  85%|████████▌ | 34/40 [01:54<00:19,  3.19s/it]              test:35/40 | accuracy 952  0.85:  88%|████████▊ | 35/40 [01:54<00:16,  3.24s/it]test:36/40 | accuracy 983  0.8532986111111112:  88%|████████▊ | 35/40 [01:57<00:16,  3.24s/it]test:36/40 | accuracy 983  0.8532986111111112:  90%|█████████ | 36/40 [01:57<00:13,  3.26s/it]test:37/40 | accuracy 1011  0.8538851351351351:  90%|█████████ | 36/40 [02:00<00:13,  3.26s/it]test:37/40 | accuracy 1011  0.8538851351351351:  92%|█████████▎| 37/40 [02:01<00:09,  3.25s/it]test:38/40 | accuracy 1037  0.852796052631579:  92%|█████████▎| 37/40 [02:04<00:09,  3.25s/it] test:38/40 | accuracy 1037  0.852796052631579:  95%|█████████▌| 38/40 [02:04<00:06,  3.26s/it]test:39/40 | accuracy 1067  0.8549679487179487:  95%|█████████▌| 38/40 [02:07<00:06,  3.26s/it]test:39/40 | accuracy 1067  0.8549679487179487:  98%|█████████▊| 39/40 [02:07<00:03,  3.26s/it]test:40/40 | accuracy 1082  0.8539857932123125:  98%|█████████▊| 39/40 [02:09<00:03,  3.26s/it]test:40/40 | accuracy 1082  0.8539857932123125: 100%|██████████| 40/40 [02:09<00:00,  2.94s/it]test:40/40 | accuracy 1082  0.8539857932123125: 100%|██████████| 40/40 [02:09<00:00,  3.24s/it]


test finished
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_64.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 128,
    "lora_dropout": 0.0,
    "r": 64
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   5%|▌         | 1/19 [00:02<00:36,  2.01s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:33,  1.95s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.93s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.93s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.92s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.92s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.90s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.90s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.90s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.91s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:21<00:15,  1.91s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.91s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.91s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.90s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.91s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.96s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.94s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.93s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_64.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors #####
  0%|          | 0/37 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/37 | accuracy 25  0.78125:   0%|          | 0/37 [00:06<?, ?it/s]test:1/37 | accuracy 25  0.78125:   3%|▎         | 1/37 [00:06<03:43,  6.22s/it]test:2/37 | accuracy 50  0.78125:   3%|▎         | 1/37 [00:11<03:43,  6.22s/it]test:2/37 | accuracy 50  0.78125:   5%|▌         | 2/37 [00:11<03:17,  5.63s/it]test:3/37 | accuracy 74  0.7708333333333334:   5%|▌         | 2/37 [00:16<03:17,  5.63s/it]test:3/37 | accuracy 74  0.7708333333333334:   8%|▊         | 3/37 [00:16<03:04,  5.42s/it]test:4/37 | accuracy 104  0.8125:   8%|▊         | 3/37 [00:21<03:04,  5.42s/it]           test:4/37 | accuracy 104  0.8125:  11%|█         | 4/37 [00:21<02:50,  5.16s/it]test:5/37 | accuracy 129  0.80625:  11%|█         | 4/37 [00:25<02:50,  5.16s/it]test:5/37 | accuracy 129  0.80625:  14%|█▎        | 5/37 [00:25<02:35,  4.87s/it]test:6/37 | accuracy 152  0.7916666666666666:  14%|█▎        | 5/37 [00:31<02:35,  4.87s/it]test:6/37 | accuracy 152  0.7916666666666666:  16%|█▌        | 6/37 [00:31<02:40,  5.17s/it]test:7/37 | accuracy 180  0.8035714285714286:  16%|█▌        | 6/37 [00:36<02:40,  5.17s/it]test:7/37 | accuracy 180  0.8035714285714286:  19%|█▉        | 7/37 [00:36<02:29,  4.99s/it]test:8/37 | accuracy 205  0.80078125:  19%|█▉        | 7/37 [00:43<02:29,  4.99s/it]        test:8/37 | accuracy 205  0.80078125:  22%|██▏       | 8/37 [00:43<02:46,  5.73s/it]test:9/37 | accuracy 230  0.7986111111111112:  22%|██▏       | 8/37 [00:49<02:46,  5.73s/it]test:9/37 | accuracy 230  0.7986111111111112:  24%|██▍       | 9/37 [00:49<02:43,  5.83s/it]test:10/37 | accuracy 253  0.790625:  24%|██▍       | 9/37 [00:54<02:43,  5.83s/it]         test:10/37 | accuracy 253  0.790625:  27%|██▋       | 10/37 [00:54<02:34,  5.72s/it]test:11/37 | accuracy 275  0.78125:  27%|██▋       | 10/37 [01:00<02:34,  5.72s/it] test:11/37 | accuracy 275  0.78125:  30%|██▉       | 11/37 [01:00<02:24,  5.57s/it]test:12/37 | accuracy 300  0.78125:  30%|██▉       | 11/37 [01:06<02:24,  5.57s/it]test:12/37 | accuracy 300  0.78125:  32%|███▏      | 12/37 [01:06<02:22,  5.71s/it]test:13/37 | accuracy 324  0.7788461538461539:  32%|███▏      | 12/37 [01:10<02:22,  5.71s/it]test:13/37 | accuracy 324  0.7788461538461539:  35%|███▌      | 13/37 [01:10<02:10,  5.42s/it]test:14/37 | accuracy 347  0.7745535714285714:  35%|███▌      | 13/37 [01:16<02:10,  5.42s/it]test:14/37 | accuracy 347  0.7745535714285714:  38%|███▊      | 14/37 [01:16<02:07,  5.53s/it]test:15/37 | accuracy 376  0.7833333333333333:  38%|███▊      | 14/37 [01:22<02:07,  5.53s/it]test:15/37 | accuracy 376  0.7833333333333333:  41%|████      | 15/37 [01:22<02:01,  5.51s/it]test:16/37 | accuracy 402  0.78515625:  41%|████      | 15/37 [01:27<02:01,  5.51s/it]        test:16/37 | accuracy 402  0.78515625:  43%|████▎     | 16/37 [01:27<01:51,  5.32s/it]test:17/37 | accuracy 427  0.7849264705882353:  43%|████▎     | 16/37 [01:32<01:51,  5.32s/it]test:17/37 | accuracy 427  0.7849264705882353:  46%|████▌     | 17/37 [01:32<01:44,  5.24s/it]test:18/37 | accuracy 452  0.7847222222222222:  46%|████▌     | 17/37 [01:37<01:44,  5.24s/it]test:18/37 | accuracy 452  0.7847222222222222:  49%|████▊     | 18/37 [01:37<01:41,  5.36s/it]test:19/37 | accuracy 481  0.7911184210526315:  49%|████▊     | 18/37 [01:43<01:41,  5.36s/it]test:19/37 | accuracy 481  0.7911184210526315:  51%|█████▏    | 19/37 [01:43<01:39,  5.51s/it]test:20/37 | accuracy 509  0.7953125:  51%|█████▏    | 19/37 [01:48<01:39,  5.51s/it]         test:20/37 | accuracy 509  0.7953125:  54%|█████▍    | 20/37 [01:48<01:30,  5.30s/it]test:21/37 | accuracy 534  0.7946428571428571:  54%|█████▍    | 20/37 [01:53<01:30,  5.30s/it]test:21/37 | accuracy 534  0.7946428571428571:  57%|█████▋    | 21/37 [01:53<01:21,  5.12s/it]test:22/37 | accuracy 562  0.7982954545454546:  57%|█████▋    | 21/37 [01:58<01:21,  5.12s/it]test:22/37 | accuracy 562  0.7982954545454546:  59%|█████▉    | 22/37 [01:58<01:16,  5.10s/it]test:23/37 | accuracy 588  0.7989130434782609:  59%|█████▉    | 22/37 [02:03<01:16,  5.10s/it]test:23/37 | accuracy 588  0.7989130434782609:  62%|██████▏   | 23/37 [02:03<01:11,  5.11s/it]test:24/37 | accuracy 613  0.7981770833333334:  62%|██████▏   | 23/37 [02:07<01:11,  5.11s/it]test:24/37 | accuracy 613  0.7981770833333334:  65%|██████▍   | 24/37 [02:08<01:04,  4.98s/it]test:25/37 | accuracy 638  0.7975:  65%|██████▍   | 24/37 [02:13<01:04,  4.98s/it]            test:25/37 | accuracy 638  0.7975:  68%|██████▊   | 25/37 [02:13<01:01,  5.12s/it]test:26/37 | accuracy 663  0.796875:  68%|██████▊   | 25/37 [02:19<01:01,  5.12s/it]test:26/37 | accuracy 663  0.796875:  70%|███████   | 26/37 [02:19<00:58,  5.35s/it]test:27/37 | accuracy 687  0.7951388888888888:  70%|███████   | 26/37 [02:25<00:58,  5.35s/it]test:27/37 | accuracy 687  0.7951388888888888:  73%|███████▎  | 27/37 [02:25<00:56,  5.65s/it]test:28/37 | accuracy 709  0.7912946428571429:  73%|███████▎  | 27/37 [02:30<00:56,  5.65s/it]test:28/37 | accuracy 709  0.7912946428571429:  76%|███████▌  | 28/37 [02:30<00:49,  5.46s/it]test:29/37 | accuracy 734  0.790948275862069:  76%|███████▌  | 28/37 [02:36<00:49,  5.46s/it] test:29/37 | accuracy 734  0.790948275862069:  78%|███████▊  | 29/37 [02:36<00:45,  5.63s/it]test:30/37 | accuracy 761  0.7927083333333333:  78%|███████▊  | 29/37 [02:41<00:45,  5.63s/it]test:30/37 | accuracy 761  0.7927083333333333:  81%|████████  | 30/37 [02:41<00:38,  5.44s/it]test:31/37 | accuracy 787  0.7933467741935484:  81%|████████  | 30/37 [02:46<00:38,  5.44s/it]test:31/37 | accuracy 787  0.7933467741935484:  84%|████████▍ | 31/37 [02:46<00:31,  5.21s/it]test:32/37 | accuracy 811  0.7919921875:  84%|████████▍ | 31/37 [02:51<00:31,  5.21s/it]      test:32/37 | accuracy 811  0.7919921875:  86%|████████▋ | 32/37 [02:51<00:26,  5.25s/it]test:33/37 | accuracy 836  0.7916666666666666:  86%|████████▋ | 32/37 [02:56<00:26,  5.25s/it]test:33/37 | accuracy 836  0.7916666666666666:  89%|████████▉ | 33/37 [02:56<00:20,  5.14s/it]test:34/37 | accuracy 861  0.7913602941176471:  89%|████████▉ | 33/37 [03:01<00:20,  5.14s/it]test:34/37 | accuracy 861  0.7913602941176471:  92%|█████████▏| 34/37 [03:01<00:15,  5.14s/it]test:35/37 | accuracy 887  0.7919642857142857:  92%|█████████▏| 34/37 [03:06<00:15,  5.14s/it]test:35/37 | accuracy 887  0.7919642857142857:  95%|█████████▍| 35/37 [03:06<00:10,  5.08s/it]test:36/37 | accuracy 909  0.7890625:  95%|█████████▍| 35/37 [03:11<00:10,  5.08s/it]         test:36/37 | accuracy 909  0.7890625:  97%|█████████▋| 36/37 [03:11<00:04,  4.93s/it]test:37/37 | accuracy 926  0.7901023890784983:  97%|█████████▋| 36/37 [03:14<00:04,  4.93s/it]test:37/37 | accuracy 926  0.7901023890784983: 100%|██████████| 37/37 [03:14<00:00,  4.35s/it]test:37/37 | accuracy 926  0.7901023890784983: 100%|██████████| 37/37 [03:14<00:00,  5.25s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_64.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 128,
    "lora_dropout": 0.0,
    "r": 64
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.93s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.92s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.92s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.90s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.91s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.90s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.89s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.90s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:18,  1.90s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.90s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.91s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.93s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.92s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.91s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.92s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.91s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.91s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.91s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.89s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_64.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors #####
  0%|          | 0/75 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/75 | accuracy 28  0.875:   0%|          | 0/75 [00:05<?, ?it/s]test:1/75 | accuracy 28  0.875:   1%|▏         | 1/75 [00:05<07:05,  5.75s/it]test:2/75 | accuracy 56  0.875:   1%|▏         | 1/75 [00:10<07:05,  5.75s/it]test:2/75 | accuracy 56  0.875:   3%|▎         | 2/75 [00:10<06:36,  5.43s/it]test:3/75 | accuracy 86  0.8958333333333334:   3%|▎         | 2/75 [00:15<06:36,  5.43s/it]test:3/75 | accuracy 86  0.8958333333333334:   4%|▍         | 3/75 [00:15<06:10,  5.14s/it]test:4/75 | accuracy 115  0.8984375:   4%|▍         | 3/75 [00:20<06:10,  5.14s/it]        test:4/75 | accuracy 115  0.8984375:   5%|▌         | 4/75 [00:20<05:45,  4.86s/it]test:5/75 | accuracy 145  0.90625:   5%|▌         | 4/75 [00:26<05:45,  4.86s/it]  test:5/75 | accuracy 145  0.90625:   7%|▋         | 5/75 [00:27<06:29,  5.57s/it]test:6/75 | accuracy 177  0.921875:   7%|▋         | 5/75 [00:31<06:29,  5.57s/it]test:6/75 | accuracy 177  0.921875:   8%|▊         | 6/75 [00:31<06:09,  5.36s/it]test:7/75 | accuracy 208  0.9285714285714286:   8%|▊         | 6/75 [00:36<06:09,  5.36s/it]test:7/75 | accuracy 208  0.9285714285714286:   9%|▉         | 7/75 [00:36<05:56,  5.24s/it]test:8/75 | accuracy 239  0.93359375:   9%|▉         | 7/75 [00:41<05:56,  5.24s/it]        test:8/75 | accuracy 239  0.93359375:  11%|█         | 8/75 [00:41<05:34,  5.00s/it]test:9/75 | accuracy 270  0.9375:  11%|█         | 8/75 [00:45<05:34,  5.00s/it]    test:9/75 | accuracy 270  0.9375:  12%|█▏        | 9/75 [00:45<05:15,  4.79s/it]test:10/75 | accuracy 300  0.9375:  12%|█▏        | 9/75 [00:51<05:15,  4.79s/it]test:10/75 | accuracy 300  0.9375:  13%|█▎        | 10/75 [00:51<05:26,  5.02s/it]test:11/75 | accuracy 330  0.9375:  13%|█▎        | 10/75 [00:55<05:26,  5.02s/it]test:11/75 | accuracy 330  0.9375:  15%|█▍        | 11/75 [00:55<05:09,  4.84s/it]test:12/75 | accuracy 360  0.9375:  15%|█▍        | 11/75 [01:00<05:09,  4.84s/it]test:12/75 | accuracy 360  0.9375:  16%|█▌        | 12/75 [01:00<05:09,  4.92s/it]test:13/75 | accuracy 388  0.9326923076923077:  16%|█▌        | 12/75 [01:05<05:09,  4.92s/it]test:13/75 | accuracy 388  0.9326923076923077:  17%|█▋        | 13/75 [01:05<04:58,  4.81s/it]test:14/75 | accuracy 418  0.9330357142857143:  17%|█▋        | 13/75 [01:10<04:58,  4.81s/it]test:14/75 | accuracy 418  0.9330357142857143:  19%|█▊        | 14/75 [01:10<05:08,  5.05s/it]test:15/75 | accuracy 447  0.93125:  19%|█▊        | 14/75 [01:15<05:08,  5.05s/it]           test:15/75 | accuracy 447  0.93125:  20%|██        | 15/75 [01:15<04:55,  4.92s/it]test:16/75 | accuracy 478  0.93359375:  20%|██        | 15/75 [01:20<04:55,  4.92s/it]test:16/75 | accuracy 478  0.93359375:  21%|██▏       | 16/75 [01:20<04:48,  4.88s/it]test:17/75 | accuracy 507  0.9319852941176471:  21%|██▏       | 16/75 [01:25<04:48,  4.88s/it]test:17/75 | accuracy 507  0.9319852941176471:  23%|██▎       | 17/75 [01:25<04:54,  5.08s/it]test:18/75 | accuracy 537  0.9322916666666666:  23%|██▎       | 17/75 [01:30<04:54,  5.08s/it]test:18/75 | accuracy 537  0.9322916666666666:  24%|██▍       | 18/75 [01:30<04:39,  4.91s/it]test:19/75 | accuracy 566  0.930921052631579:  24%|██▍       | 18/75 [01:35<04:39,  4.91s/it] test:19/75 | accuracy 566  0.930921052631579:  25%|██▌       | 19/75 [01:35<04:35,  4.93s/it]test:20/75 | accuracy 596  0.93125:  25%|██▌       | 19/75 [01:39<04:35,  4.93s/it]          test:20/75 | accuracy 596  0.93125:  27%|██▋       | 20/75 [01:39<04:24,  4.81s/it]test:21/75 | accuracy 625  0.9300595238095238:  27%|██▋       | 20/75 [01:44<04:24,  4.81s/it]test:21/75 | accuracy 625  0.9300595238095238:  28%|██▊       | 21/75 [01:44<04:16,  4.75s/it]test:22/75 | accuracy 653  0.9275568181818182:  28%|██▊       | 21/75 [01:49<04:16,  4.75s/it]test:22/75 | accuracy 653  0.9275568181818182:  29%|██▉       | 22/75 [01:49<04:14,  4.80s/it]test:23/75 | accuracy 684  0.9293478260869565:  29%|██▉       | 22/75 [01:55<04:14,  4.80s/it]test:23/75 | accuracy 684  0.9293478260869565:  31%|███       | 23/75 [01:55<04:22,  5.05s/it]test:24/75 | accuracy 715  0.9309895833333334:  31%|███       | 23/75 [01:59<04:22,  5.05s/it]test:24/75 | accuracy 715  0.9309895833333334:  32%|███▏      | 24/75 [01:59<04:11,  4.93s/it]test:25/75 | accuracy 743  0.92875:  32%|███▏      | 24/75 [02:04<04:11,  4.93s/it]           test:25/75 | accuracy 743  0.92875:  33%|███▎      | 25/75 [02:04<04:09,  4.99s/it]test:26/75 | accuracy 772  0.9278846153846154:  33%|███▎      | 25/75 [02:09<04:09,  4.99s/it]test:26/75 | accuracy 772  0.9278846153846154:  35%|███▍      | 26/75 [02:09<04:03,  4.96s/it]test:27/75 | accuracy 801  0.9270833333333334:  35%|███▍      | 26/75 [02:14<04:03,  4.96s/it]test:27/75 | accuracy 801  0.9270833333333334:  36%|███▌      | 27/75 [02:14<03:58,  4.97s/it]test:28/75 | accuracy 832  0.9285714285714286:  36%|███▌      | 27/75 [02:19<03:58,  4.97s/it]test:28/75 | accuracy 832  0.9285714285714286:  37%|███▋      | 28/75 [02:19<03:56,  5.02s/it]test:29/75 | accuracy 861  0.927801724137931:  37%|███▋      | 28/75 [02:24<03:56,  5.02s/it] test:29/75 | accuracy 861  0.927801724137931:  39%|███▊      | 29/75 [02:24<03:45,  4.89s/it]test:30/75 | accuracy 888  0.925:  39%|███▊      | 29/75 [02:29<03:45,  4.89s/it]            test:30/75 | accuracy 888  0.925:  40%|████      | 30/75 [02:29<03:38,  4.85s/it]test:31/75 | accuracy 919  0.9264112903225806:  40%|████      | 30/75 [02:34<03:38,  4.85s/it]test:31/75 | accuracy 919  0.9264112903225806:  41%|████▏     | 31/75 [02:34<03:37,  4.94s/it]test:32/75 | accuracy 949  0.9267578125:  41%|████▏     | 31/75 [02:40<03:37,  4.94s/it]      test:32/75 | accuracy 949  0.9267578125:  43%|████▎     | 32/75 [02:40<03:50,  5.37s/it]test:33/75 | accuracy 979  0.9270833333333334:  43%|████▎     | 32/75 [02:45<03:50,  5.37s/it]test:33/75 | accuracy 979  0.9270833333333334:  44%|████▍     | 33/75 [02:45<03:38,  5.21s/it]test:34/75 | accuracy 1008  0.9264705882352942:  44%|████▍     | 33/75 [02:51<03:38,  5.21s/it]test:34/75 | accuracy 1008  0.9264705882352942:  45%|████▌     | 34/75 [02:51<03:37,  5.31s/it]test:35/75 | accuracy 1036  0.925:  45%|████▌     | 34/75 [02:55<03:37,  5.31s/it]             test:35/75 | accuracy 1036  0.925:  47%|████▋     | 35/75 [02:55<03:23,  5.09s/it]test:36/75 | accuracy 1066  0.9253472222222222:  47%|████▋     | 35/75 [03:00<03:23,  5.09s/it]test:36/75 | accuracy 1066  0.9253472222222222:  48%|████▊     | 36/75 [03:00<03:11,  4.91s/it]test:37/75 | accuracy 1095  0.924831081081081:  48%|████▊     | 36/75 [03:05<03:11,  4.91s/it] test:37/75 | accuracy 1095  0.924831081081081:  49%|████▉     | 37/75 [03:05<03:07,  4.94s/it]test:38/75 | accuracy 1120  0.9210526315789473:  49%|████▉     | 37/75 [03:09<03:07,  4.94s/it]test:38/75 | accuracy 1120  0.9210526315789473:  51%|█████     | 38/75 [03:09<02:59,  4.86s/it]test:39/75 | accuracy 1147  0.9190705128205128:  51%|█████     | 38/75 [03:15<02:59,  4.86s/it]test:39/75 | accuracy 1147  0.9190705128205128:  52%|█████▏    | 39/75 [03:15<02:59,  4.98s/it]test:40/75 | accuracy 1176  0.91875:  52%|█████▏    | 39/75 [03:20<02:59,  4.98s/it]           test:40/75 | accuracy 1176  0.91875:  53%|█████▎    | 40/75 [03:20<03:02,  5.22s/it]test:41/75 | accuracy 1206  0.9192073170731707:  53%|█████▎    | 40/75 [03:26<03:02,  5.22s/it]test:41/75 | accuracy 1206  0.9192073170731707:  55%|█████▍    | 41/75 [03:26<03:04,  5.41s/it]test:42/75 | accuracy 1238  0.9211309523809523:  55%|█████▍    | 41/75 [03:31<03:04,  5.41s/it]test:42/75 | accuracy 1238  0.9211309523809523:  56%|█████▌    | 42/75 [03:31<02:48,  5.12s/it]test:43/75 | accuracy 1267  0.9207848837209303:  56%|█████▌    | 42/75 [03:36<02:48,  5.12s/it]test:43/75 | accuracy 1267  0.9207848837209303:  57%|█████▋    | 43/75 [03:36<02:48,  5.26s/it]test:44/75 | accuracy 1294  0.9190340909090909:  57%|█████▋    | 43/75 [03:41<02:48,  5.26s/it]test:44/75 | accuracy 1294  0.9190340909090909:  59%|█████▊    | 44/75 [03:41<02:41,  5.22s/it]test:45/75 | accuracy 1321  0.9173611111111111:  59%|█████▊    | 44/75 [03:46<02:41,  5.22s/it]test:45/75 | accuracy 1321  0.9173611111111111:  60%|██████    | 45/75 [03:46<02:31,  5.06s/it]test:46/75 | accuracy 1351  0.9177989130434783:  60%|██████    | 45/75 [03:51<02:31,  5.06s/it]test:46/75 | accuracy 1351  0.9177989130434783:  61%|██████▏   | 46/75 [03:51<02:21,  4.90s/it]test:47/75 | accuracy 1381  0.918218085106383:  61%|██████▏   | 46/75 [03:55<02:21,  4.90s/it] test:47/75 | accuracy 1381  0.918218085106383:  63%|██████▎   | 47/75 [03:55<02:16,  4.87s/it]test:48/75 | accuracy 1407  0.916015625:  63%|██████▎   | 47/75 [04:00<02:16,  4.87s/it]      test:48/75 | accuracy 1407  0.916015625:  64%|██████▍   | 48/75 [04:00<02:12,  4.90s/it]test:49/75 | accuracy 1436  0.9158163265306123:  64%|██████▍   | 48/75 [04:05<02:12,  4.90s/it]test:49/75 | accuracy 1436  0.9158163265306123:  65%|██████▌   | 49/75 [04:05<02:03,  4.74s/it]test:50/75 | accuracy 1465  0.915625:  65%|██████▌   | 49/75 [04:09<02:03,  4.74s/it]          test:50/75 | accuracy 1465  0.915625:  67%|██████▋   | 50/75 [04:09<01:57,  4.71s/it]test:51/75 | accuracy 1494  0.9154411764705882:  67%|██████▋   | 50/75 [04:14<01:57,  4.71s/it]test:51/75 | accuracy 1494  0.9154411764705882:  68%|██████▊   | 51/75 [04:14<01:54,  4.76s/it]test:52/75 | accuracy 1524  0.9158653846153846:  68%|██████▊   | 51/75 [04:20<01:54,  4.76s/it]test:52/75 | accuracy 1524  0.9158653846153846:  69%|██████▉   | 52/75 [04:20<01:53,  4.94s/it]test:53/75 | accuracy 1553  0.9156839622641509:  69%|██████▉   | 52/75 [04:24<01:53,  4.94s/it]test:53/75 | accuracy 1553  0.9156839622641509:  71%|███████   | 53/75 [04:24<01:45,  4.81s/it]test:54/75 | accuracy 1581  0.9149305555555556:  71%|███████   | 53/75 [04:29<01:45,  4.81s/it]test:54/75 | accuracy 1581  0.9149305555555556:  72%|███████▏  | 54/75 [04:29<01:39,  4.76s/it]test:55/75 | accuracy 1613  0.9164772727272728:  72%|███████▏  | 54/75 [04:34<01:39,  4.76s/it]test:55/75 | accuracy 1613  0.9164772727272728:  73%|███████▎  | 55/75 [04:34<01:38,  4.91s/it]test:56/75 | accuracy 1643  0.9168526785714286:  73%|███████▎  | 55/75 [04:41<01:38,  4.91s/it]test:56/75 | accuracy 1643  0.9168526785714286:  75%|███████▍  | 56/75 [04:41<01:42,  5.41s/it]test:57/75 | accuracy 1672  0.9166666666666666:  75%|███████▍  | 56/75 [04:46<01:42,  5.41s/it]test:57/75 | accuracy 1672  0.9166666666666666:  76%|███████▌  | 57/75 [04:46<01:37,  5.40s/it]test:58/75 | accuracy 1704  0.9181034482758621:  76%|███████▌  | 57/75 [04:50<01:37,  5.40s/it]test:58/75 | accuracy 1704  0.9181034482758621:  77%|███████▋  | 58/75 [04:50<01:24,  4.97s/it]test:59/75 | accuracy 1735  0.9189618644067796:  77%|███████▋  | 58/75 [04:55<01:24,  4.97s/it]test:59/75 | accuracy 1735  0.9189618644067796:  79%|███████▊  | 59/75 [04:55<01:20,  5.03s/it]test:60/75 | accuracy 1763  0.9182291666666667:  79%|███████▊  | 59/75 [05:00<01:20,  5.03s/it]test:60/75 | accuracy 1763  0.9182291666666667:  80%|████████  | 60/75 [05:00<01:14,  4.96s/it]test:61/75 | accuracy 1789  0.9164959016393442:  80%|████████  | 60/75 [05:04<01:14,  4.96s/it]test:61/75 | accuracy 1789  0.9164959016393442:  81%|████████▏ | 61/75 [05:04<01:07,  4.80s/it]test:62/75 | accuracy 1821  0.9178427419354839:  81%|████████▏ | 61/75 [05:09<01:07,  4.80s/it]test:62/75 | accuracy 1821  0.9178427419354839:  83%|████████▎ | 62/75 [05:09<01:00,  4.63s/it]test:63/75 | accuracy 1848  0.9166666666666666:  83%|████████▎ | 62/75 [05:13<01:00,  4.63s/it]test:63/75 | accuracy 1848  0.9166666666666666:  84%|████████▍ | 63/75 [05:13<00:55,  4.66s/it]test:64/75 | accuracy 1875  0.91552734375:  84%|████████▍ | 63/75 [05:18<00:55,  4.66s/it]     test:64/75 | accuracy 1875  0.91552734375:  85%|████████▌ | 64/75 [05:18<00:52,  4.73s/it]test:65/75 | accuracy 1905  0.9158653846153846:  85%|████████▌ | 64/75 [05:23<00:52,  4.73s/it]test:65/75 | accuracy 1905  0.9158653846153846:  87%|████████▋ | 65/75 [05:23<00:47,  4.79s/it]test:66/75 | accuracy 1932  0.9147727272727273:  87%|████████▋ | 65/75 [05:27<00:47,  4.79s/it]test:66/75 | accuracy 1932  0.9147727272727273:  88%|████████▊ | 66/75 [05:27<00:41,  4.62s/it]test:67/75 | accuracy 1964  0.9160447761194029:  88%|████████▊ | 66/75 [05:32<00:41,  4.62s/it]test:67/75 | accuracy 1964  0.9160447761194029:  89%|████████▉ | 67/75 [05:32<00:36,  4.53s/it]test:68/75 | accuracy 1993  0.9159007352941176:  89%|████████▉ | 67/75 [05:36<00:36,  4.53s/it]test:68/75 | accuracy 1993  0.9159007352941176:  91%|█████████ | 68/75 [05:36<00:31,  4.47s/it]test:69/75 | accuracy 2021  0.9153079710144928:  91%|█████████ | 68/75 [05:41<00:31,  4.47s/it]test:69/75 | accuracy 2021  0.9153079710144928:  92%|█████████▏| 69/75 [05:41<00:27,  4.53s/it]test:70/75 | accuracy 2050  0.9151785714285714:  92%|█████████▏| 69/75 [05:45<00:27,  4.53s/it]test:70/75 | accuracy 2050  0.9151785714285714:  93%|█████████▎| 70/75 [05:45<00:22,  4.57s/it]test:71/75 | accuracy 2078  0.914612676056338:  93%|█████████▎| 70/75 [05:51<00:22,  4.57s/it] test:71/75 | accuracy 2078  0.914612676056338:  95%|█████████▍| 71/75 [05:51<00:19,  4.76s/it]test:72/75 | accuracy 2105  0.9136284722222222:  95%|█████████▍| 71/75 [05:55<00:19,  4.76s/it]test:72/75 | accuracy 2105  0.9136284722222222:  96%|█████████▌| 72/75 [05:55<00:14,  4.72s/it]test:73/75 | accuracy 2135  0.9139554794520548:  96%|█████████▌| 72/75 [06:00<00:14,  4.72s/it]test:73/75 | accuracy 2135  0.9139554794520548:  97%|█████████▋| 73/75 [06:00<00:09,  4.64s/it]test:74/75 | accuracy 2166  0.9146959459459459:  97%|█████████▋| 73/75 [06:05<00:09,  4.64s/it]test:74/75 | accuracy 2166  0.9146959459459459:  99%|█████████▊| 74/75 [06:05<00:04,  4.81s/it]test:75/75 | accuracy 2174  0.914983164983165:  99%|█████████▊| 74/75 [06:06<00:04,  4.81s/it] test:75/75 | accuracy 2174  0.914983164983165: 100%|██████████| 75/75 [06:06<00:00,  3.80s/it]test:75/75 | accuracy 2174  0.914983164983165: 100%|██████████| 75/75 [06:06<00:00,  4.89s/it]


test finished
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_64.1/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 128,
    "lora_dropout": 0.0,
    "r": 64
  },
  "shared_adapter_num": 1,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:35,  1.95s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.93s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.92s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.90s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.91s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.92s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.90s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.91s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.91s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.92s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:21<00:15,  1.93s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:23<00:13,  1.95s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:25<00:11,  1.94s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.92s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.92s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.92s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.91s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:34<00:01,  1.91s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:36<00:00,  1.90s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_64.1 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors
##### Loading 64 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_64.1/model.safetensors #####
  0%|          | 0/16 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/16 | accuracy 25  0.78125:   0%|          | 0/16 [00:05<?, ?it/s]test:1/16 | accuracy 25  0.78125:   6%|▋         | 1/16 [00:05<01:16,  5.11s/it]test:2/16 | accuracy 53  0.828125:   6%|▋         | 1/16 [00:09<01:16,  5.11s/it]test:2/16 | accuracy 53  0.828125:  12%|█▎        | 2/16 [00:09<01:07,  4.79s/it]test:3/16 | accuracy 80  0.8333333333333334:  12%|█▎        | 2/16 [00:13<01:07,  4.79s/it]test:3/16 | accuracy 80  0.8333333333333334:  19%|█▉        | 3/16 [00:13<00:54,  4.19s/it]test:4/16 | accuracy 108  0.84375:  19%|█▉        | 3/16 [00:17<00:54,  4.19s/it]          test:4/16 | accuracy 108  0.84375:  25%|██▌       | 4/16 [00:17<00:49,  4.12s/it]test:5/16 | accuracy 136  0.85:  25%|██▌       | 4/16 [00:21<00:49,  4.12s/it]   test:5/16 | accuracy 136  0.85:  31%|███▏      | 5/16 [00:21<00:45,  4.11s/it]test:6/16 | accuracy 164  0.8541666666666666:  31%|███▏      | 5/16 [00:25<00:45,  4.11s/it]test:6/16 | accuracy 164  0.8541666666666666:  38%|███▊      | 6/16 [00:25<00:40,  4.09s/it]test:7/16 | accuracy 191  0.8526785714285714:  38%|███▊      | 6/16 [00:29<00:40,  4.09s/it]test:7/16 | accuracy 191  0.8526785714285714:  44%|████▍     | 7/16 [00:29<00:36,  4.05s/it]test:8/16 | accuracy 218  0.8515625:  44%|████▍     | 7/16 [00:33<00:36,  4.05s/it]         test:8/16 | accuracy 218  0.8515625:  50%|█████     | 8/16 [00:33<00:31,  4.00s/it]test:9/16 | accuracy 244  0.8472222222222222:  50%|█████     | 8/16 [00:36<00:31,  4.00s/it]test:9/16 | accuracy 244  0.8472222222222222:  56%|█████▋    | 9/16 [00:36<00:27,  3.90s/it]test:10/16 | accuracy 272  0.85:  56%|█████▋    | 9/16 [00:40<00:27,  3.90s/it]             test:10/16 | accuracy 272  0.85:  62%|██████▎   | 10/16 [00:40<00:22,  3.78s/it]test:11/16 | accuracy 300  0.8522727272727273:  62%|██████▎   | 10/16 [00:44<00:22,  3.78s/it]test:11/16 | accuracy 300  0.8522727272727273:  69%|██████▉   | 11/16 [00:44<00:18,  3.75s/it]test:12/16 | accuracy 328  0.8541666666666666:  69%|██████▉   | 11/16 [00:47<00:18,  3.75s/it]test:12/16 | accuracy 328  0.8541666666666666:  75%|███████▌  | 12/16 [00:47<00:15,  3.78s/it]test:13/16 | accuracy 358  0.8605769230769231:  75%|███████▌  | 12/16 [00:51<00:15,  3.78s/it]test:13/16 | accuracy 358  0.8605769230769231:  81%|████████▏ | 13/16 [00:51<00:11,  3.71s/it]test:14/16 | accuracy 389  0.8683035714285714:  81%|████████▏ | 13/16 [00:55<00:11,  3.71s/it]test:14/16 | accuracy 389  0.8683035714285714:  88%|████████▊ | 14/16 [00:55<00:07,  3.79s/it]test:15/16 | accuracy 420  0.875:  88%|████████▊ | 14/16 [00:59<00:07,  3.79s/it]             test:15/16 | accuracy 420  0.875:  94%|█████████▍| 15/16 [00:59<00:03,  3.78s/it]test:16/16 | accuracy 437  0.874:  94%|█████████▍| 15/16 [01:02<00:03,  3.78s/it]test:16/16 | accuracy 437  0.874: 100%|██████████| 16/16 [01:02<00:00,  3.50s/it]test:16/16 | accuracy 437  0.874: 100%|██████████| 16/16 [01:02<00:00,  3.88s/it]


test finished

============================= JOB FEEDBACK =============================

Job ID: 2629216
Cluster: hk
User/Group: hgf_mxv5488/hk-project-p0022189
Account: hk-project-p0022189
State: COMPLETED (exit code 0)
Partition: accelerated
Nodes: 1
Cores per node: 152
Nodelist: hkn0732
CPU Utilized: 01:17:48
CPU Efficiency: 0.68% of 7-22:48:08 core-walltime
Job Wall-clock time: 01:15:19
Starttime: Sun Sep  8 08:37:52 2024
Endtime: Sun Sep  8 09:53:11 2024
Memory Utilized: 7.36 GB
Memory Efficiency: 0.00% of 0.00 MB
Energy Consumed: 3458568 Joule / 960.713333333333 Watthours
Average node power draw: 765.339234343881 Watt
