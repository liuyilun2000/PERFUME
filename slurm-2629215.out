MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.4/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 4,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:04<01:29,  4.99s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:09<01:24,  4.95s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:14<01:19,  4.95s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:19<01:13,  4.92s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:24<01:08,  4.92s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:29<01:04,  4.93s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:34<00:58,  4.90s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:39<00:54,  4.93s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:44<00:49,  4.93s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:49<00:44,  4.91s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:54<00:39,  4.91s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:59<00:34,  4.92s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [01:03<00:29,  4.91s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [01:08<00:24,  4.90s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [01:13<00:19,  4.90s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [01:18<00:14,  4.92s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [01:23<00:09,  4.89s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [01:28<00:04,  4.91s/it]Loading checkpoint shards: 100%|██████████| 19/19 [01:32<00:00,  4.69s/it]Loading checkpoint shards: 100%|██████████| 19/19 [01:32<00:00,  4.88s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.4 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors
##### Loading 256 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors #####
  0%|          | 0/103 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/103 | accuracy 22  0.6875:   0%|          | 0/103 [00:04<?, ?it/s]test:1/103 | accuracy 22  0.6875:   1%|          | 1/103 [00:04<07:05,  4.17s/it]test:2/103 | accuracy 41  0.640625:   1%|          | 1/103 [00:06<07:05,  4.17s/it]test:2/103 | accuracy 41  0.640625:   2%|▏         | 2/103 [00:06<05:28,  3.26s/it]test:3/103 | accuracy 62  0.6458333333333334:   2%|▏         | 2/103 [00:09<05:28,  3.26s/it]test:3/103 | accuracy 62  0.6458333333333334:   3%|▎         | 3/103 [00:09<04:58,  2.99s/it]test:4/103 | accuracy 85  0.6640625:   3%|▎         | 3/103 [00:12<04:58,  2.99s/it]         test:4/103 | accuracy 85  0.6640625:   4%|▍         | 4/103 [00:12<04:41,  2.84s/it]test:5/103 | accuracy 107  0.66875:   4%|▍         | 4/103 [00:14<04:41,  2.84s/it] test:5/103 | accuracy 107  0.66875:   5%|▍         | 5/103 [00:14<04:30,  2.76s/it]test:6/103 | accuracy 128  0.6666666666666666:   5%|▍         | 5/103 [00:17<04:30,  2.76s/it]test:6/103 | accuracy 128  0.6666666666666666:   6%|▌         | 6/103 [00:17<04:23,  2.71s/it]test:7/103 | accuracy 151  0.6741071428571429:   6%|▌         | 6/103 [00:20<04:23,  2.71s/it]test:7/103 | accuracy 151  0.6741071428571429:   7%|▋         | 7/103 [00:20<04:20,  2.71s/it]test:8/103 | accuracy 174  0.6796875:   7%|▋         | 7/103 [00:22<04:20,  2.71s/it]         test:8/103 | accuracy 174  0.6796875:   8%|▊         | 8/103 [00:22<04:14,  2.68s/it]test:9/103 | accuracy 197  0.6840277777777778:   8%|▊         | 8/103 [00:25<04:14,  2.68s/it]test:9/103 | accuracy 197  0.6840277777777778:   9%|▊         | 9/103 [00:25<04:07,  2.64s/it]test:10/103 | accuracy 222  0.69375:   9%|▊         | 9/103 [00:27<04:07,  2.64s/it]          test:10/103 | accuracy 222  0.69375:  10%|▉         | 10/103 [00:27<04:05,  2.64s/it]test:11/103 | accuracy 248  0.7045454545454546:  10%|▉         | 10/103 [00:30<04:05,  2.64s/it]test:11/103 | accuracy 248  0.7045454545454546:  11%|█         | 11/103 [00:30<04:02,  2.64s/it]test:12/103 | accuracy 269  0.7005208333333334:  11%|█         | 11/103 [00:33<04:02,  2.64s/it]test:12/103 | accuracy 269  0.7005208333333334:  12%|█▏        | 12/103 [00:33<03:59,  2.63s/it]test:13/103 | accuracy 289  0.6947115384615384:  12%|█▏        | 12/103 [00:35<03:59,  2.63s/it]test:13/103 | accuracy 289  0.6947115384615384:  13%|█▎        | 13/103 [00:35<03:57,  2.64s/it]test:14/103 | accuracy 308  0.6875:  13%|█▎        | 13/103 [00:38<03:57,  2.64s/it]            test:14/103 | accuracy 308  0.6875:  14%|█▎        | 14/103 [00:38<03:54,  2.64s/it]test:15/103 | accuracy 329  0.6854166666666667:  14%|█▎        | 14/103 [00:40<03:54,  2.64s/it]test:15/103 | accuracy 329  0.6854166666666667:  15%|█▍        | 15/103 [00:40<03:51,  2.63s/it]test:16/103 | accuracy 347  0.677734375:  15%|█▍        | 15/103 [00:43<03:51,  2.63s/it]       test:16/103 | accuracy 347  0.677734375:  16%|█▌        | 16/103 [00:43<03:48,  2.62s/it]test:17/103 | accuracy 371  0.6819852941176471:  16%|█▌        | 16/103 [00:46<03:48,  2.62s/it]test:17/103 | accuracy 371  0.6819852941176471:  17%|█▋        | 17/103 [00:46<03:46,  2.64s/it]test:18/103 | accuracy 389  0.6753472222222222:  17%|█▋        | 17/103 [00:48<03:46,  2.64s/it]test:18/103 | accuracy 389  0.6753472222222222:  17%|█▋        | 18/103 [00:48<03:42,  2.62s/it]test:19/103 | accuracy 408  0.6710526315789473:  17%|█▋        | 18/103 [00:51<03:42,  2.62s/it]test:19/103 | accuracy 408  0.6710526315789473:  18%|█▊        | 19/103 [00:51<03:40,  2.63s/it]test:20/103 | accuracy 431  0.6734375:  18%|█▊        | 19/103 [00:54<03:40,  2.63s/it]         test:20/103 | accuracy 431  0.6734375:  19%|█▉        | 20/103 [00:54<03:36,  2.61s/it]test:21/103 | accuracy 456  0.6785714285714286:  19%|█▉        | 20/103 [00:56<03:36,  2.61s/it]test:21/103 | accuracy 456  0.6785714285714286:  20%|██        | 21/103 [00:56<03:37,  2.65s/it]test:22/103 | accuracy 480  0.6818181818181818:  20%|██        | 21/103 [00:59<03:37,  2.65s/it]test:22/103 | accuracy 480  0.6818181818181818:  21%|██▏       | 22/103 [00:59<03:32,  2.63s/it]test:23/103 | accuracy 505  0.686141304347826:  21%|██▏       | 22/103 [01:01<03:32,  2.63s/it] test:23/103 | accuracy 505  0.686141304347826:  22%|██▏       | 23/103 [01:01<03:30,  2.63s/it]test:24/103 | accuracy 526  0.6848958333333334:  22%|██▏       | 23/103 [01:04<03:30,  2.63s/it]test:24/103 | accuracy 526  0.6848958333333334:  23%|██▎       | 24/103 [01:04<03:28,  2.63s/it]test:25/103 | accuracy 550  0.6875:  23%|██▎       | 24/103 [01:07<03:28,  2.63s/it]            test:25/103 | accuracy 550  0.6875:  24%|██▍       | 25/103 [01:07<03:25,  2.63s/it]test:26/103 | accuracy 572  0.6875:  24%|██▍       | 25/103 [01:09<03:25,  2.63s/it]test:26/103 | accuracy 572  0.6875:  25%|██▌       | 26/103 [01:09<03:23,  2.64s/it]test:27/103 | accuracy 593  0.6863425925925926:  25%|██▌       | 26/103 [01:12<03:23,  2.64s/it]test:27/103 | accuracy 593  0.6863425925925926:  26%|██▌       | 27/103 [01:12<03:20,  2.64s/it]test:28/103 | accuracy 612  0.6830357142857143:  26%|██▌       | 27/103 [01:15<03:20,  2.64s/it]test:28/103 | accuracy 612  0.6830357142857143:  27%|██▋       | 28/103 [01:15<03:17,  2.64s/it]test:29/103 | accuracy 633  0.6821120689655172:  27%|██▋       | 28/103 [01:17<03:17,  2.64s/it]test:29/103 | accuracy 633  0.6821120689655172:  28%|██▊       | 29/103 [01:17<03:16,  2.66s/it]test:30/103 | accuracy 656  0.6833333333333333:  28%|██▊       | 29/103 [01:20<03:16,  2.66s/it]test:30/103 | accuracy 656  0.6833333333333333:  29%|██▉       | 30/103 [01:20<03:14,  2.66s/it]test:31/103 | accuracy 678  0.6834677419354839:  29%|██▉       | 30/103 [01:23<03:14,  2.66s/it]test:31/103 | accuracy 678  0.6834677419354839:  30%|███       | 31/103 [01:23<03:10,  2.65s/it]test:32/103 | accuracy 699  0.6826171875:  30%|███       | 31/103 [01:25<03:10,  2.65s/it]      test:32/103 | accuracy 699  0.6826171875:  31%|███       | 32/103 [01:25<03:06,  2.63s/it]test:33/103 | accuracy 720  0.6818181818181818:  31%|███       | 32/103 [01:28<03:06,  2.63s/it]test:33/103 | accuracy 720  0.6818181818181818:  32%|███▏      | 33/103 [01:28<03:05,  2.65s/it]test:34/103 | accuracy 742  0.6819852941176471:  32%|███▏      | 33/103 [01:31<03:05,  2.65s/it]test:34/103 | accuracy 742  0.6819852941176471:  33%|███▎      | 34/103 [01:31<03:02,  2.65s/it]test:35/103 | accuracy 764  0.6821428571428572:  33%|███▎      | 34/103 [01:33<03:02,  2.65s/it]test:35/103 | accuracy 764  0.6821428571428572:  34%|███▍      | 35/103 [01:33<02:59,  2.64s/it]test:36/103 | accuracy 783  0.6796875:  34%|███▍      | 35/103 [01:36<02:59,  2.64s/it]         test:36/103 | accuracy 783  0.6796875:  35%|███▍      | 36/103 [01:36<02:55,  2.62s/it]test:37/103 | accuracy 810  0.6841216216216216:  35%|███▍      | 36/103 [01:38<02:55,  2.62s/it]test:37/103 | accuracy 810  0.6841216216216216:  36%|███▌      | 37/103 [01:38<02:53,  2.64s/it]test:38/103 | accuracy 835  0.6866776315789473:  36%|███▌      | 37/103 [01:41<02:53,  2.64s/it]test:38/103 | accuracy 835  0.6866776315789473:  37%|███▋      | 38/103 [01:41<02:51,  2.64s/it]test:39/103 | accuracy 850  0.6810897435897436:  37%|███▋      | 38/103 [01:44<02:51,  2.64s/it]test:39/103 | accuracy 850  0.6810897435897436:  38%|███▊      | 39/103 [01:44<02:48,  2.63s/it]test:40/103 | accuracy 871  0.68046875:  38%|███▊      | 39/103 [01:46<02:48,  2.63s/it]        test:40/103 | accuracy 871  0.68046875:  39%|███▉      | 40/103 [01:46<02:46,  2.64s/it]test:41/103 | accuracy 895  0.6821646341463414:  39%|███▉      | 40/103 [01:49<02:46,  2.64s/it]test:41/103 | accuracy 895  0.6821646341463414:  40%|███▉      | 41/103 [01:49<02:43,  2.64s/it]test:42/103 | accuracy 920  0.6845238095238095:  40%|███▉      | 41/103 [01:52<02:43,  2.64s/it]test:42/103 | accuracy 920  0.6845238095238095:  41%|████      | 42/103 [01:52<02:40,  2.63s/it]test:43/103 | accuracy 940  0.6831395348837209:  41%|████      | 42/103 [01:54<02:40,  2.63s/it]test:43/103 | accuracy 940  0.6831395348837209:  42%|████▏     | 43/103 [01:54<02:37,  2.62s/it]test:44/103 | accuracy 959  0.6811079545454546:  42%|████▏     | 43/103 [01:57<02:37,  2.62s/it]test:44/103 | accuracy 959  0.6811079545454546:  43%|████▎     | 44/103 [01:57<02:34,  2.62s/it]test:45/103 | accuracy 982  0.6819444444444445:  43%|████▎     | 44/103 [01:59<02:34,  2.62s/it]test:45/103 | accuracy 982  0.6819444444444445:  44%|████▎     | 45/103 [01:59<02:31,  2.61s/it]test:46/103 | accuracy 1006  0.6834239130434783:  44%|████▎     | 45/103 [02:02<02:31,  2.61s/it]test:46/103 | accuracy 1006  0.6834239130434783:  45%|████▍     | 46/103 [02:02<02:30,  2.64s/it]test:47/103 | accuracy 1028  0.6835106382978723:  45%|████▍     | 46/103 [02:05<02:30,  2.64s/it]test:47/103 | accuracy 1028  0.6835106382978723:  46%|████▌     | 47/103 [02:05<02:29,  2.67s/it]test:48/103 | accuracy 1051  0.6842447916666666:  46%|████▌     | 47/103 [02:08<02:29,  2.67s/it]test:48/103 | accuracy 1051  0.6842447916666666:  47%|████▋     | 48/103 [02:08<02:26,  2.67s/it]test:49/103 | accuracy 1074  0.6849489795918368:  47%|████▋     | 48/103 [02:10<02:26,  2.67s/it]test:49/103 | accuracy 1074  0.6849489795918368:  48%|████▊     | 49/103 [02:10<02:23,  2.66s/it]test:50/103 | accuracy 1092  0.6825:  48%|████▊     | 49/103 [02:13<02:23,  2.66s/it]            test:50/103 | accuracy 1092  0.6825:  49%|████▊     | 50/103 [02:13<02:20,  2.65s/it]test:51/103 | accuracy 1115  0.6832107843137255:  49%|████▊     | 50/103 [02:15<02:20,  2.65s/it]test:51/103 | accuracy 1115  0.6832107843137255:  50%|████▉     | 51/103 [02:15<02:17,  2.65s/it]test:52/103 | accuracy 1138  0.6838942307692307:  50%|████▉     | 51/103 [02:18<02:17,  2.65s/it]test:52/103 | accuracy 1138  0.6838942307692307:  50%|█████     | 52/103 [02:18<02:14,  2.64s/it]test:53/103 | accuracy 1160  0.6839622641509434:  50%|█████     | 52/103 [02:21<02:14,  2.64s/it]test:53/103 | accuracy 1160  0.6839622641509434:  51%|█████▏    | 53/103 [02:21<02:11,  2.64s/it]test:54/103 | accuracy 1180  0.6828703703703703:  51%|█████▏    | 53/103 [02:23<02:11,  2.64s/it]test:54/103 | accuracy 1180  0.6828703703703703:  52%|█████▏    | 54/103 [02:23<02:09,  2.64s/it]test:55/103 | accuracy 1203  0.6835227272727272:  52%|█████▏    | 54/103 [02:26<02:09,  2.64s/it]test:55/103 | accuracy 1203  0.6835227272727272:  53%|█████▎    | 55/103 [02:26<02:06,  2.64s/it]test:56/103 | accuracy 1225  0.68359375:  53%|█████▎    | 55/103 [02:29<02:06,  2.64s/it]        test:56/103 | accuracy 1225  0.68359375:  54%|█████▍    | 56/103 [02:29<02:04,  2.64s/it]test:57/103 | accuracy 1247  0.6836622807017544:  54%|█████▍    | 56/103 [02:31<02:04,  2.64s/it]test:57/103 | accuracy 1247  0.6836622807017544:  55%|█████▌    | 57/103 [02:31<02:01,  2.64s/it]test:58/103 | accuracy 1269  0.6837284482758621:  55%|█████▌    | 57/103 [02:34<02:01,  2.64s/it]test:58/103 | accuracy 1269  0.6837284482758621:  56%|█████▋    | 58/103 [02:34<01:59,  2.66s/it]test:59/103 | accuracy 1293  0.6848516949152542:  56%|█████▋    | 58/103 [02:37<01:59,  2.66s/it]test:59/103 | accuracy 1293  0.6848516949152542:  57%|█████▋    | 59/103 [02:37<01:57,  2.67s/it]test:60/103 | accuracy 1314  0.684375:  57%|█████▋    | 59/103 [02:39<01:57,  2.67s/it]          test:60/103 | accuracy 1314  0.684375:  58%|█████▊    | 60/103 [02:39<01:54,  2.66s/it]test:61/103 | accuracy 1336  0.6844262295081968:  58%|█████▊    | 60/103 [02:42<01:54,  2.66s/it]test:61/103 | accuracy 1336  0.6844262295081968:  59%|█████▉    | 61/103 [02:42<01:51,  2.65s/it]test:62/103 | accuracy 1359  0.6849798387096774:  59%|█████▉    | 61/103 [02:45<01:51,  2.65s/it]test:62/103 | accuracy 1359  0.6849798387096774:  60%|██████    | 62/103 [02:45<01:48,  2.65s/it]test:63/103 | accuracy 1379  0.6840277777777778:  60%|██████    | 62/103 [02:47<01:48,  2.65s/it]test:63/103 | accuracy 1379  0.6840277777777778:  61%|██████    | 63/103 [02:47<01:45,  2.65s/it]test:64/103 | accuracy 1408  0.6875:  61%|██████    | 63/103 [02:50<01:45,  2.65s/it]            test:64/103 | accuracy 1408  0.6875:  62%|██████▏   | 64/103 [02:50<01:43,  2.64s/it]test:65/103 | accuracy 1427  0.6860576923076923:  62%|██████▏   | 64/103 [02:52<01:43,  2.64s/it]test:65/103 | accuracy 1427  0.6860576923076923:  63%|██████▎   | 65/103 [02:53<01:40,  2.64s/it]test:66/103 | accuracy 1447  0.6851325757575758:  63%|██████▎   | 65/103 [02:55<01:40,  2.64s/it]test:66/103 | accuracy 1447  0.6851325757575758:  64%|██████▍   | 66/103 [02:55<01:38,  2.66s/it]test:67/103 | accuracy 1469  0.6851679104477612:  64%|██████▍   | 66/103 [02:58<01:38,  2.66s/it]test:67/103 | accuracy 1469  0.6851679104477612:  65%|██████▌   | 67/103 [02:58<01:35,  2.66s/it]test:68/103 | accuracy 1486  0.6829044117647058:  65%|██████▌   | 67/103 [03:01<01:35,  2.66s/it]test:68/103 | accuracy 1486  0.6829044117647058:  66%|██████▌   | 68/103 [03:01<01:33,  2.66s/it]test:69/103 | accuracy 1511  0.6843297101449275:  66%|██████▌   | 68/103 [03:03<01:33,  2.66s/it]test:69/103 | accuracy 1511  0.6843297101449275:  67%|██████▋   | 69/103 [03:03<01:30,  2.67s/it]test:70/103 | accuracy 1532  0.6839285714285714:  67%|██████▋   | 69/103 [03:06<01:30,  2.67s/it]test:70/103 | accuracy 1532  0.6839285714285714:  68%|██████▊   | 70/103 [03:06<01:27,  2.66s/it]test:71/103 | accuracy 1557  0.6852992957746479:  68%|██████▊   | 70/103 [03:08<01:27,  2.66s/it]test:71/103 | accuracy 1557  0.6852992957746479:  69%|██████▉   | 71/103 [03:09<01:24,  2.65s/it]test:72/103 | accuracy 1580  0.6857638888888888:  69%|██████▉   | 71/103 [03:11<01:24,  2.65s/it]test:72/103 | accuracy 1580  0.6857638888888888:  70%|██████▉   | 72/103 [03:11<01:22,  2.65s/it]test:73/103 | accuracy 1603  0.6862157534246576:  70%|██████▉   | 72/103 [03:14<01:22,  2.65s/it]test:73/103 | accuracy 1603  0.6862157534246576:  71%|███████   | 73/103 [03:14<01:19,  2.65s/it]test:74/103 | accuracy 1627  0.6870777027027027:  71%|███████   | 73/103 [03:16<01:19,  2.65s/it]test:74/103 | accuracy 1627  0.6870777027027027:  72%|███████▏  | 74/103 [03:16<01:17,  2.66s/it]test:75/103 | accuracy 1648  0.6866666666666666:  72%|███████▏  | 74/103 [03:19<01:17,  2.66s/it]test:75/103 | accuracy 1648  0.6866666666666666:  73%|███████▎  | 75/103 [03:19<01:14,  2.65s/it]test:76/103 | accuracy 1670  0.6866776315789473:  73%|███████▎  | 75/103 [03:22<01:14,  2.65s/it]test:76/103 | accuracy 1670  0.6866776315789473:  74%|███████▍  | 76/103 [03:22<01:11,  2.66s/it]test:77/103 | accuracy 1691  0.6862824675324676:  74%|███████▍  | 76/103 [03:24<01:11,  2.66s/it]test:77/103 | accuracy 1691  0.6862824675324676:  75%|███████▍  | 77/103 [03:24<01:08,  2.64s/it]test:78/103 | accuracy 1715  0.6870993589743589:  75%|███████▍  | 77/103 [03:27<01:08,  2.64s/it]test:78/103 | accuracy 1715  0.6870993589743589:  76%|███████▌  | 78/103 [03:27<01:07,  2.69s/it]test:79/103 | accuracy 1736  0.6867088607594937:  76%|███████▌  | 78/103 [03:30<01:07,  2.69s/it]test:79/103 | accuracy 1736  0.6867088607594937:  77%|███████▋  | 79/103 [03:30<01:04,  2.68s/it]test:80/103 | accuracy 1754  0.68515625:  77%|███████▋  | 79/103 [03:32<01:04,  2.68s/it]        test:80/103 | accuracy 1754  0.68515625:  78%|███████▊  | 80/103 [03:32<01:01,  2.66s/it]test:81/103 | accuracy 1772  0.683641975308642:  78%|███████▊  | 80/103 [03:35<01:01,  2.66s/it]test:81/103 | accuracy 1772  0.683641975308642:  79%|███████▊  | 81/103 [03:35<00:58,  2.67s/it]test:82/103 | accuracy 1796  0.6844512195121951:  79%|███████▊  | 81/103 [03:38<00:58,  2.67s/it]test:82/103 | accuracy 1796  0.6844512195121951:  80%|███████▉  | 82/103 [03:38<00:55,  2.67s/it]test:83/103 | accuracy 1820  0.6852409638554217:  80%|███████▉  | 82/103 [03:40<00:55,  2.67s/it]test:83/103 | accuracy 1820  0.6852409638554217:  81%|████████  | 83/103 [03:40<00:53,  2.66s/it]test:84/103 | accuracy 1842  0.6852678571428571:  81%|████████  | 83/103 [03:43<00:53,  2.66s/it]test:84/103 | accuracy 1842  0.6852678571428571:  82%|████████▏ | 84/103 [03:43<00:51,  2.69s/it]test:85/103 | accuracy 1864  0.6852941176470588:  82%|████████▏ | 84/103 [03:46<00:51,  2.69s/it]test:85/103 | accuracy 1864  0.6852941176470588:  83%|████████▎ | 85/103 [03:46<00:48,  2.70s/it]test:86/103 | accuracy 1887  0.6856831395348837:  83%|████████▎ | 85/103 [03:49<00:48,  2.70s/it]test:86/103 | accuracy 1887  0.6856831395348837:  83%|████████▎ | 86/103 [03:49<00:45,  2.68s/it]test:87/103 | accuracy 1903  0.6835488505747126:  83%|████████▎ | 86/103 [03:51<00:45,  2.68s/it]test:87/103 | accuracy 1903  0.6835488505747126:  84%|████████▍ | 87/103 [03:51<00:42,  2.66s/it]test:88/103 | accuracy 1924  0.6832386363636364:  84%|████████▍ | 87/103 [03:54<00:42,  2.66s/it]test:88/103 | accuracy 1924  0.6832386363636364:  85%|████████▌ | 88/103 [03:54<00:39,  2.65s/it]test:89/103 | accuracy 1947  0.6836376404494382:  85%|████████▌ | 88/103 [03:56<00:39,  2.65s/it]test:89/103 | accuracy 1947  0.6836376404494382:  86%|████████▋ | 89/103 [03:56<00:37,  2.65s/it]test:90/103 | accuracy 1968  0.6833333333333333:  86%|████████▋ | 89/103 [03:59<00:37,  2.65s/it]test:90/103 | accuracy 1968  0.6833333333333333:  87%|████████▋ | 90/103 [03:59<00:34,  2.67s/it]test:91/103 | accuracy 1989  0.6830357142857143:  87%|████████▋ | 90/103 [04:02<00:34,  2.67s/it]test:91/103 | accuracy 1989  0.6830357142857143:  88%|████████▊ | 91/103 [04:02<00:31,  2.67s/it]test:92/103 | accuracy 2010  0.6827445652173914:  88%|████████▊ | 91/103 [04:05<00:31,  2.67s/it]test:92/103 | accuracy 2010  0.6827445652173914:  89%|████████▉ | 92/103 [04:05<00:29,  2.69s/it]test:93/103 | accuracy 2035  0.6838037634408602:  89%|████████▉ | 92/103 [04:07<00:29,  2.69s/it]test:93/103 | accuracy 2035  0.6838037634408602:  90%|█████████ | 93/103 [04:07<00:27,  2.70s/it]test:94/103 | accuracy 2055  0.6831781914893617:  90%|█████████ | 93/103 [04:10<00:27,  2.70s/it]test:94/103 | accuracy 2055  0.6831781914893617:  91%|█████████▏| 94/103 [04:10<00:24,  2.69s/it]test:95/103 | accuracy 2077  0.6832236842105263:  91%|█████████▏| 94/103 [04:13<00:24,  2.69s/it]test:95/103 | accuracy 2077  0.6832236842105263:  92%|█████████▏| 95/103 [04:13<00:21,  2.69s/it]test:96/103 | accuracy 2104  0.6848958333333334:  92%|█████████▏| 95/103 [04:15<00:21,  2.69s/it]test:96/103 | accuracy 2104  0.6848958333333334:  93%|█████████▎| 96/103 [04:15<00:18,  2.69s/it]test:97/103 | accuracy 2129  0.685889175257732:  93%|█████████▎| 96/103 [04:18<00:18,  2.69s/it] test:97/103 | accuracy 2129  0.685889175257732:  94%|█████████▍| 97/103 [04:18<00:16,  2.69s/it]test:98/103 | accuracy 2154  0.6868622448979592:  94%|█████████▍| 97/103 [04:21<00:16,  2.69s/it]test:98/103 | accuracy 2154  0.6868622448979592:  95%|█████████▌| 98/103 [04:21<00:13,  2.68s/it]test:99/103 | accuracy 2181  0.6884469696969697:  95%|█████████▌| 98/103 [04:23<00:13,  2.68s/it]test:99/103 | accuracy 2181  0.6884469696969697:  96%|█████████▌| 99/103 [04:23<00:10,  2.68s/it]test:100/103 | accuracy 2203  0.6884375:  96%|█████████▌| 99/103 [04:26<00:10,  2.68s/it]        test:100/103 | accuracy 2203  0.6884375:  97%|█████████▋| 100/103 [04:26<00:08,  2.67s/it]test:101/103 | accuracy 2223  0.687809405940594:  97%|█████████▋| 100/103 [04:29<00:08,  2.67s/it]test:101/103 | accuracy 2223  0.687809405940594:  98%|█████████▊| 101/103 [04:29<00:05,  2.69s/it]test:102/103 | accuracy 2247  0.6884191176470589:  98%|█████████▊| 101/103 [04:31<00:05,  2.69s/it]test:102/103 | accuracy 2247  0.6884191176470589:  99%|█████████▉| 102/103 [04:31<00:02,  2.69s/it]test:103/103 | accuracy 2251  0.6883792048929663:  99%|█████████▉| 102/103 [04:32<00:02,  2.69s/it]test:103/103 | accuracy 2251  0.6883792048929663: 100%|██████████| 103/103 [04:32<00:00,  2.19s/it]test:103/103 | accuracy 2251  0.6883792048929663: 100%|██████████| 103/103 [04:32<00:00,  2.65s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.4/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 4,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.94s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.94s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:31,  1.94s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.91s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.89s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.88s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.87s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.88s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:18,  1.88s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:18<00:16,  1.88s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.88s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.88s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.88s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.87s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.87s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.88s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.87s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:33<00:01,  1.88s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.87s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.4 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors
##### Loading 256 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors #####
  0%|          | 0/58 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/58 | accuracy 23  0.71875:   0%|          | 0/58 [00:09<?, ?it/s]test:1/58 | accuracy 23  0.71875:   2%|▏         | 1/58 [00:09<09:28,  9.98s/it]test:2/58 | accuracy 52  0.8125:   2%|▏         | 1/58 [00:15<09:28,  9.98s/it] test:2/58 | accuracy 52  0.8125:   3%|▎         | 2/58 [00:15<07:07,  7.64s/it]test:3/58 | accuracy 77  0.8020833333333334:   3%|▎         | 2/58 [00:22<07:07,  7.64s/it]test:3/58 | accuracy 77  0.8020833333333334:   5%|▌         | 3/58 [00:22<06:26,  7.03s/it]test:4/58 | accuracy 107  0.8359375:   5%|▌         | 3/58 [00:28<06:26,  7.03s/it]        test:4/58 | accuracy 107  0.8359375:   7%|▋         | 4/58 [00:28<05:58,  6.63s/it]test:5/58 | accuracy 132  0.825:   7%|▋         | 4/58 [00:38<05:58,  6.63s/it]    test:5/58 | accuracy 132  0.825:   9%|▊         | 5/58 [00:38<06:58,  7.90s/it]test:6/58 | accuracy 159  0.828125:   9%|▊         | 5/58 [00:45<06:58,  7.90s/it]test:6/58 | accuracy 159  0.828125:  10%|█         | 6/58 [00:45<06:36,  7.63s/it]test:7/58 | accuracy 190  0.8482142857142857:  10%|█         | 6/58 [00:52<06:36,  7.63s/it]test:7/58 | accuracy 190  0.8482142857142857:  12%|█▏        | 7/58 [00:52<06:20,  7.46s/it]test:8/58 | accuracy 217  0.84765625:  12%|█▏        | 7/58 [01:00<06:20,  7.46s/it]        test:8/58 | accuracy 217  0.84765625:  14%|█▍        | 8/58 [01:00<06:21,  7.64s/it]test:9/58 | accuracy 246  0.8541666666666666:  14%|█▍        | 8/58 [01:08<06:21,  7.64s/it]test:9/58 | accuracy 246  0.8541666666666666:  16%|█▌        | 9/58 [01:08<06:24,  7.84s/it]Traceback (most recent call last):
  File "commonsense_evaluate.py", line 346, in <module>
    main()
  File "commonsense_evaluate.py", line 124, in main
    prompts,outputs = evaluate(instructions)
  File "commonsense_evaluate.py", line 94, in evaluate
    generation_output = model.generate(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py", line 2063, in generate
    result = self._beam_search(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py", line 3238, in _beam_search
    outputs = self(**model_inputs, return_dict=True)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 1535, in forward
    outputs = self.model(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 1395, in forward
    layer_outputs = decoder_layer(
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 1113, in forward
    hidden_states, router_logits, shared_routing_adapter_router_logits = self.block_sparse_moe(hidden_states)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 994, in forward
    current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list, None]
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/hkfs/home/project/hk-project-p0022189/hgf_mxv5488/PERFUME/mixtral_modification/modeling_mixtral.py", line 882, in forward
    current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.68 GiB. GPU 1 has a total capacity of 39.38 GiB of which 523.50 MiB is free. Including non-PyTorch memory, this process has 38.83 GiB memory in use. Of the allocated memory 34.24 GiB is allocated by PyTorch, and 4.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
test:9/58 | accuracy 246  0.8541666666666666:  16%|█▌        | 9/58 [01:15<06:52,  8.41s/it]MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.4/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 4,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.91s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.91s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.90s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.88s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.88s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.89s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.87s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.88s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:16<00:18,  1.89s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:18<00:16,  1.88s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.88s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.88s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.88s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.87s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.89s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.89s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:31<00:03,  1.87s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:33<00:01,  1.88s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.87s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.4 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors
##### Loading 256 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors #####
  0%|          | 0/62 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/62 | accuracy 20  0.625:   0%|          | 0/62 [00:04<?, ?it/s]test:1/62 | accuracy 20  0.625:   2%|▏         | 1/62 [00:04<05:00,  4.93s/it]test:2/62 | accuracy 45  0.703125:   2%|▏         | 1/62 [00:08<05:00,  4.93s/it]test:2/62 | accuracy 45  0.703125:   3%|▎         | 2/62 [00:08<04:13,  4.23s/it]test:3/62 | accuracy 68  0.7083333333333334:   3%|▎         | 2/62 [00:12<04:13,  4.23s/it]test:3/62 | accuracy 68  0.7083333333333334:   5%|▍         | 3/62 [00:12<03:57,  4.02s/it]test:4/62 | accuracy 91  0.7109375:   5%|▍         | 3/62 [00:16<03:57,  4.02s/it]         test:4/62 | accuracy 91  0.7109375:   6%|▋         | 4/62 [00:16<03:47,  3.92s/it]test:5/62 | accuracy 119  0.74375:   6%|▋         | 4/62 [00:19<03:47,  3.92s/it] test:5/62 | accuracy 119  0.74375:   8%|▊         | 5/62 [00:20<03:41,  3.88s/it]test:6/62 | accuracy 148  0.7708333333333334:   8%|▊         | 5/62 [00:23<03:41,  3.88s/it]test:6/62 | accuracy 148  0.7708333333333334:  10%|▉         | 6/62 [00:23<03:32,  3.79s/it]test:7/62 | accuracy 175  0.78125:  10%|▉         | 6/62 [00:27<03:32,  3.79s/it]           test:7/62 | accuracy 175  0.78125:  11%|█▏        | 7/62 [00:27<03:34,  3.90s/it]test:8/62 | accuracy 201  0.78515625:  11%|█▏        | 7/62 [00:31<03:34,  3.90s/it]test:8/62 | accuracy 201  0.78515625:  13%|█▎        | 8/62 [00:31<03:29,  3.89s/it]test:9/62 | accuracy 224  0.7777777777777778:  13%|█▎        | 8/62 [00:35<03:29,  3.89s/it]test:9/62 | accuracy 224  0.7777777777777778:  15%|█▍        | 9/62 [00:35<03:31,  4.00s/it]test:10/62 | accuracy 247  0.771875:  15%|█▍        | 9/62 [00:39<03:31,  4.00s/it]         test:10/62 | accuracy 247  0.771875:  16%|█▌        | 10/62 [00:39<03:25,  3.95s/it]test:11/62 | accuracy 274  0.7784090909090909:  16%|█▌        | 10/62 [00:43<03:25,  3.95s/it]test:11/62 | accuracy 274  0.7784090909090909:  18%|█▊        | 11/62 [00:43<03:21,  3.95s/it]test:12/62 | accuracy 300  0.78125:  18%|█▊        | 11/62 [00:48<03:21,  3.95s/it]           test:12/62 | accuracy 300  0.78125:  19%|█▉        | 12/62 [00:48<03:24,  4.08s/it]test:13/62 | accuracy 328  0.7884615384615384:  19%|█▉        | 12/62 [00:52<03:24,  4.08s/it]test:13/62 | accuracy 328  0.7884615384615384:  21%|██        | 13/62 [00:52<03:22,  4.14s/it]test:14/62 | accuracy 353  0.7879464285714286:  21%|██        | 13/62 [00:56<03:22,  4.14s/it]test:14/62 | accuracy 353  0.7879464285714286:  23%|██▎       | 14/62 [00:56<03:16,  4.09s/it]test:15/62 | accuracy 378  0.7875:  23%|██▎       | 14/62 [01:00<03:16,  4.09s/it]            test:15/62 | accuracy 378  0.7875:  24%|██▍       | 15/62 [01:00<03:12,  4.09s/it]test:16/62 | accuracy 405  0.791015625:  24%|██▍       | 15/62 [01:04<03:12,  4.09s/it]test:16/62 | accuracy 405  0.791015625:  26%|██▌       | 16/62 [01:04<03:09,  4.11s/it]test:17/62 | accuracy 434  0.7977941176470589:  26%|██▌       | 16/62 [01:08<03:09,  4.11s/it]test:17/62 | accuracy 434  0.7977941176470589:  27%|██▋       | 17/62 [01:08<03:02,  4.04s/it]test:18/62 | accuracy 460  0.7986111111111112:  27%|██▋       | 17/62 [01:12<03:02,  4.04s/it]test:18/62 | accuracy 460  0.7986111111111112:  29%|██▉       | 18/62 [01:12<02:52,  3.93s/it]test:19/62 | accuracy 482  0.7927631578947368:  29%|██▉       | 18/62 [01:15<02:52,  3.93s/it]test:19/62 | accuracy 482  0.7927631578947368:  31%|███       | 19/62 [01:15<02:46,  3.87s/it]test:20/62 | accuracy 504  0.7875:  31%|███       | 19/62 [01:19<02:46,  3.87s/it]            test:20/62 | accuracy 504  0.7875:  32%|███▏      | 20/62 [01:19<02:43,  3.90s/it]test:21/62 | accuracy 532  0.7916666666666666:  32%|███▏      | 20/62 [01:23<02:43,  3.90s/it]test:21/62 | accuracy 532  0.7916666666666666:  34%|███▍      | 21/62 [01:23<02:38,  3.86s/it]test:22/62 | accuracy 559  0.7940340909090909:  34%|███▍      | 21/62 [01:27<02:38,  3.86s/it]test:22/62 | accuracy 559  0.7940340909090909:  35%|███▌      | 22/62 [01:27<02:31,  3.80s/it]test:23/62 | accuracy 586  0.7961956521739131:  35%|███▌      | 22/62 [01:31<02:31,  3.80s/it]test:23/62 | accuracy 586  0.7961956521739131:  37%|███▋      | 23/62 [01:31<02:33,  3.94s/it]test:24/62 | accuracy 614  0.7994791666666666:  37%|███▋      | 23/62 [01:35<02:33,  3.94s/it]test:24/62 | accuracy 614  0.7994791666666666:  39%|███▊      | 24/62 [01:35<02:35,  4.10s/it]test:25/62 | accuracy 636  0.795:  39%|███▊      | 24/62 [01:39<02:35,  4.10s/it]             test:25/62 | accuracy 636  0.795:  40%|████      | 25/62 [01:39<02:28,  4.02s/it]test:26/62 | accuracy 657  0.7896634615384616:  40%|████      | 25/62 [01:43<02:28,  4.02s/it]test:26/62 | accuracy 657  0.7896634615384616:  42%|████▏     | 26/62 [01:43<02:21,  3.94s/it]test:27/62 | accuracy 681  0.7881944444444444:  42%|████▏     | 26/62 [01:47<02:21,  3.94s/it]test:27/62 | accuracy 681  0.7881944444444444:  44%|████▎     | 27/62 [01:47<02:19,  3.97s/it]test:28/62 | accuracy 706  0.7879464285714286:  44%|████▎     | 27/62 [01:51<02:19,  3.97s/it]test:28/62 | accuracy 706  0.7879464285714286:  45%|████▌     | 28/62 [01:51<02:13,  3.94s/it]test:29/62 | accuracy 728  0.7844827586206896:  45%|████▌     | 28/62 [01:55<02:13,  3.94s/it]test:29/62 | accuracy 728  0.7844827586206896:  47%|████▋     | 29/62 [01:55<02:11,  3.97s/it]test:30/62 | accuracy 749  0.7802083333333333:  47%|████▋     | 29/62 [01:59<02:11,  3.97s/it]test:30/62 | accuracy 749  0.7802083333333333:  48%|████▊     | 30/62 [01:59<02:06,  3.94s/it]test:31/62 | accuracy 776  0.782258064516129:  48%|████▊     | 30/62 [02:02<02:06,  3.94s/it] test:31/62 | accuracy 776  0.782258064516129:  50%|█████     | 31/62 [02:02<01:58,  3.83s/it]test:32/62 | accuracy 803  0.7841796875:  50%|█████     | 31/62 [02:06<01:58,  3.83s/it]     test:32/62 | accuracy 803  0.7841796875:  52%|█████▏    | 32/62 [02:06<01:54,  3.82s/it]test:33/62 | accuracy 829  0.7850378787878788:  52%|█████▏    | 32/62 [02:10<01:54,  3.82s/it]test:33/62 | accuracy 829  0.7850378787878788:  53%|█████▎    | 33/62 [02:10<01:49,  3.79s/it]test:34/62 | accuracy 851  0.7821691176470589:  53%|█████▎    | 33/62 [02:14<01:49,  3.79s/it]test:34/62 | accuracy 851  0.7821691176470589:  55%|█████▍    | 34/62 [02:14<01:47,  3.84s/it]test:35/62 | accuracy 879  0.7848214285714286:  55%|█████▍    | 34/62 [02:18<01:47,  3.84s/it]test:35/62 | accuracy 879  0.7848214285714286:  56%|█████▋    | 35/62 [02:18<01:47,  4.00s/it]test:36/62 | accuracy 903  0.7838541666666666:  56%|█████▋    | 35/62 [02:22<01:47,  4.00s/it]test:36/62 | accuracy 903  0.7838541666666666:  58%|█████▊    | 36/62 [02:22<01:40,  3.88s/it]test:37/62 | accuracy 929  0.7846283783783784:  58%|█████▊    | 36/62 [02:26<01:40,  3.88s/it]test:37/62 | accuracy 929  0.7846283783783784:  60%|█████▉    | 37/62 [02:26<01:37,  3.89s/it]test:38/62 | accuracy 958  0.787828947368421:  60%|█████▉    | 37/62 [02:30<01:37,  3.89s/it] test:38/62 | accuracy 958  0.787828947368421:  61%|██████▏   | 38/62 [02:30<01:32,  3.86s/it]test:39/62 | accuracy 983  0.7876602564102564:  61%|██████▏   | 38/62 [02:33<01:32,  3.86s/it]test:39/62 | accuracy 983  0.7876602564102564:  63%|██████▎   | 39/62 [02:33<01:28,  3.85s/it]test:40/62 | accuracy 1009  0.78828125:  63%|██████▎   | 39/62 [02:37<01:28,  3.85s/it]       test:40/62 | accuracy 1009  0.78828125:  65%|██████▍   | 40/62 [02:37<01:24,  3.86s/it]test:41/62 | accuracy 1036  0.7896341463414634:  65%|██████▍   | 40/62 [02:41<01:24,  3.86s/it]test:41/62 | accuracy 1036  0.7896341463414634:  66%|██████▌   | 41/62 [02:41<01:20,  3.83s/it]test:42/62 | accuracy 1058  0.7872023809523809:  66%|██████▌   | 41/62 [02:45<01:20,  3.83s/it]test:42/62 | accuracy 1058  0.7872023809523809:  68%|██████▊   | 42/62 [02:45<01:16,  3.81s/it]test:43/62 | accuracy 1083  0.7870639534883721:  68%|██████▊   | 42/62 [02:49<01:16,  3.81s/it]test:43/62 | accuracy 1083  0.7870639534883721:  69%|██████▉   | 43/62 [02:49<01:12,  3.80s/it]test:44/62 | accuracy 1105  0.7848011363636364:  69%|██████▉   | 43/62 [02:52<01:12,  3.80s/it]test:44/62 | accuracy 1105  0.7848011363636364:  71%|███████   | 44/62 [02:52<01:07,  3.77s/it]test:45/62 | accuracy 1132  0.7861111111111111:  71%|███████   | 44/62 [02:56<01:07,  3.77s/it]test:45/62 | accuracy 1132  0.7861111111111111:  73%|███████▎  | 45/62 [02:56<01:05,  3.84s/it]test:46/62 | accuracy 1154  0.7839673913043478:  73%|███████▎  | 45/62 [03:00<01:05,  3.84s/it]test:46/62 | accuracy 1154  0.7839673913043478:  74%|███████▍  | 46/62 [03:00<01:02,  3.91s/it]test:47/62 | accuracy 1177  0.7825797872340425:  74%|███████▍  | 46/62 [03:04<01:02,  3.91s/it]test:47/62 | accuracy 1177  0.7825797872340425:  76%|███████▌  | 47/62 [03:04<00:59,  3.97s/it]test:48/62 | accuracy 1201  0.7819010416666666:  76%|███████▌  | 47/62 [03:08<00:59,  3.97s/it]test:48/62 | accuracy 1201  0.7819010416666666:  77%|███████▋  | 48/62 [03:08<00:54,  3.90s/it]test:49/62 | accuracy 1226  0.7818877551020408:  77%|███████▋  | 48/62 [03:12<00:54,  3.90s/it]test:49/62 | accuracy 1226  0.7818877551020408:  79%|███████▉  | 49/62 [03:12<00:49,  3.79s/it]test:50/62 | accuracy 1251  0.781875:  79%|███████▉  | 49/62 [03:16<00:49,  3.79s/it]          test:50/62 | accuracy 1251  0.781875:  81%|████████  | 50/62 [03:16<00:46,  3.84s/it]test:51/62 | accuracy 1276  0.7818627450980392:  81%|████████  | 50/62 [03:20<00:46,  3.84s/it]test:51/62 | accuracy 1276  0.7818627450980392:  82%|████████▏ | 51/62 [03:20<00:42,  3.88s/it]test:52/62 | accuracy 1305  0.7842548076923077:  82%|████████▏ | 51/62 [03:23<00:42,  3.88s/it]test:52/62 | accuracy 1305  0.7842548076923077:  84%|████████▍ | 52/62 [03:24<00:38,  3.87s/it]test:53/62 | accuracy 1331  0.7847877358490566:  84%|████████▍ | 52/62 [03:27<00:38,  3.87s/it]test:53/62 | accuracy 1331  0.7847877358490566:  85%|████████▌ | 53/62 [03:27<00:34,  3.88s/it]test:54/62 | accuracy 1354  0.7835648148148148:  85%|████████▌ | 53/62 [03:31<00:34,  3.88s/it]test:54/62 | accuracy 1354  0.7835648148148148:  87%|████████▋ | 54/62 [03:31<00:31,  3.88s/it]test:55/62 | accuracy 1380  0.7840909090909091:  87%|████████▋ | 54/62 [03:35<00:31,  3.88s/it]test:55/62 | accuracy 1380  0.7840909090909091:  89%|████████▊ | 55/62 [03:35<00:26,  3.86s/it]test:56/62 | accuracy 1408  0.7857142857142857:  89%|████████▊ | 55/62 [03:39<00:26,  3.86s/it]test:56/62 | accuracy 1408  0.7857142857142857:  90%|█████████ | 56/62 [03:39<00:22,  3.80s/it]test:57/62 | accuracy 1432  0.7850877192982456:  90%|█████████ | 56/62 [03:43<00:22,  3.80s/it]test:57/62 | accuracy 1432  0.7850877192982456:  92%|█████████▏| 57/62 [03:43<00:18,  3.80s/it]test:58/62 | accuracy 1458  0.7855603448275862:  92%|█████████▏| 57/62 [03:47<00:18,  3.80s/it]test:58/62 | accuracy 1458  0.7855603448275862:  94%|█████████▎| 58/62 [03:47<00:15,  3.90s/it]test:59/62 | accuracy 1486  0.7870762711864406:  94%|█████████▎| 58/62 [03:51<00:15,  3.90s/it]test:59/62 | accuracy 1486  0.7870762711864406:  95%|█████████▌| 59/62 [03:51<00:11,  3.90s/it]test:60/62 | accuracy 1514  0.7885416666666667:  95%|█████████▌| 59/62 [03:54<00:11,  3.90s/it]test:60/62 | accuracy 1514  0.7885416666666667:  97%|█████████▋| 60/62 [03:54<00:07,  3.86s/it]test:61/62 | accuracy 1541  0.7894467213114754:  97%|█████████▋| 60/62 [03:58<00:07,  3.86s/it]test:61/62 | accuracy 1541  0.7894467213114754:  98%|█████████▊| 61/62 [03:58<00:03,  3.85s/it]test:62/62 | accuracy 1543  0.7896622313203685:  98%|█████████▊| 61/62 [03:59<00:03,  3.85s/it]test:62/62 | accuracy 1543  0.7896622313203685: 100%|██████████| 62/62 [03:59<00:00,  2.93s/it]test:62/62 | accuracy 1543  0.7896622313203685: 100%|██████████| 62/62 [03:59<00:00,  3.86s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.4/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 4,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.94s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.91s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.91s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.91s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.90s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.89s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.87s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.87s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:16<00:18,  1.87s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:18<00:16,  1.87s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.88s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.88s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.88s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.87s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.87s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.87s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:31<00:03,  1.86s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:33<00:01,  1.87s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.86s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.4 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors
##### Loading 256 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors #####
  0%|          | 0/314 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/314 | accuracy 26  0.8125:   0%|          | 0/314 [00:06<?, ?it/s]test:1/314 | accuracy 26  0.8125:   0%|          | 1/314 [00:06<35:32,  6.81s/it]test:2/314 | accuracy 52  0.8125:   0%|          | 1/314 [00:12<35:32,  6.81s/it]test:2/314 | accuracy 52  0.8125:   1%|          | 2/314 [00:12<32:40,  6.28s/it]test:3/314 | accuracy 75  0.78125:   1%|          | 2/314 [00:18<32:40,  6.28s/it]test:3/314 | accuracy 75  0.78125:   1%|          | 3/314 [00:18<31:58,  6.17s/it]test:4/314 | accuracy 99  0.7734375:   1%|          | 3/314 [00:25<31:58,  6.17s/it]test:4/314 | accuracy 99  0.7734375:   1%|▏         | 4/314 [00:25<32:43,  6.33s/it]test:5/314 | accuracy 126  0.7875:   1%|▏         | 4/314 [00:32<32:43,  6.33s/it]  test:5/314 | accuracy 126  0.7875:   2%|▏         | 5/314 [00:32<33:31,  6.51s/it]test:6/314 | accuracy 151  0.7864583333333334:   2%|▏         | 5/314 [00:37<33:31,  6.51s/it]test:6/314 | accuracy 151  0.7864583333333334:   2%|▏         | 6/314 [00:37<31:52,  6.21s/it]test:7/314 | accuracy 177  0.7901785714285714:   2%|▏         | 6/314 [00:43<31:52,  6.21s/it]test:7/314 | accuracy 177  0.7901785714285714:   2%|▏         | 7/314 [00:43<31:37,  6.18s/it]test:8/314 | accuracy 203  0.79296875:   2%|▏         | 7/314 [00:49<31:37,  6.18s/it]        test:8/314 | accuracy 203  0.79296875:   3%|▎         | 8/314 [00:49<30:53,  6.06s/it]test:9/314 | accuracy 232  0.8055555555555556:   3%|▎         | 8/314 [00:55<30:53,  6.06s/it]test:9/314 | accuracy 232  0.8055555555555556:   3%|▎         | 9/314 [00:55<30:14,  5.95s/it]test:10/314 | accuracy 263  0.821875:   3%|▎         | 9/314 [01:01<30:14,  5.95s/it]         test:10/314 | accuracy 263  0.821875:   3%|▎         | 10/314 [01:01<30:42,  6.06s/it]test:11/314 | accuracy 288  0.8181818181818182:   3%|▎         | 10/314 [01:07<30:42,  6.06s/it]test:11/314 | accuracy 288  0.8181818181818182:   4%|▎         | 11/314 [01:07<30:34,  6.05s/it]test:12/314 | accuracy 312  0.8125:   4%|▎         | 11/314 [01:13<30:34,  6.05s/it]            test:12/314 | accuracy 312  0.8125:   4%|▍         | 12/314 [01:13<29:48,  5.92s/it]test:13/314 | accuracy 339  0.8149038461538461:   4%|▍         | 12/314 [01:19<29:48,  5.92s/it]test:13/314 | accuracy 339  0.8149038461538461:   4%|▍         | 13/314 [01:19<30:32,  6.09s/it]test:14/314 | accuracy 368  0.8214285714285714:   4%|▍         | 13/314 [01:25<30:32,  6.09s/it]test:14/314 | accuracy 368  0.8214285714285714:   4%|▍         | 14/314 [01:25<30:28,  6.10s/it]test:15/314 | accuracy 397  0.8270833333333333:   4%|▍         | 14/314 [01:32<30:28,  6.10s/it]test:15/314 | accuracy 397  0.8270833333333333:   5%|▍         | 15/314 [01:32<30:43,  6.17s/it]test:16/314 | accuracy 426  0.83203125:   5%|▍         | 15/314 [01:38<30:43,  6.17s/it]        test:16/314 | accuracy 426  0.83203125:   5%|▌         | 16/314 [01:38<30:52,  6.22s/it]test:17/314 | accuracy 452  0.8308823529411765:   5%|▌         | 16/314 [01:45<30:52,  6.22s/it]test:17/314 | accuracy 452  0.8308823529411765:   5%|▌         | 17/314 [01:45<32:27,  6.56s/it]test:18/314 | accuracy 479  0.8315972222222222:   5%|▌         | 17/314 [01:52<32:27,  6.56s/it]test:18/314 | accuracy 479  0.8315972222222222:   6%|▌         | 18/314 [01:52<31:39,  6.42s/it]test:19/314 | accuracy 506  0.8322368421052632:   6%|▌         | 18/314 [01:57<31:39,  6.42s/it]test:19/314 | accuracy 506  0.8322368421052632:   6%|▌         | 19/314 [01:57<30:06,  6.12s/it]test:20/314 | accuracy 531  0.8296875:   6%|▌         | 19/314 [02:03<30:06,  6.12s/it]         test:20/314 | accuracy 531  0.8296875:   6%|▋         | 20/314 [02:03<29:15,  5.97s/it]test:21/314 | accuracy 562  0.8363095238095238:   6%|▋         | 20/314 [02:09<29:15,  5.97s/it]test:21/314 | accuracy 562  0.8363095238095238:   7%|▋         | 21/314 [02:09<29:27,  6.03s/it]test:22/314 | accuracy 591  0.8394886363636364:   7%|▋         | 21/314 [02:16<29:27,  6.03s/it]test:22/314 | accuracy 591  0.8394886363636364:   7%|▋         | 22/314 [02:16<30:31,  6.27s/it]test:23/314 | accuracy 621  0.84375:   7%|▋         | 22/314 [02:21<30:31,  6.27s/it]           test:23/314 | accuracy 621  0.84375:   7%|▋         | 23/314 [02:21<29:38,  6.11s/it]test:24/314 | accuracy 650  0.8463541666666666:   7%|▋         | 23/314 [02:27<29:38,  6.11s/it]test:24/314 | accuracy 650  0.8463541666666666:   8%|▊         | 24/314 [02:27<28:25,  5.88s/it]test:25/314 | accuracy 675  0.84375:   8%|▊         | 24/314 [02:34<28:25,  5.88s/it]           test:25/314 | accuracy 675  0.84375:   8%|▊         | 25/314 [02:34<30:19,  6.30s/it]test:26/314 | accuracy 703  0.8449519230769231:   8%|▊         | 25/314 [02:40<30:19,  6.30s/it]test:26/314 | accuracy 703  0.8449519230769231:   8%|▊         | 26/314 [02:40<30:21,  6.33s/it]test:27/314 | accuracy 731  0.8460648148148148:   8%|▊         | 26/314 [02:47<30:21,  6.33s/it]test:27/314 | accuracy 731  0.8460648148148148:   9%|▊         | 27/314 [02:47<30:19,  6.34s/it]test:28/314 | accuracy 759  0.8470982142857143:   9%|▊         | 27/314 [02:53<30:19,  6.34s/it]test:28/314 | accuracy 759  0.8470982142857143:   9%|▉         | 28/314 [02:53<30:45,  6.45s/it]test:29/314 | accuracy 789  0.8502155172413793:   9%|▉         | 28/314 [02:59<30:45,  6.45s/it]test:29/314 | accuracy 789  0.8502155172413793:   9%|▉         | 29/314 [02:59<29:39,  6.24s/it]test:30/314 | accuracy 814  0.8479166666666667:   9%|▉         | 29/314 [03:05<29:39,  6.24s/it]test:30/314 | accuracy 814  0.8479166666666667:  10%|▉         | 30/314 [03:05<28:55,  6.11s/it]test:31/314 | accuracy 837  0.84375:  10%|▉         | 30/314 [03:11<28:55,  6.11s/it]           test:31/314 | accuracy 837  0.84375:  10%|▉         | 31/314 [03:11<28:33,  6.05s/it]test:32/314 | accuracy 863  0.8427734375:  10%|▉         | 31/314 [03:18<28:33,  6.05s/it]test:32/314 | accuracy 863  0.8427734375:  10%|█         | 32/314 [03:18<29:19,  6.24s/it]test:33/314 | accuracy 892  0.8446969696969697:  10%|█         | 32/314 [03:23<29:19,  6.24s/it]test:33/314 | accuracy 892  0.8446969696969697:  11%|█         | 33/314 [03:23<28:30,  6.09s/it]test:34/314 | accuracy 921  0.8465073529411765:  11%|█         | 33/314 [03:29<28:30,  6.09s/it]test:34/314 | accuracy 921  0.8465073529411765:  11%|█         | 34/314 [03:29<27:48,  5.96s/it]test:35/314 | accuracy 950  0.8482142857142857:  11%|█         | 34/314 [03:34<27:48,  5.96s/it]test:35/314 | accuracy 950  0.8482142857142857:  11%|█         | 35/314 [03:34<26:55,  5.79s/it]test:36/314 | accuracy 979  0.8498263888888888:  11%|█         | 35/314 [03:40<26:55,  5.79s/it]test:36/314 | accuracy 979  0.8498263888888888:  11%|█▏        | 36/314 [03:40<26:47,  5.78s/it]test:37/314 | accuracy 1008  0.8513513513513513:  11%|█▏        | 36/314 [03:47<26:47,  5.78s/it]test:37/314 | accuracy 1008  0.8513513513513513:  12%|█▏        | 37/314 [03:47<28:37,  6.20s/it]test:38/314 | accuracy 1036  0.8519736842105263:  12%|█▏        | 37/314 [03:53<28:37,  6.20s/it]test:38/314 | accuracy 1036  0.8519736842105263:  12%|█▏        | 38/314 [03:53<27:16,  5.93s/it]test:39/314 | accuracy 1062  0.8509615384615384:  12%|█▏        | 38/314 [03:58<27:16,  5.93s/it]test:39/314 | accuracy 1062  0.8509615384615384:  12%|█▏        | 39/314 [03:58<26:51,  5.86s/it]test:40/314 | accuracy 1088  0.85:  12%|█▏        | 39/314 [04:05<26:51,  5.86s/it]              test:40/314 | accuracy 1088  0.85:  13%|█▎        | 40/314 [04:05<27:25,  6.01s/it]test:41/314 | accuracy 1114  0.8490853658536586:  13%|█▎        | 40/314 [04:11<27:25,  6.01s/it]test:41/314 | accuracy 1114  0.8490853658536586:  13%|█▎        | 41/314 [04:11<27:35,  6.07s/it]test:42/314 | accuracy 1136  0.8452380952380952:  13%|█▎        | 41/314 [04:17<27:35,  6.07s/it]test:42/314 | accuracy 1136  0.8452380952380952:  13%|█▎        | 42/314 [04:17<27:31,  6.07s/it]test:43/314 | accuracy 1165  0.846656976744186:  13%|█▎        | 42/314 [04:24<27:31,  6.07s/it] test:43/314 | accuracy 1165  0.846656976744186:  14%|█▎        | 43/314 [04:24<28:25,  6.29s/it]test:44/314 | accuracy 1192  0.8465909090909091:  14%|█▎        | 43/314 [04:30<28:25,  6.29s/it]test:44/314 | accuracy 1192  0.8465909090909091:  14%|█▍        | 44/314 [04:30<28:27,  6.32s/it]test:45/314 | accuracy 1222  0.8486111111111111:  14%|█▍        | 44/314 [04:37<28:27,  6.32s/it]test:45/314 | accuracy 1222  0.8486111111111111:  14%|█▍        | 45/314 [04:37<28:29,  6.36s/it]test:46/314 | accuracy 1245  0.8457880434782609:  14%|█▍        | 45/314 [04:43<28:29,  6.36s/it]test:46/314 | accuracy 1245  0.8457880434782609:  15%|█▍        | 46/314 [04:43<28:49,  6.46s/it]test:47/314 | accuracy 1271  0.8450797872340425:  15%|█▍        | 46/314 [04:49<28:49,  6.46s/it]test:47/314 | accuracy 1271  0.8450797872340425:  15%|█▍        | 47/314 [04:49<27:09,  6.10s/it]test:48/314 | accuracy 1297  0.8444010416666666:  15%|█▍        | 47/314 [04:54<27:09,  6.10s/it]test:48/314 | accuracy 1297  0.8444010416666666:  15%|█▌        | 48/314 [04:54<26:41,  6.02s/it]test:49/314 | accuracy 1321  0.8424744897959183:  15%|█▌        | 48/314 [05:01<26:41,  6.02s/it]test:49/314 | accuracy 1321  0.8424744897959183:  16%|█▌        | 49/314 [05:01<27:19,  6.19s/it]test:50/314 | accuracy 1349  0.843125:  16%|█▌        | 49/314 [05:08<27:19,  6.19s/it]          test:50/314 | accuracy 1349  0.843125:  16%|█▌        | 50/314 [05:08<28:04,  6.38s/it]test:51/314 | accuracy 1377  0.84375:  16%|█▌        | 50/314 [05:13<28:04,  6.38s/it] test:51/314 | accuracy 1377  0.84375:  16%|█▌        | 51/314 [05:13<26:59,  6.16s/it]test:52/314 | accuracy 1403  0.8431490384615384:  16%|█▌        | 51/314 [05:19<26:59,  6.16s/it]test:52/314 | accuracy 1403  0.8431490384615384:  17%|█▋        | 52/314 [05:19<26:01,  5.96s/it]test:53/314 | accuracy 1429  0.8425707547169812:  17%|█▋        | 52/314 [05:26<26:01,  5.96s/it]test:53/314 | accuracy 1429  0.8425707547169812:  17%|█▋        | 53/314 [05:26<27:08,  6.24s/it]test:54/314 | accuracy 1459  0.8443287037037037:  17%|█▋        | 53/314 [05:33<27:08,  6.24s/it]test:54/314 | accuracy 1459  0.8443287037037037:  17%|█▋        | 54/314 [05:33<28:06,  6.49s/it]test:55/314 | accuracy 1485  0.84375:  17%|█▋        | 54/314 [05:39<28:06,  6.49s/it]           test:55/314 | accuracy 1485  0.84375:  18%|█▊        | 55/314 [05:39<27:12,  6.30s/it]test:56/314 | accuracy 1515  0.8454241071428571:  18%|█▊        | 55/314 [05:45<27:12,  6.30s/it]test:56/314 | accuracy 1515  0.8454241071428571:  18%|█▊        | 56/314 [05:45<26:44,  6.22s/it]test:57/314 | accuracy 1541  0.8448464912280702:  18%|█▊        | 56/314 [05:50<26:44,  6.22s/it]test:57/314 | accuracy 1541  0.8448464912280702:  18%|█▊        | 57/314 [05:50<25:54,  6.05s/it]test:58/314 | accuracy 1570  0.8459051724137931:  18%|█▊        | 57/314 [05:57<25:54,  6.05s/it]test:58/314 | accuracy 1570  0.8459051724137931:  18%|█▊        | 58/314 [05:57<25:54,  6.07s/it]test:59/314 | accuracy 1599  0.846927966101695:  18%|█▊        | 58/314 [06:04<25:54,  6.07s/it] test:59/314 | accuracy 1599  0.846927966101695:  19%|█▉        | 59/314 [06:04<27:14,  6.41s/it]test:60/314 | accuracy 1628  0.8479166666666667:  19%|█▉        | 59/314 [06:10<27:14,  6.41s/it]test:60/314 | accuracy 1628  0.8479166666666667:  19%|█▉        | 60/314 [06:10<26:23,  6.23s/it]test:61/314 | accuracy 1658  0.8493852459016393:  19%|█▉        | 60/314 [06:16<26:23,  6.23s/it]test:61/314 | accuracy 1658  0.8493852459016393:  19%|█▉        | 61/314 [06:16<26:41,  6.33s/it]test:62/314 | accuracy 1685  0.8492943548387096:  19%|█▉        | 61/314 [06:23<26:41,  6.33s/it]test:62/314 | accuracy 1685  0.8492943548387096:  20%|█▉        | 62/314 [06:23<27:26,  6.53s/it]test:63/314 | accuracy 1712  0.8492063492063492:  20%|█▉        | 62/314 [06:29<27:26,  6.53s/it]test:63/314 | accuracy 1712  0.8492063492063492:  20%|██        | 63/314 [06:29<26:01,  6.22s/it]test:64/314 | accuracy 1740  0.849609375:  20%|██        | 63/314 [06:34<26:01,  6.22s/it]       test:64/314 | accuracy 1740  0.849609375:  20%|██        | 64/314 [06:34<25:21,  6.09s/it]test:65/314 | accuracy 1764  0.8480769230769231:  20%|██        | 64/314 [06:41<25:21,  6.09s/it]test:65/314 | accuracy 1764  0.8480769230769231:  21%|██        | 65/314 [06:41<25:57,  6.25s/it]test:66/314 | accuracy 1790  0.8475378787878788:  21%|██        | 65/314 [06:47<25:57,  6.25s/it]test:66/314 | accuracy 1790  0.8475378787878788:  21%|██        | 66/314 [06:47<24:57,  6.04s/it]test:67/314 | accuracy 1818  0.8479477611940298:  21%|██        | 66/314 [06:52<24:57,  6.04s/it]test:67/314 | accuracy 1818  0.8479477611940298:  21%|██▏       | 67/314 [06:52<24:32,  5.96s/it]test:68/314 | accuracy 1848  0.8492647058823529:  21%|██▏       | 67/314 [07:00<24:32,  5.96s/it]test:68/314 | accuracy 1848  0.8492647058823529:  22%|██▏       | 68/314 [07:00<26:41,  6.51s/it]test:69/314 | accuracy 1876  0.8496376811594203:  22%|██▏       | 68/314 [07:06<26:41,  6.51s/it]test:69/314 | accuracy 1876  0.8496376811594203:  22%|██▏       | 69/314 [07:06<25:37,  6.28s/it]test:70/314 | accuracy 1899  0.8477678571428572:  22%|██▏       | 69/314 [07:11<25:37,  6.28s/it]test:70/314 | accuracy 1899  0.8477678571428572:  22%|██▏       | 70/314 [07:11<24:27,  6.01s/it]test:71/314 | accuracy 1928  0.8485915492957746:  22%|██▏       | 70/314 [07:17<24:27,  6.01s/it]test:71/314 | accuracy 1928  0.8485915492957746:  23%|██▎       | 71/314 [07:17<23:38,  5.84s/it]test:72/314 | accuracy 1956  0.8489583333333334:  23%|██▎       | 71/314 [07:22<23:38,  5.84s/it]test:72/314 | accuracy 1956  0.8489583333333334:  23%|██▎       | 72/314 [07:22<23:08,  5.74s/it]test:73/314 | accuracy 1984  0.8493150684931506:  23%|██▎       | 72/314 [07:29<23:08,  5.74s/it]test:73/314 | accuracy 1984  0.8493150684931506:  23%|██▎       | 73/314 [07:29<23:52,  5.95s/it]test:74/314 | accuracy 2011  0.8492398648648649:  23%|██▎       | 73/314 [07:34<23:52,  5.95s/it]test:74/314 | accuracy 2011  0.8492398648648649:  24%|██▎       | 74/314 [07:34<22:46,  5.69s/it]test:75/314 | accuracy 2038  0.8491666666666666:  24%|██▎       | 74/314 [07:40<22:46,  5.69s/it]test:75/314 | accuracy 2038  0.8491666666666666:  24%|██▍       | 75/314 [07:40<22:59,  5.77s/it]test:76/314 | accuracy 2065  0.8490953947368421:  24%|██▍       | 75/314 [07:45<22:59,  5.77s/it]test:76/314 | accuracy 2065  0.8490953947368421:  24%|██▍       | 76/314 [07:45<22:42,  5.73s/it]test:77/314 | accuracy 2092  0.849025974025974:  24%|██▍       | 76/314 [07:50<22:42,  5.73s/it] test:77/314 | accuracy 2092  0.849025974025974:  25%|██▍       | 77/314 [07:51<21:58,  5.56s/it]test:78/314 | accuracy 2122  0.8501602564102564:  25%|██▍       | 77/314 [07:56<21:58,  5.56s/it]test:78/314 | accuracy 2122  0.8501602564102564:  25%|██▍       | 78/314 [07:57<22:23,  5.69s/it]test:79/314 | accuracy 2152  0.8512658227848101:  25%|██▍       | 78/314 [08:03<22:23,  5.69s/it]test:79/314 | accuracy 2152  0.8512658227848101:  25%|██▌       | 79/314 [08:03<23:35,  6.02s/it]test:80/314 | accuracy 2180  0.8515625:  25%|██▌       | 79/314 [08:09<23:35,  6.02s/it]         test:80/314 | accuracy 2180  0.8515625:  25%|██▌       | 80/314 [08:09<23:21,  5.99s/it]test:81/314 | accuracy 2203  0.8499228395061729:  25%|██▌       | 80/314 [08:15<23:21,  5.99s/it]test:81/314 | accuracy 2203  0.8499228395061729:  26%|██▌       | 81/314 [08:15<23:11,  5.97s/it]test:82/314 | accuracy 2230  0.8498475609756098:  26%|██▌       | 81/314 [08:21<23:11,  5.97s/it]test:82/314 | accuracy 2230  0.8498475609756098:  26%|██▌       | 82/314 [08:21<23:05,  5.97s/it]test:83/314 | accuracy 2259  0.8505271084337349:  26%|██▌       | 82/314 [08:28<23:05,  5.97s/it]test:83/314 | accuracy 2259  0.8505271084337349:  26%|██▋       | 83/314 [08:28<24:07,  6.26s/it]test:84/314 | accuracy 2289  0.8515625:  26%|██▋       | 83/314 [08:35<24:07,  6.26s/it]         test:84/314 | accuracy 2289  0.8515625:  27%|██▋       | 84/314 [08:35<24:50,  6.48s/it]test:85/314 | accuracy 2316  0.8514705882352941:  27%|██▋       | 84/314 [08:41<24:50,  6.48s/it]test:85/314 | accuracy 2316  0.8514705882352941:  27%|██▋       | 85/314 [08:41<24:37,  6.45s/it]test:86/314 | accuracy 2344  0.8517441860465116:  27%|██▋       | 85/314 [08:48<24:37,  6.45s/it]test:86/314 | accuracy 2344  0.8517441860465116:  27%|██▋       | 86/314 [08:48<24:37,  6.48s/it]test:87/314 | accuracy 2369  0.850933908045977:  27%|██▋       | 86/314 [08:54<24:37,  6.48s/it] test:87/314 | accuracy 2369  0.850933908045977:  28%|██▊       | 87/314 [08:54<24:16,  6.42s/it]test:88/314 | accuracy 2397  0.8512073863636364:  28%|██▊       | 87/314 [09:00<24:16,  6.42s/it]test:88/314 | accuracy 2397  0.8512073863636364:  28%|██▊       | 88/314 [09:00<23:07,  6.14s/it]test:89/314 | accuracy 2421  0.8500702247191011:  28%|██▊       | 88/314 [09:06<23:07,  6.14s/it]test:89/314 | accuracy 2421  0.8500702247191011:  28%|██▊       | 89/314 [09:06<22:54,  6.11s/it]test:90/314 | accuracy 2447  0.8496527777777778:  28%|██▊       | 89/314 [09:12<22:54,  6.11s/it]test:90/314 | accuracy 2447  0.8496527777777778:  29%|██▊       | 90/314 [09:12<22:49,  6.11s/it]test:91/314 | accuracy 2474  0.8495879120879121:  29%|██▊       | 90/314 [09:18<22:49,  6.11s/it]test:91/314 | accuracy 2474  0.8495879120879121:  29%|██▉       | 91/314 [09:18<22:37,  6.09s/it]test:92/314 | accuracy 2501  0.8495244565217391:  29%|██▉       | 91/314 [09:24<22:37,  6.09s/it]test:92/314 | accuracy 2501  0.8495244565217391:  29%|██▉       | 92/314 [09:24<22:04,  5.96s/it]test:93/314 | accuracy 2528  0.8494623655913979:  29%|██▉       | 92/314 [09:30<22:04,  5.96s/it]test:93/314 | accuracy 2528  0.8494623655913979:  30%|██▉       | 93/314 [09:30<22:32,  6.12s/it]test:94/314 | accuracy 2558  0.8503989361702128:  30%|██▉       | 93/314 [09:36<22:32,  6.12s/it]test:94/314 | accuracy 2558  0.8503989361702128:  30%|██▉       | 94/314 [09:36<22:16,  6.07s/it]test:95/314 | accuracy 2581  0.8490131578947369:  30%|██▉       | 94/314 [09:41<22:16,  6.07s/it]test:95/314 | accuracy 2581  0.8490131578947369:  30%|███       | 95/314 [09:41<21:22,  5.85s/it]test:96/314 | accuracy 2607  0.8486328125:  30%|███       | 95/314 [09:47<21:22,  5.85s/it]      test:96/314 | accuracy 2607  0.8486328125:  31%|███       | 96/314 [09:47<21:09,  5.82s/it]test:97/314 | accuracy 2635  0.8489046391752577:  31%|███       | 96/314 [09:53<21:09,  5.82s/it]test:97/314 | accuracy 2635  0.8489046391752577:  31%|███       | 97/314 [09:53<20:52,  5.77s/it]test:98/314 | accuracy 2658  0.8475765306122449:  31%|███       | 97/314 [09:59<20:52,  5.77s/it]test:98/314 | accuracy 2658  0.8475765306122449:  31%|███       | 98/314 [09:59<21:28,  5.96s/it]test:99/314 | accuracy 2686  0.8478535353535354:  31%|███       | 98/314 [10:05<21:28,  5.96s/it]test:99/314 | accuracy 2686  0.8478535353535354:  32%|███▏      | 99/314 [10:05<21:32,  6.01s/it]test:100/314 | accuracy 2712  0.8475:  32%|███▏      | 99/314 [10:12<21:32,  6.01s/it]           test:100/314 | accuracy 2712  0.8475:  32%|███▏      | 100/314 [10:12<22:10,  6.22s/it]test:101/314 | accuracy 2738  0.8471534653465347:  32%|███▏      | 100/314 [10:18<22:10,  6.22s/it]test:101/314 | accuracy 2738  0.8471534653465347:  32%|███▏      | 101/314 [10:18<21:46,  6.13s/it]test:102/314 | accuracy 2765  0.8471200980392157:  32%|███▏      | 101/314 [10:27<21:46,  6.13s/it]test:102/314 | accuracy 2765  0.8471200980392157:  32%|███▏      | 102/314 [10:27<25:11,  7.13s/it]test:103/314 | accuracy 2794  0.8476941747572816:  32%|███▏      | 102/314 [10:37<25:11,  7.13s/it]test:103/314 | accuracy 2794  0.8476941747572816:  33%|███▎      | 103/314 [10:37<27:59,  7.96s/it]test:104/314 | accuracy 2825  0.8488581730769231:  33%|███▎      | 103/314 [10:47<27:59,  7.96s/it]test:104/314 | accuracy 2825  0.8488581730769231:  33%|███▎      | 104/314 [10:47<29:17,  8.37s/it]test:105/314 | accuracy 2855  0.8497023809523809:  33%|███▎      | 104/314 [10:56<29:17,  8.37s/it]test:105/314 | accuracy 2855  0.8497023809523809:  33%|███▎      | 105/314 [10:56<30:01,  8.62s/it]test:106/314 | accuracy 2885  0.8505306603773585:  33%|███▎      | 105/314 [11:05<30:01,  8.62s/it]test:106/314 | accuracy 2885  0.8505306603773585:  34%|███▍      | 106/314 [11:05<30:31,  8.81s/it]test:107/314 | accuracy 2912  0.8504672897196262:  34%|███▍      | 106/314 [11:14<30:31,  8.81s/it]test:107/314 | accuracy 2912  0.8504672897196262:  34%|███▍      | 107/314 [11:14<30:40,  8.89s/it]test:108/314 | accuracy 2941  0.8509837962962963:  34%|███▍      | 107/314 [11:23<30:40,  8.89s/it]test:108/314 | accuracy 2941  0.8509837962962963:  34%|███▍      | 108/314 [11:23<30:32,  8.90s/it]test:109/314 | accuracy 2972  0.8520642201834863:  34%|███▍      | 108/314 [11:32<30:32,  8.90s/it]test:109/314 | accuracy 2972  0.8520642201834863:  35%|███▍      | 109/314 [11:32<30:50,  9.03s/it]test:110/314 | accuracy 3003  0.853125:  35%|███▍      | 109/314 [11:42<30:50,  9.03s/it]          test:110/314 | accuracy 3003  0.853125:  35%|███▌      | 110/314 [11:42<31:01,  9.12s/it]test:111/314 | accuracy 3034  0.8541666666666666:  35%|███▌      | 110/314 [11:51<31:01,  9.12s/it]test:111/314 | accuracy 3034  0.8541666666666666:  35%|███▌      | 111/314 [11:52<31:28,  9.30s/it]test:112/314 | accuracy 3063  0.8546316964285714:  35%|███▌      | 111/314 [12:01<31:28,  9.30s/it]test:112/314 | accuracy 3063  0.8546316964285714:  36%|███▌      | 112/314 [12:01<31:12,  9.27s/it]test:113/314 | accuracy 3092  0.8550884955752213:  36%|███▌      | 112/314 [12:11<31:12,  9.27s/it]test:113/314 | accuracy 3092  0.8550884955752213:  36%|███▌      | 113/314 [12:11<32:03,  9.57s/it]test:114/314 | accuracy 3119  0.8549890350877193:  36%|███▌      | 113/314 [12:20<32:03,  9.57s/it]test:114/314 | accuracy 3119  0.8549890350877193:  36%|███▋      | 114/314 [12:20<31:44,  9.52s/it]test:115/314 | accuracy 3149  0.8557065217391304:  36%|███▋      | 114/314 [12:30<31:44,  9.52s/it]test:115/314 | accuracy 3149  0.8557065217391304:  37%|███▋      | 115/314 [12:30<31:32,  9.51s/it]test:116/314 | accuracy 3180  0.8566810344827587:  37%|███▋      | 115/314 [12:39<31:32,  9.51s/it]test:116/314 | accuracy 3180  0.8566810344827587:  37%|███▋      | 116/314 [12:39<30:46,  9.32s/it]test:117/314 | accuracy 3210  0.8573717948717948:  37%|███▋      | 116/314 [12:48<30:46,  9.32s/it]test:117/314 | accuracy 3210  0.8573717948717948:  37%|███▋      | 117/314 [12:48<30:19,  9.24s/it]test:118/314 | accuracy 3238  0.857521186440678:  37%|███▋      | 117/314 [12:57<30:19,  9.24s/it] test:118/314 | accuracy 3238  0.857521186440678:  38%|███▊      | 118/314 [12:57<30:28,  9.33s/it]test:119/314 | accuracy 3269  0.8584558823529411:  38%|███▊      | 118/314 [13:07<30:28,  9.33s/it]test:119/314 | accuracy 3269  0.8584558823529411:  38%|███▊      | 119/314 [13:07<30:43,  9.45s/it]test:120/314 | accuracy 3300  0.859375:  38%|███▊      | 119/314 [13:16<30:43,  9.45s/it]          test:120/314 | accuracy 3300  0.859375:  38%|███▊      | 120/314 [13:16<30:27,  9.42s/it]test:121/314 | accuracy 3328  0.859504132231405:  38%|███▊      | 120/314 [13:25<30:27,  9.42s/it]test:121/314 | accuracy 3328  0.859504132231405:  39%|███▊      | 121/314 [13:26<30:01,  9.34s/it]test:122/314 | accuracy 3356  0.8596311475409836:  39%|███▊      | 121/314 [13:35<30:01,  9.34s/it]test:122/314 | accuracy 3356  0.8596311475409836:  39%|███▉      | 122/314 [13:35<29:53,  9.34s/it]test:123/314 | accuracy 3385  0.860010162601626:  39%|███▉      | 122/314 [13:44<29:53,  9.34s/it] test:123/314 | accuracy 3385  0.860010162601626:  39%|███▉      | 123/314 [13:45<30:01,  9.43s/it]test:124/314 | accuracy 3411  0.8596270161290323:  39%|███▉      | 123/314 [13:54<30:01,  9.43s/it]test:124/314 | accuracy 3411  0.8596270161290323:  39%|███▉      | 124/314 [13:54<29:30,  9.32s/it]test:125/314 | accuracy 3443  0.86075:  39%|███▉      | 124/314 [14:03<29:30,  9.32s/it]           test:125/314 | accuracy 3443  0.86075:  40%|███▉      | 125/314 [14:03<29:39,  9.41s/it]test:126/314 | accuracy 3470  0.8606150793650794:  40%|███▉      | 125/314 [14:13<29:39,  9.41s/it]test:126/314 | accuracy 3470  0.8606150793650794:  40%|████      | 126/314 [14:13<29:27,  9.40s/it]test:127/314 | accuracy 3496  0.860236220472441:  40%|████      | 126/314 [14:22<29:27,  9.40s/it] test:127/314 | accuracy 3496  0.860236220472441:  40%|████      | 127/314 [14:22<29:38,  9.51s/it]test:128/314 | accuracy 3527  0.861083984375:  40%|████      | 127/314 [14:33<29:38,  9.51s/it]   test:128/314 | accuracy 3527  0.861083984375:  41%|████      | 128/314 [14:33<30:30,  9.84s/it]test:129/314 | accuracy 3557  0.8616763565891473:  41%|████      | 128/314 [14:43<30:30,  9.84s/it]test:129/314 | accuracy 3557  0.8616763565891473:  41%|████      | 129/314 [14:43<30:06,  9.77s/it]test:130/314 | accuracy 3586  0.8620192307692308:  41%|████      | 129/314 [14:51<30:06,  9.77s/it]test:130/314 | accuracy 3586  0.8620192307692308:  41%|████▏     | 130/314 [14:51<29:06,  9.49s/it]test:131/314 | accuracy 3615  0.8623568702290076:  41%|████▏     | 130/314 [15:01<29:06,  9.49s/it]test:131/314 | accuracy 3615  0.8623568702290076:  42%|████▏     | 131/314 [15:01<28:39,  9.40s/it]test:132/314 | accuracy 3646  0.8631628787878788:  42%|████▏     | 131/314 [15:09<28:39,  9.40s/it]test:132/314 | accuracy 3646  0.8631628787878788:  42%|████▏     | 132/314 [15:10<28:03,  9.25s/it]test:133/314 | accuracy 3677  0.8639567669172933:  42%|████▏     | 132/314 [15:19<28:03,  9.25s/it]test:133/314 | accuracy 3677  0.8639567669172933:  42%|████▏     | 133/314 [15:19<28:25,  9.42s/it]test:134/314 | accuracy 3706  0.8642723880597015:  42%|████▏     | 133/314 [15:29<28:25,  9.42s/it]test:134/314 | accuracy 3706  0.8642723880597015:  43%|████▎     | 134/314 [15:29<28:09,  9.39s/it]test:135/314 | accuracy 3733  0.8641203703703704:  43%|████▎     | 134/314 [15:38<28:09,  9.39s/it]test:135/314 | accuracy 3733  0.8641203703703704:  43%|████▎     | 135/314 [15:38<27:45,  9.30s/it]test:136/314 | accuracy 3763  0.8646599264705882:  43%|████▎     | 135/314 [15:48<27:45,  9.30s/it]test:136/314 | accuracy 3763  0.8646599264705882:  43%|████▎     | 136/314 [15:48<28:09,  9.49s/it]test:137/314 | accuracy 3795  0.8656478102189781:  43%|████▎     | 136/314 [15:56<28:09,  9.49s/it]test:137/314 | accuracy 3795  0.8656478102189781:  44%|████▎     | 137/314 [15:56<27:23,  9.28s/it]test:138/314 | accuracy 3825  0.8661684782608695:  44%|████▎     | 137/314 [16:06<27:23,  9.28s/it]test:138/314 | accuracy 3825  0.8661684782608695:  44%|████▍     | 138/314 [16:06<27:18,  9.31s/it]test:139/314 | accuracy 3851  0.8657823741007195:  44%|████▍     | 138/314 [16:15<27:18,  9.31s/it]test:139/314 | accuracy 3851  0.8657823741007195:  44%|████▍     | 139/314 [16:15<26:49,  9.20s/it]test:140/314 | accuracy 3881  0.8662946428571429:  44%|████▍     | 139/314 [16:24<26:49,  9.20s/it]test:140/314 | accuracy 3881  0.8662946428571429:  45%|████▍     | 140/314 [16:24<26:19,  9.08s/it]test:141/314 | accuracy 3910  0.8665780141843972:  45%|████▍     | 140/314 [16:33<26:19,  9.08s/it]test:141/314 | accuracy 3910  0.8665780141843972:  45%|████▍     | 141/314 [16:33<26:23,  9.15s/it]test:142/314 | accuracy 3942  0.8675176056338029:  45%|████▍     | 141/314 [16:42<26:23,  9.15s/it]test:142/314 | accuracy 3942  0.8675176056338029:  45%|████▌     | 142/314 [16:42<26:35,  9.28s/it]test:143/314 | accuracy 3971  0.8677884615384616:  45%|████▌     | 142/314 [16:52<26:35,  9.28s/it]test:143/314 | accuracy 3971  0.8677884615384616:  46%|████▌     | 143/314 [16:52<26:26,  9.28s/it]test:144/314 | accuracy 4000  0.8680555555555556:  46%|████▌     | 143/314 [17:01<26:26,  9.28s/it]test:144/314 | accuracy 4000  0.8680555555555556:  46%|████▌     | 144/314 [17:01<26:29,  9.35s/it]test:145/314 | accuracy 4029  0.8683189655172414:  46%|████▌     | 144/314 [17:11<26:29,  9.35s/it]test:145/314 | accuracy 4029  0.8683189655172414:  46%|████▌     | 145/314 [17:11<26:18,  9.34s/it]test:146/314 | accuracy 4058  0.8685787671232876:  46%|████▌     | 145/314 [17:20<26:18,  9.34s/it]test:146/314 | accuracy 4058  0.8685787671232876:  46%|████▋     | 146/314 [17:20<26:13,  9.37s/it]test:147/314 | accuracy 4089  0.8692602040816326:  46%|████▋     | 146/314 [17:31<26:13,  9.37s/it]test:147/314 | accuracy 4089  0.8692602040816326:  47%|████▋     | 147/314 [17:31<27:47,  9.99s/it]test:148/314 | accuracy 4116  0.8690878378378378:  47%|████▋     | 147/314 [17:40<27:47,  9.99s/it]test:148/314 | accuracy 4116  0.8690878378378378:  47%|████▋     | 148/314 [17:40<26:50,  9.70s/it]test:149/314 | accuracy 4146  0.8695469798657718:  47%|████▋     | 148/314 [17:49<26:50,  9.70s/it]test:149/314 | accuracy 4146  0.8695469798657718:  47%|████▋     | 149/314 [17:50<26:05,  9.49s/it]test:150/314 | accuracy 4178  0.8704166666666666:  47%|████▋     | 149/314 [18:00<26:05,  9.49s/it]test:150/314 | accuracy 4178  0.8704166666666666:  48%|████▊     | 150/314 [18:00<26:57,  9.86s/it]test:151/314 | accuracy 4208  0.8708609271523179:  48%|████▊     | 150/314 [18:10<26:57,  9.86s/it]test:151/314 | accuracy 4208  0.8708609271523179:  48%|████▊     | 151/314 [18:10<26:36,  9.79s/it]test:152/314 | accuracy 4234  0.8704769736842105:  48%|████▊     | 151/314 [18:19<26:36,  9.79s/it]test:152/314 | accuracy 4234  0.8704769736842105:  48%|████▊     | 152/314 [18:19<26:04,  9.66s/it]test:153/314 | accuracy 4264  0.8709150326797386:  48%|████▊     | 152/314 [18:29<26:04,  9.66s/it]test:153/314 | accuracy 4264  0.8709150326797386:  49%|████▊     | 153/314 [18:29<25:58,  9.68s/it]test:154/314 | accuracy 4293  0.8711444805194806:  49%|████▊     | 153/314 [18:38<25:58,  9.68s/it]test:154/314 | accuracy 4293  0.8711444805194806:  49%|████▉     | 154/314 [18:38<25:18,  9.49s/it]test:155/314 | accuracy 4321  0.8711693548387097:  49%|████▉     | 154/314 [18:47<25:18,  9.49s/it]test:155/314 | accuracy 4321  0.8711693548387097:  49%|████▉     | 155/314 [18:47<24:59,  9.43s/it]test:156/314 | accuracy 4351  0.8715945512820513:  49%|████▉     | 155/314 [18:57<24:59,  9.43s/it]test:156/314 | accuracy 4351  0.8715945512820513:  50%|████▉     | 156/314 [18:57<25:04,  9.52s/it]test:157/314 | accuracy 4382  0.8722133757961783:  50%|████▉     | 156/314 [19:06<25:04,  9.52s/it]test:157/314 | accuracy 4382  0.8722133757961783:  50%|█████     | 157/314 [19:06<24:52,  9.51s/it]test:158/314 | accuracy 4412  0.872626582278481:  50%|█████     | 157/314 [19:16<24:52,  9.51s/it] test:158/314 | accuracy 4412  0.872626582278481:  50%|█████     | 158/314 [19:16<24:25,  9.39s/it]test:159/314 | accuracy 4443  0.8732311320754716:  50%|█████     | 158/314 [19:25<24:25,  9.39s/it]test:159/314 | accuracy 4443  0.8732311320754716:  51%|█████     | 159/314 [19:25<24:30,  9.49s/it]test:160/314 | accuracy 4473  0.8736328125:  51%|█████     | 159/314 [19:35<24:30,  9.49s/it]      test:160/314 | accuracy 4473  0.8736328125:  51%|█████     | 160/314 [19:35<24:21,  9.49s/it]test:161/314 | accuracy 4501  0.873641304347826:  51%|█████     | 160/314 [19:44<24:21,  9.49s/it]test:161/314 | accuracy 4501  0.873641304347826:  51%|█████▏    | 161/314 [19:45<24:22,  9.56s/it]test:162/314 | accuracy 4533  0.8744212962962963:  51%|█████▏    | 161/314 [19:53<24:22,  9.56s/it]test:162/314 | accuracy 4533  0.8744212962962963:  52%|█████▏    | 162/314 [19:54<23:50,  9.41s/it]test:163/314 | accuracy 4564  0.875:  52%|█████▏    | 162/314 [20:03<23:50,  9.41s/it]             test:163/314 | accuracy 4564  0.875:  52%|█████▏    | 163/314 [20:03<23:38,  9.39s/it]test:164/314 | accuracy 4593  0.8751905487804879:  52%|█████▏    | 163/314 [20:12<23:38,  9.39s/it]test:164/314 | accuracy 4593  0.8751905487804879:  52%|█████▏    | 164/314 [20:12<23:28,  9.39s/it]test:165/314 | accuracy 4620  0.875:  52%|█████▏    | 164/314 [20:22<23:28,  9.39s/it]             test:165/314 | accuracy 4620  0.875:  53%|█████▎    | 165/314 [20:22<23:37,  9.51s/it]test:166/314 | accuracy 4651  0.8755647590361446:  53%|█████▎    | 165/314 [20:32<23:37,  9.51s/it]test:166/314 | accuracy 4651  0.8755647590361446:  53%|█████▎    | 166/314 [20:32<23:26,  9.50s/it]test:167/314 | accuracy 4682  0.876122754491018:  53%|█████▎    | 166/314 [20:41<23:26,  9.50s/it] test:167/314 | accuracy 4682  0.876122754491018:  53%|█████▎    | 167/314 [20:41<23:12,  9.47s/it]test:168/314 | accuracy 4712  0.8764880952380952:  53%|█████▎    | 167/314 [20:50<23:12,  9.47s/it]test:168/314 | accuracy 4712  0.8764880952380952:  54%|█████▎    | 168/314 [20:50<22:43,  9.34s/it]test:169/314 | accuracy 4742  0.8768491124260355:  54%|█████▎    | 168/314 [20:59<22:43,  9.34s/it]test:169/314 | accuracy 4742  0.8768491124260355:  54%|█████▍    | 169/314 [21:00<22:41,  9.39s/it]test:170/314 | accuracy 4771  0.8770220588235295:  54%|█████▍    | 169/314 [21:10<22:41,  9.39s/it]test:170/314 | accuracy 4771  0.8770220588235295:  54%|█████▍    | 170/314 [21:10<23:08,  9.64s/it]test:171/314 | accuracy 4800  0.8771929824561403:  54%|█████▍    | 170/314 [21:19<23:08,  9.64s/it]test:171/314 | accuracy 4800  0.8771929824561403:  54%|█████▍    | 171/314 [21:19<22:50,  9.58s/it]test:172/314 | accuracy 4830  0.8775436046511628:  54%|█████▍    | 171/314 [21:28<22:50,  9.58s/it]test:172/314 | accuracy 4830  0.8775436046511628:  55%|█████▍    | 172/314 [21:29<22:31,  9.52s/it]test:173/314 | accuracy 4858  0.877528901734104:  55%|█████▍    | 172/314 [21:38<22:31,  9.52s/it] test:173/314 | accuracy 4858  0.877528901734104:  55%|█████▌    | 173/314 [21:38<22:13,  9.46s/it]test:174/314 | accuracy 4888  0.8778735632183908:  55%|█████▌    | 173/314 [21:47<22:13,  9.46s/it]test:174/314 | accuracy 4888  0.8778735632183908:  55%|█████▌    | 174/314 [21:47<21:47,  9.34s/it]test:175/314 | accuracy 4919  0.8783928571428572:  55%|█████▌    | 174/314 [21:56<21:47,  9.34s/it]test:175/314 | accuracy 4919  0.8783928571428572:  56%|█████▌    | 175/314 [21:56<21:45,  9.39s/it]test:176/314 | accuracy 4951  0.8790838068181818:  56%|█████▌    | 175/314 [22:06<21:45,  9.39s/it]test:176/314 | accuracy 4951  0.8790838068181818:  56%|█████▌    | 176/314 [22:06<21:42,  9.44s/it]test:177/314 | accuracy 4980  0.8792372881355932:  56%|█████▌    | 176/314 [22:15<21:42,  9.44s/it]test:177/314 | accuracy 4980  0.8792372881355932:  56%|█████▋    | 177/314 [22:15<21:33,  9.44s/it]test:178/314 | accuracy 5011  0.8797401685393258:  56%|█████▋    | 177/314 [22:25<21:33,  9.44s/it]test:178/314 | accuracy 5011  0.8797401685393258:  57%|█████▋    | 178/314 [22:25<21:22,  9.43s/it]test:179/314 | accuracy 5040  0.8798882681564246:  57%|█████▋    | 178/314 [22:34<21:22,  9.43s/it]test:179/314 | accuracy 5040  0.8798882681564246:  57%|█████▋    | 179/314 [22:34<21:19,  9.48s/it]test:180/314 | accuracy 5072  0.8805555555555555:  57%|█████▋    | 179/314 [22:44<21:19,  9.48s/it]test:180/314 | accuracy 5072  0.8805555555555555:  57%|█████▋    | 180/314 [22:44<21:18,  9.54s/it]test:181/314 | accuracy 5102  0.8808701657458563:  57%|█████▋    | 180/314 [22:54<21:18,  9.54s/it]test:181/314 | accuracy 5102  0.8808701657458563:  58%|█████▊    | 181/314 [22:54<21:12,  9.57s/it]test:182/314 | accuracy 5133  0.881353021978022:  58%|█████▊    | 181/314 [23:03<21:12,  9.57s/it] test:182/314 | accuracy 5133  0.881353021978022:  58%|█████▊    | 182/314 [23:03<20:57,  9.52s/it]test:183/314 | accuracy 5164  0.8818306010928961:  58%|█████▊    | 182/314 [23:12<20:57,  9.52s/it]test:183/314 | accuracy 5164  0.8818306010928961:  58%|█████▊    | 183/314 [23:12<20:25,  9.36s/it]test:184/314 | accuracy 5195  0.8823029891304348:  58%|█████▊    | 183/314 [23:22<20:25,  9.36s/it]test:184/314 | accuracy 5195  0.8823029891304348:  59%|█████▊    | 184/314 [23:22<20:31,  9.47s/it]test:185/314 | accuracy 5226  0.8827702702702702:  59%|█████▊    | 184/314 [23:31<20:31,  9.47s/it]test:185/314 | accuracy 5226  0.8827702702702702:  59%|█████▉    | 185/314 [23:31<20:25,  9.50s/it]test:186/314 | accuracy 5258  0.8834005376344086:  59%|█████▉    | 185/314 [23:40<20:25,  9.50s/it]test:186/314 | accuracy 5258  0.8834005376344086:  59%|█████▉    | 186/314 [23:41<20:01,  9.38s/it]test:187/314 | accuracy 5288  0.8836898395721925:  59%|█████▉    | 186/314 [23:50<20:01,  9.38s/it]test:187/314 | accuracy 5288  0.8836898395721925:  60%|█████▉    | 187/314 [23:50<19:55,  9.41s/it]test:188/314 | accuracy 5317  0.8838098404255319:  60%|█████▉    | 187/314 [23:59<19:55,  9.41s/it]test:188/314 | accuracy 5317  0.8838098404255319:  60%|█████▉    | 188/314 [23:59<19:35,  9.33s/it]test:189/314 | accuracy 5345  0.8837632275132276:  60%|█████▉    | 188/314 [24:08<19:35,  9.33s/it]test:189/314 | accuracy 5345  0.8837632275132276:  60%|██████    | 189/314 [24:08<19:22,  9.30s/it]test:190/314 | accuracy 5374  0.8838815789473684:  60%|██████    | 189/314 [24:18<19:22,  9.30s/it]test:190/314 | accuracy 5374  0.8838815789473684:  61%|██████    | 190/314 [24:18<19:09,  9.27s/it]test:191/314 | accuracy 5404  0.8841623036649214:  61%|██████    | 190/314 [24:27<19:09,  9.27s/it]test:191/314 | accuracy 5404  0.8841623036649214:  61%|██████    | 191/314 [24:27<19:05,  9.31s/it]test:192/314 | accuracy 5435  0.8846028645833334:  61%|██████    | 191/314 [24:36<19:05,  9.31s/it]test:192/314 | accuracy 5435  0.8846028645833334:  61%|██████    | 192/314 [24:36<18:45,  9.22s/it]test:193/314 | accuracy 5463  0.8845531088082902:  61%|██████    | 192/314 [24:45<18:45,  9.22s/it]test:193/314 | accuracy 5463  0.8845531088082902:  61%|██████▏   | 193/314 [24:45<18:38,  9.24s/it]test:194/314 | accuracy 5492  0.8846649484536082:  61%|██████▏   | 193/314 [24:55<18:38,  9.24s/it]test:194/314 | accuracy 5492  0.8846649484536082:  62%|██████▏   | 194/314 [24:55<18:40,  9.34s/it]test:195/314 | accuracy 5523  0.8850961538461538:  62%|██████▏   | 194/314 [25:04<18:40,  9.34s/it]test:195/314 | accuracy 5523  0.8850961538461538:  62%|██████▏   | 195/314 [25:04<18:25,  9.29s/it]test:196/314 | accuracy 5552  0.8852040816326531:  62%|██████▏   | 195/314 [25:13<18:25,  9.29s/it]test:196/314 | accuracy 5552  0.8852040816326531:  62%|██████▏   | 196/314 [25:14<18:24,  9.36s/it]test:197/314 | accuracy 5581  0.8853109137055838:  62%|██████▏   | 196/314 [25:22<18:24,  9.36s/it]test:197/314 | accuracy 5581  0.8853109137055838:  63%|██████▎   | 197/314 [25:23<18:01,  9.24s/it]test:198/314 | accuracy 5612  0.8857323232323232:  63%|██████▎   | 197/314 [25:32<18:01,  9.24s/it]test:198/314 | accuracy 5612  0.8857323232323232:  63%|██████▎   | 198/314 [25:32<17:48,  9.21s/it]test:199/314 | accuracy 5641  0.8858354271356784:  63%|██████▎   | 198/314 [25:41<17:48,  9.21s/it]test:199/314 | accuracy 5641  0.8858354271356784:  63%|██████▎   | 199/314 [25:41<17:38,  9.20s/it]test:200/314 | accuracy 5670  0.8859375:  63%|██████▎   | 199/314 [25:50<17:38,  9.20s/it]         test:200/314 | accuracy 5670  0.8859375:  64%|██████▎   | 200/314 [25:50<17:34,  9.25s/it]test:201/314 | accuracy 5698  0.8858830845771144:  64%|██████▎   | 200/314 [25:59<17:34,  9.25s/it]test:201/314 | accuracy 5698  0.8858830845771144:  64%|██████▍   | 201/314 [25:59<17:17,  9.19s/it]test:202/314 | accuracy 5726  0.8858292079207921:  64%|██████▍   | 201/314 [26:08<17:17,  9.19s/it]test:202/314 | accuracy 5726  0.8858292079207921:  64%|██████▍   | 202/314 [26:08<17:06,  9.17s/it]test:203/314 | accuracy 5755  0.885929802955665:  64%|██████▍   | 202/314 [26:18<17:06,  9.17s/it] test:203/314 | accuracy 5755  0.885929802955665:  65%|██████▍   | 203/314 [26:18<17:04,  9.23s/it]test:204/314 | accuracy 5785  0.8861825980392157:  65%|██████▍   | 203/314 [26:27<17:04,  9.23s/it]test:204/314 | accuracy 5785  0.8861825980392157:  65%|██████▍   | 204/314 [26:27<16:53,  9.22s/it]test:205/314 | accuracy 5817  0.8867378048780488:  65%|██████▍   | 204/314 [26:36<16:53,  9.22s/it]test:205/314 | accuracy 5817  0.8867378048780488:  65%|██████▌   | 205/314 [26:37<16:56,  9.33s/it]test:206/314 | accuracy 5849  0.8872876213592233:  65%|██████▌   | 205/314 [26:46<16:56,  9.33s/it]test:206/314 | accuracy 5849  0.8872876213592233:  66%|██████▌   | 206/314 [26:46<16:54,  9.39s/it]test:207/314 | accuracy 5879  0.887530193236715:  66%|██████▌   | 206/314 [26:56<16:54,  9.39s/it] test:207/314 | accuracy 5879  0.887530193236715:  66%|██████▌   | 207/314 [26:56<16:51,  9.45s/it]test:208/314 | accuracy 5905  0.8871694711538461:  66%|██████▌   | 207/314 [27:05<16:51,  9.45s/it]test:208/314 | accuracy 5905  0.8871694711538461:  66%|██████▌   | 208/314 [27:05<16:31,  9.35s/it]test:209/314 | accuracy 5937  0.8877093301435407:  66%|██████▌   | 208/314 [27:14<16:31,  9.35s/it]test:209/314 | accuracy 5937  0.8877093301435407:  67%|██████▋   | 209/314 [27:14<16:23,  9.36s/it]test:210/314 | accuracy 5969  0.8882440476190476:  67%|██████▋   | 209/314 [27:23<16:23,  9.36s/it]test:210/314 | accuracy 5969  0.8882440476190476:  67%|██████▋   | 210/314 [27:24<16:13,  9.36s/it]test:211/314 | accuracy 5997  0.888181279620853:  67%|██████▋   | 210/314 [27:33<16:13,  9.36s/it] test:211/314 | accuracy 5997  0.888181279620853:  67%|██████▋   | 211/314 [27:33<16:18,  9.50s/it]test:212/314 | accuracy 6024  0.8879716981132075:  67%|██████▋   | 211/314 [27:42<16:18,  9.50s/it]test:212/314 | accuracy 6024  0.8879716981132075:  68%|██████▊   | 212/314 [27:42<15:50,  9.32s/it]test:213/314 | accuracy 6053  0.8880575117370892:  68%|██████▊   | 212/314 [27:52<15:50,  9.32s/it]test:213/314 | accuracy 6053  0.8880575117370892:  68%|██████▊   | 213/314 [27:52<16:04,  9.55s/it]test:214/314 | accuracy 6082  0.888142523364486:  68%|██████▊   | 213/314 [28:01<16:04,  9.55s/it] test:214/314 | accuracy 6082  0.888142523364486:  68%|██████▊   | 214/314 [28:01<15:35,  9.36s/it]test:215/314 | accuracy 6111  0.8882267441860465:  68%|██████▊   | 214/314 [28:11<15:35,  9.36s/it]test:215/314 | accuracy 6111  0.8882267441860465:  68%|██████▊   | 215/314 [28:11<15:36,  9.46s/it]test:216/314 | accuracy 6140  0.8883101851851852:  68%|██████▊   | 215/314 [28:20<15:36,  9.46s/it]test:216/314 | accuracy 6140  0.8883101851851852:  69%|██████▉   | 216/314 [28:20<15:22,  9.42s/it]test:217/314 | accuracy 6169  0.8883928571428571:  69%|██████▉   | 216/314 [28:30<15:22,  9.42s/it]test:217/314 | accuracy 6169  0.8883928571428571:  69%|██████▉   | 217/314 [28:30<15:13,  9.42s/it]test:218/314 | accuracy 6199  0.888618119266055:  69%|██████▉   | 217/314 [28:39<15:13,  9.42s/it] test:218/314 | accuracy 6199  0.888618119266055:  69%|██████▉   | 218/314 [28:39<15:07,  9.46s/it]test:219/314 | accuracy 6230  0.8889840182648402:  69%|██████▉   | 218/314 [28:49<15:07,  9.46s/it]test:219/314 | accuracy 6230  0.8889840182648402:  70%|██████▉   | 219/314 [28:50<15:24,  9.73s/it]test:220/314 | accuracy 6257  0.8887784090909091:  70%|██████▉   | 219/314 [28:59<15:24,  9.73s/it]test:220/314 | accuracy 6257  0.8887784090909091:  70%|███████   | 220/314 [28:59<15:03,  9.61s/it]test:221/314 | accuracy 6285  0.8887160633484162:  70%|███████   | 220/314 [29:08<15:03,  9.61s/it]test:221/314 | accuracy 6285  0.8887160633484162:  70%|███████   | 221/314 [29:08<14:50,  9.57s/it]test:222/314 | accuracy 6317  0.8892173423423423:  70%|███████   | 221/314 [29:17<14:50,  9.57s/it]test:222/314 | accuracy 6317  0.8892173423423423:  71%|███████   | 222/314 [29:18<14:28,  9.44s/it]test:223/314 | accuracy 6346  0.8892937219730942:  71%|███████   | 222/314 [29:27<14:28,  9.44s/it]test:223/314 | accuracy 6346  0.8892937219730942:  71%|███████   | 223/314 [29:27<14:18,  9.44s/it]test:224/314 | accuracy 6377  0.8896484375:  71%|███████   | 223/314 [29:36<14:18,  9.44s/it]      test:224/314 | accuracy 6377  0.8896484375:  71%|███████▏  | 224/314 [29:36<13:55,  9.28s/it]test:225/314 | accuracy 6406  0.8897222222222222:  71%|███████▏  | 224/314 [29:45<13:55,  9.28s/it]test:225/314 | accuracy 6406  0.8897222222222222:  72%|███████▏  | 225/314 [29:45<13:43,  9.26s/it]test:226/314 | accuracy 6437  0.8900719026548672:  72%|███████▏  | 225/314 [29:55<13:43,  9.26s/it]test:226/314 | accuracy 6437  0.8900719026548672:  72%|███████▏  | 226/314 [29:55<13:52,  9.46s/it]test:227/314 | accuracy 6467  0.8902808370044053:  72%|███████▏  | 226/314 [30:05<13:52,  9.46s/it]test:227/314 | accuracy 6467  0.8902808370044053:  72%|███████▏  | 227/314 [30:05<13:52,  9.56s/it]test:228/314 | accuracy 6496  0.8903508771929824:  72%|███████▏  | 227/314 [30:14<13:52,  9.56s/it]test:228/314 | accuracy 6496  0.8903508771929824:  73%|███████▎  | 228/314 [30:14<13:35,  9.49s/it]test:229/314 | accuracy 6528  0.8908296943231441:  73%|███████▎  | 228/314 [30:24<13:35,  9.49s/it]test:229/314 | accuracy 6528  0.8908296943231441:  73%|███████▎  | 229/314 [30:24<13:35,  9.60s/it]test:230/314 | accuracy 6556  0.8907608695652174:  73%|███████▎  | 229/314 [30:33<13:35,  9.60s/it]test:230/314 | accuracy 6556  0.8907608695652174:  73%|███████▎  | 230/314 [30:34<13:26,  9.60s/it]test:231/314 | accuracy 6584  0.8906926406926406:  73%|███████▎  | 230/314 [30:43<13:26,  9.60s/it]test:231/314 | accuracy 6584  0.8906926406926406:  74%|███████▎  | 231/314 [30:43<13:09,  9.51s/it]test:232/314 | accuracy 6614  0.8908943965517241:  74%|███████▎  | 231/314 [30:52<13:09,  9.51s/it]test:232/314 | accuracy 6614  0.8908943965517241:  74%|███████▍  | 232/314 [30:52<12:51,  9.41s/it]test:233/314 | accuracy 6645  0.8912285407725322:  74%|███████▍  | 232/314 [31:02<12:51,  9.41s/it]test:233/314 | accuracy 6645  0.8912285407725322:  74%|███████▍  | 233/314 [31:02<12:47,  9.47s/it]test:234/314 | accuracy 6677  0.8916933760683761:  74%|███████▍  | 233/314 [31:11<12:47,  9.47s/it]test:234/314 | accuracy 6677  0.8916933760683761:  75%|███████▍  | 234/314 [31:11<12:38,  9.48s/it]test:235/314 | accuracy 6707  0.8918882978723405:  75%|███████▍  | 234/314 [31:21<12:38,  9.48s/it]test:235/314 | accuracy 6707  0.8918882978723405:  75%|███████▍  | 235/314 [31:21<12:29,  9.49s/it]test:236/314 | accuracy 6739  0.8923463983050848:  75%|███████▍  | 235/314 [31:30<12:29,  9.49s/it]test:236/314 | accuracy 6739  0.8923463983050848:  75%|███████▌  | 236/314 [31:30<12:10,  9.36s/it]test:237/314 | accuracy 6765  0.8920094936708861:  75%|███████▌  | 236/314 [31:39<12:10,  9.36s/it]test:237/314 | accuracy 6765  0.8920094936708861:  75%|███████▌  | 237/314 [31:39<12:02,  9.39s/it]test:238/314 | accuracy 6795  0.8922006302521008:  75%|███████▌  | 237/314 [31:49<12:02,  9.39s/it]test:238/314 | accuracy 6795  0.8922006302521008:  76%|███████▌  | 238/314 [31:49<11:58,  9.45s/it]test:239/314 | accuracy 6827  0.8926516736401674:  76%|███████▌  | 238/314 [31:59<11:58,  9.45s/it]test:239/314 | accuracy 6827  0.8926516736401674:  76%|███████▌  | 239/314 [31:59<11:58,  9.58s/it]test:240/314 | accuracy 6858  0.89296875:  76%|███████▌  | 239/314 [32:08<11:58,  9.58s/it]        test:240/314 | accuracy 6858  0.89296875:  76%|███████▋  | 240/314 [32:08<11:50,  9.59s/it]test:241/314 | accuracy 6888  0.8931535269709544:  76%|███████▋  | 240/314 [32:18<11:50,  9.59s/it]test:241/314 | accuracy 6888  0.8931535269709544:  77%|███████▋  | 241/314 [32:18<11:37,  9.56s/it]test:242/314 | accuracy 6919  0.8934659090909091:  77%|███████▋  | 241/314 [32:27<11:37,  9.56s/it]test:242/314 | accuracy 6919  0.8934659090909091:  77%|███████▋  | 242/314 [32:27<11:28,  9.57s/it]test:243/314 | accuracy 6950  0.893775720164609:  77%|███████▋  | 242/314 [32:37<11:28,  9.57s/it] test:243/314 | accuracy 6950  0.893775720164609:  77%|███████▋  | 243/314 [32:37<11:15,  9.52s/it]test:244/314 | accuracy 6980  0.8939549180327869:  77%|███████▋  | 243/314 [32:46<11:15,  9.52s/it]test:244/314 | accuracy 6980  0.8939549180327869:  78%|███████▊  | 244/314 [32:46<10:57,  9.40s/it]test:245/314 | accuracy 7010  0.8941326530612245:  78%|███████▊  | 244/314 [32:55<10:57,  9.40s/it]test:245/314 | accuracy 7010  0.8941326530612245:  78%|███████▊  | 245/314 [32:55<10:47,  9.38s/it]test:246/314 | accuracy 7040  0.8943089430894309:  78%|███████▊  | 245/314 [33:04<10:47,  9.38s/it]test:246/314 | accuracy 7040  0.8943089430894309:  78%|███████▊  | 246/314 [33:05<10:37,  9.38s/it]test:247/314 | accuracy 7069  0.8943572874493927:  78%|███████▊  | 246/314 [33:14<10:37,  9.38s/it]test:247/314 | accuracy 7069  0.8943572874493927:  79%|███████▊  | 247/314 [33:14<10:36,  9.50s/it]test:248/314 | accuracy 7096  0.8941532258064516:  79%|███████▊  | 247/314 [33:24<10:36,  9.50s/it]test:248/314 | accuracy 7096  0.8941532258064516:  79%|███████▉  | 248/314 [33:24<10:24,  9.46s/it]test:249/314 | accuracy 7126  0.8943273092369478:  79%|███████▉  | 248/314 [33:33<10:24,  9.46s/it]test:249/314 | accuracy 7126  0.8943273092369478:  79%|███████▉  | 249/314 [33:33<10:12,  9.42s/it]test:250/314 | accuracy 7154  0.89425:  79%|███████▉  | 249/314 [33:43<10:12,  9.42s/it]           test:250/314 | accuracy 7154  0.89425:  80%|███████▉  | 250/314 [33:43<10:11,  9.56s/it]test:251/314 | accuracy 7183  0.8942978087649402:  80%|███████▉  | 250/314 [33:52<10:11,  9.56s/it]test:251/314 | accuracy 7183  0.8942978087649402:  80%|███████▉  | 251/314 [33:52<09:57,  9.48s/it]test:252/314 | accuracy 7215  0.8947172619047619:  80%|███████▉  | 251/314 [34:02<09:57,  9.48s/it]test:252/314 | accuracy 7215  0.8947172619047619:  80%|████████  | 252/314 [34:02<09:46,  9.46s/it]test:253/314 | accuracy 7247  0.8951333992094862:  80%|████████  | 252/314 [34:11<09:46,  9.46s/it]test:253/314 | accuracy 7247  0.8951333992094862:  81%|████████  | 253/314 [34:12<09:43,  9.56s/it]test:254/314 | accuracy 7278  0.8954232283464567:  81%|████████  | 253/314 [34:21<09:43,  9.56s/it]test:254/314 | accuracy 7278  0.8954232283464567:  81%|████████  | 254/314 [34:21<09:34,  9.58s/it]test:255/314 | accuracy 7309  0.8957107843137255:  81%|████████  | 254/314 [34:30<09:34,  9.58s/it]test:255/314 | accuracy 7309  0.8957107843137255:  81%|████████  | 255/314 [34:31<09:21,  9.52s/it]test:256/314 | accuracy 7339  0.8958740234375:  81%|████████  | 255/314 [34:40<09:21,  9.52s/it]   test:256/314 | accuracy 7339  0.8958740234375:  82%|████████▏ | 256/314 [34:40<09:11,  9.52s/it]test:257/314 | accuracy 7369  0.8960359922178989:  82%|████████▏ | 256/314 [34:49<09:11,  9.52s/it]test:257/314 | accuracy 7369  0.8960359922178989:  82%|████████▏ | 257/314 [34:49<09:00,  9.49s/it]test:258/314 | accuracy 7398  0.8960755813953488:  82%|████████▏ | 257/314 [34:59<09:00,  9.49s/it]test:258/314 | accuracy 7398  0.8960755813953488:  82%|████████▏ | 258/314 [34:59<08:50,  9.48s/it]test:259/314 | accuracy 7426  0.8959942084942085:  82%|████████▏ | 258/314 [35:09<08:50,  9.48s/it]test:259/314 | accuracy 7426  0.8959942084942085:  82%|████████▏ | 259/314 [35:09<08:47,  9.59s/it]test:260/314 | accuracy 7456  0.8961538461538462:  82%|████████▏ | 259/314 [35:18<08:47,  9.59s/it]test:260/314 | accuracy 7456  0.8961538461538462:  83%|████████▎ | 260/314 [35:18<08:33,  9.51s/it]test:261/314 | accuracy 7485  0.8961925287356322:  83%|████████▎ | 260/314 [35:27<08:33,  9.51s/it]test:261/314 | accuracy 7485  0.8961925287356322:  83%|████████▎ | 261/314 [35:27<08:22,  9.48s/it]test:262/314 | accuracy 7514  0.8962309160305344:  83%|████████▎ | 261/314 [35:37<08:22,  9.48s/it]test:262/314 | accuracy 7514  0.8962309160305344:  83%|████████▎ | 262/314 [35:37<08:11,  9.45s/it]test:263/314 | accuracy 7546  0.8966254752851711:  83%|████████▎ | 262/314 [35:46<08:11,  9.45s/it]test:263/314 | accuracy 7546  0.8966254752851711:  84%|████████▍ | 263/314 [35:46<07:58,  9.39s/it]test:264/314 | accuracy 7577  0.8968986742424242:  84%|████████▍ | 263/314 [35:56<07:58,  9.39s/it]test:264/314 | accuracy 7577  0.8968986742424242:  84%|████████▍ | 264/314 [35:56<07:54,  9.49s/it]test:265/314 | accuracy 7603  0.8965801886792453:  84%|████████▍ | 264/314 [36:05<07:54,  9.49s/it]test:265/314 | accuracy 7603  0.8965801886792453:  84%|████████▍ | 265/314 [36:05<07:42,  9.44s/it]test:266/314 | accuracy 7633  0.896734022556391:  84%|████████▍ | 265/314 [36:15<07:42,  9.44s/it] test:266/314 | accuracy 7633  0.896734022556391:  85%|████████▍ | 266/314 [36:15<07:38,  9.55s/it]test:267/314 | accuracy 7664  0.897003745318352:  85%|████████▍ | 266/314 [36:24<07:38,  9.55s/it]test:267/314 | accuracy 7664  0.897003745318352:  85%|████████▌ | 267/314 [36:24<07:27,  9.52s/it]test:268/314 | accuracy 7695  0.8972714552238806:  85%|████████▌ | 267/314 [36:34<07:27,  9.52s/it]test:268/314 | accuracy 7695  0.8972714552238806:  85%|████████▌ | 268/314 [36:34<07:20,  9.57s/it]test:269/314 | accuracy 7725  0.8974210037174721:  85%|████████▌ | 268/314 [36:44<07:20,  9.57s/it]test:269/314 | accuracy 7725  0.8974210037174721:  86%|████████▌ | 269/314 [36:44<07:12,  9.61s/it]test:270/314 | accuracy 7754  0.8974537037037037:  86%|████████▌ | 269/314 [36:53<07:12,  9.61s/it]test:270/314 | accuracy 7754  0.8974537037037037:  86%|████████▌ | 270/314 [36:53<07:00,  9.55s/it]test:271/314 | accuracy 7781  0.8972555350553506:  86%|████████▌ | 270/314 [37:02<07:00,  9.55s/it]test:271/314 | accuracy 7781  0.8972555350553506:  86%|████████▋ | 271/314 [37:02<06:45,  9.42s/it]test:272/314 | accuracy 7811  0.8974034926470589:  86%|████████▋ | 271/314 [37:12<06:45,  9.42s/it]test:272/314 | accuracy 7811  0.8974034926470589:  87%|████████▋ | 272/314 [37:12<06:41,  9.57s/it]test:273/314 | accuracy 7841  0.8975503663003663:  87%|████████▋ | 272/314 [37:21<06:41,  9.57s/it]test:273/314 | accuracy 7841  0.8975503663003663:  87%|████████▋ | 273/314 [37:21<06:28,  9.47s/it]test:274/314 | accuracy 7868  0.8973540145985401:  87%|████████▋ | 273/314 [37:31<06:28,  9.47s/it]test:274/314 | accuracy 7868  0.8973540145985401:  87%|████████▋ | 274/314 [37:31<06:17,  9.44s/it]test:275/314 | accuracy 7898  0.8975:  87%|████████▋ | 274/314 [37:40<06:17,  9.44s/it]            test:275/314 | accuracy 7898  0.8975:  88%|████████▊ | 275/314 [37:40<06:07,  9.42s/it]test:276/314 | accuracy 7927  0.8975317028985508:  88%|████████▊ | 275/314 [37:49<06:07,  9.42s/it]test:276/314 | accuracy 7927  0.8975317028985508:  88%|████████▊ | 276/314 [37:50<05:58,  9.42s/it]test:277/314 | accuracy 7959  0.8979016245487365:  88%|████████▊ | 276/314 [37:59<05:58,  9.42s/it]test:277/314 | accuracy 7959  0.8979016245487365:  88%|████████▊ | 277/314 [37:59<05:48,  9.41s/it]test:278/314 | accuracy 7989  0.8980440647482014:  88%|████████▊ | 277/314 [38:08<05:48,  9.41s/it]test:278/314 | accuracy 7989  0.8980440647482014:  89%|████████▊ | 278/314 [38:08<05:38,  9.41s/it]test:279/314 | accuracy 8018  0.8980734767025089:  89%|████████▊ | 278/314 [38:17<05:38,  9.41s/it]test:279/314 | accuracy 8018  0.8980734767025089:  89%|████████▉ | 279/314 [38:17<05:25,  9.30s/it]test:280/314 | accuracy 8047  0.8981026785714286:  89%|████████▉ | 279/314 [38:27<05:25,  9.30s/it]test:280/314 | accuracy 8047  0.8981026785714286:  89%|████████▉ | 280/314 [38:28<05:23,  9.52s/it]test:281/314 | accuracy 8079  0.8984653024911032:  89%|████████▉ | 280/314 [38:37<05:23,  9.52s/it]test:281/314 | accuracy 8079  0.8984653024911032:  89%|████████▉ | 281/314 [38:37<05:15,  9.56s/it]test:282/314 | accuracy 8108  0.8984929078014184:  89%|████████▉ | 281/314 [38:46<05:15,  9.56s/it]test:282/314 | accuracy 8108  0.8984929078014184:  90%|████████▉ | 282/314 [38:47<05:04,  9.50s/it]test:283/314 | accuracy 8137  0.8985203180212014:  90%|████████▉ | 282/314 [38:56<05:04,  9.50s/it]test:283/314 | accuracy 8137  0.8985203180212014:  90%|█████████ | 283/314 [38:56<04:53,  9.46s/it]test:284/314 | accuracy 8164  0.8983274647887324:  90%|█████████ | 283/314 [39:05<04:53,  9.46s/it]test:284/314 | accuracy 8164  0.8983274647887324:  90%|█████████ | 284/314 [39:05<04:43,  9.44s/it]test:285/314 | accuracy 8195  0.8985745614035088:  90%|█████████ | 284/314 [39:15<04:43,  9.44s/it]test:285/314 | accuracy 8195  0.8985745614035088:  91%|█████████ | 285/314 [39:15<04:34,  9.46s/it]test:286/314 | accuracy 8223  0.8984921328671329:  91%|█████████ | 285/314 [39:24<04:34,  9.46s/it]test:286/314 | accuracy 8223  0.8984921328671329:  91%|█████████ | 286/314 [39:24<04:24,  9.44s/it]test:287/314 | accuracy 8250  0.898301393728223:  91%|█████████ | 286/314 [39:33<04:24,  9.44s/it] test:287/314 | accuracy 8250  0.898301393728223:  91%|█████████▏| 287/314 [39:34<04:14,  9.44s/it]test:288/314 | accuracy 8282  0.8986545138888888:  91%|█████████▏| 287/314 [39:43<04:14,  9.44s/it]test:288/314 | accuracy 8282  0.8986545138888888:  92%|█████████▏| 288/314 [39:43<04:06,  9.47s/it]test:289/314 | accuracy 8312  0.8987889273356401:  92%|█████████▏| 288/314 [39:52<04:06,  9.47s/it]test:289/314 | accuracy 8312  0.8987889273356401:  92%|█████████▏| 289/314 [39:53<03:56,  9.47s/it]test:290/314 | accuracy 8343  0.8990301724137931:  92%|█████████▏| 289/314 [40:02<03:56,  9.47s/it]test:290/314 | accuracy 8343  0.8990301724137931:  92%|█████████▏| 290/314 [40:02<03:47,  9.47s/it]test:291/314 | accuracy 8374  0.8992697594501718:  92%|█████████▏| 290/314 [40:11<03:47,  9.47s/it]test:291/314 | accuracy 8374  0.8992697594501718:  93%|█████████▎| 291/314 [40:11<03:35,  9.36s/it]test:292/314 | accuracy 8405  0.899507705479452:  93%|█████████▎| 291/314 [40:20<03:35,  9.36s/it] test:292/314 | accuracy 8405  0.899507705479452:  93%|█████████▎| 292/314 [40:20<03:24,  9.30s/it]test:293/314 | accuracy 8437  0.8998506825938567:  93%|█████████▎| 292/314 [40:30<03:24,  9.30s/it]test:293/314 | accuracy 8437  0.8998506825938567:  93%|█████████▎| 293/314 [40:30<03:18,  9.47s/it]test:294/314 | accuracy 8468  0.9000850340136054:  93%|█████████▎| 293/314 [40:40<03:18,  9.47s/it]test:294/314 | accuracy 8468  0.9000850340136054:  94%|█████████▎| 294/314 [40:40<03:11,  9.57s/it]test:295/314 | accuracy 8498  0.9002118644067797:  94%|█████████▎| 294/314 [40:49<03:11,  9.57s/it]test:295/314 | accuracy 8498  0.9002118644067797:  94%|█████████▍| 295/314 [40:49<02:59,  9.45s/it]test:296/314 | accuracy 8527  0.9002322635135135:  94%|█████████▍| 295/314 [40:59<02:59,  9.45s/it]test:296/314 | accuracy 8527  0.9002322635135135:  94%|█████████▍| 296/314 [40:59<02:52,  9.56s/it]test:297/314 | accuracy 8555  0.9001473063973064:  94%|█████████▍| 296/314 [41:09<02:52,  9.56s/it]test:297/314 | accuracy 8555  0.9001473063973064:  95%|█████████▍| 297/314 [41:09<02:43,  9.59s/it]test:298/314 | accuracy 8583  0.9000629194630873:  95%|█████████▍| 297/314 [41:18<02:43,  9.59s/it]test:298/314 | accuracy 8583  0.9000629194630873:  95%|█████████▍| 298/314 [41:18<02:31,  9.49s/it]test:299/314 | accuracy 8614  0.9002926421404682:  95%|█████████▍| 298/314 [41:27<02:31,  9.49s/it]test:299/314 | accuracy 8614  0.9002926421404682:  95%|█████████▌| 299/314 [41:27<02:22,  9.50s/it]test:300/314 | accuracy 8645  0.9005208333333333:  95%|█████████▌| 299/314 [41:37<02:22,  9.50s/it]test:300/314 | accuracy 8645  0.9005208333333333:  96%|█████████▌| 300/314 [41:37<02:12,  9.46s/it]test:301/314 | accuracy 8675  0.9006436877076412:  96%|█████████▌| 300/314 [41:46<02:12,  9.46s/it]test:301/314 | accuracy 8675  0.9006436877076412:  96%|█████████▌| 301/314 [41:46<02:03,  9.48s/it]test:302/314 | accuracy 8702  0.9004552980132451:  96%|█████████▌| 301/314 [41:55<02:03,  9.48s/it]test:302/314 | accuracy 8702  0.9004552980132451:  96%|█████████▌| 302/314 [41:56<01:52,  9.37s/it]test:303/314 | accuracy 8730  0.9003712871287128:  96%|█████████▌| 302/314 [42:05<01:52,  9.37s/it]test:303/314 | accuracy 8730  0.9003712871287128:  96%|█████████▋| 303/314 [42:05<01:43,  9.39s/it]test:304/314 | accuracy 8761  0.9005962171052632:  96%|█████████▋| 303/314 [42:14<01:43,  9.39s/it]test:304/314 | accuracy 8761  0.9005962171052632:  97%|█████████▋| 304/314 [42:14<01:32,  9.29s/it]test:305/314 | accuracy 8791  0.9007172131147541:  97%|█████████▋| 304/314 [42:23<01:32,  9.29s/it]test:305/314 | accuracy 8791  0.9007172131147541:  97%|█████████▋| 305/314 [42:24<01:24,  9.37s/it]test:306/314 | accuracy 8821  0.9008374183006536:  97%|█████████▋| 305/314 [42:32<01:24,  9.37s/it]test:306/314 | accuracy 8821  0.9008374183006536:  97%|█████████▋| 306/314 [42:33<01:14,  9.26s/it]test:307/314 | accuracy 8849  0.9007532573289903:  97%|█████████▋| 306/314 [42:42<01:14,  9.26s/it]test:307/314 | accuracy 8849  0.9007532573289903:  98%|█████████▊| 307/314 [42:42<01:05,  9.35s/it]test:308/314 | accuracy 8879  0.900872564935065:  98%|█████████▊| 307/314 [42:51<01:05,  9.35s/it] test:308/314 | accuracy 8879  0.900872564935065:  98%|█████████▊| 308/314 [42:52<00:56,  9.38s/it]test:309/314 | accuracy 8907  0.9007888349514563:  98%|█████████▊| 308/314 [43:01<00:56,  9.38s/it]test:309/314 | accuracy 8907  0.9007888349514563:  98%|█████████▊| 309/314 [43:01<00:46,  9.39s/it]test:310/314 | accuracy 8936  0.9008064516129032:  98%|█████████▊| 309/314 [43:10<00:46,  9.39s/it]test:310/314 | accuracy 8936  0.9008064516129032:  99%|█████████▊| 310/314 [43:10<00:37,  9.40s/it]test:311/314 | accuracy 8965  0.9008239549839229:  99%|█████████▊| 310/314 [43:20<00:37,  9.40s/it]test:311/314 | accuracy 8965  0.9008239549839229:  99%|█████████▉| 311/314 [43:20<00:28,  9.44s/it]test:312/314 | accuracy 8996  0.9010416666666666:  99%|█████████▉| 311/314 [43:29<00:28,  9.44s/it]test:312/314 | accuracy 8996  0.9010416666666666:  99%|█████████▉| 312/314 [43:29<00:18,  9.47s/it]test:313/314 | accuracy 9027  0.9012579872204473:  99%|█████████▉| 312/314 [43:39<00:18,  9.47s/it]test:313/314 | accuracy 9027  0.9012579872204473: 100%|█████████▉| 313/314 [43:39<00:09,  9.44s/it]test:314/314 | accuracy 9052  0.9014140609440351: 100%|█████████▉| 313/314 [43:46<00:09,  9.44s/it]test:314/314 | accuracy 9052  0.9014140609440351: 100%|██████████| 314/314 [43:47<00:00,  8.96s/it]test:314/314 | accuracy 9052  0.9014140609440351: 100%|██████████| 314/314 [43:47<00:00,  8.37s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.4/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 4,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.90s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:31,  1.87s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:29,  1.86s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:27,  1.85s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:25,  1.84s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:23,  1.84s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:12<00:21,  1.83s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:14<00:20,  1.83s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:16<00:18,  1.84s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:18<00:16,  1.83s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:14,  1.84s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:12,  1.84s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:23<00:11,  1.84s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:25<00:09,  1.83s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:27<00:07,  1.84s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:29<00:05,  1.84s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:31<00:03,  1.83s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:33<00:01,  1.83s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:34<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:34<00:00,  1.82s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.4 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors
##### Loading 256 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors #####
  0%|          | 0/40 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/40 | accuracy 28  0.875:   0%|          | 0/40 [00:04<?, ?it/s]test:1/40 | accuracy 28  0.875:   2%|▎         | 1/40 [00:04<03:00,  4.63s/it]test:2/40 | accuracy 56  0.875:   2%|▎         | 1/40 [00:07<03:00,  4.63s/it]test:2/40 | accuracy 56  0.875:   5%|▌         | 2/40 [00:07<02:25,  3.82s/it]test:3/40 | accuracy 85  0.8854166666666666:   5%|▌         | 2/40 [00:11<02:25,  3.82s/it]test:3/40 | accuracy 85  0.8854166666666666:   8%|▊         | 3/40 [00:11<02:11,  3.56s/it]test:4/40 | accuracy 112  0.875:   8%|▊         | 3/40 [00:14<02:11,  3.56s/it]            test:4/40 | accuracy 112  0.875:  10%|█         | 4/40 [00:14<02:03,  3.43s/it]test:5/40 | accuracy 140  0.875:  10%|█         | 4/40 [00:17<02:03,  3.43s/it]test:5/40 | accuracy 140  0.875:  12%|█▎        | 5/40 [00:17<01:58,  3.37s/it]test:6/40 | accuracy 169  0.8802083333333334:  12%|█▎        | 5/40 [00:20<01:58,  3.37s/it]test:6/40 | accuracy 169  0.8802083333333334:  15%|█▌        | 6/40 [00:20<01:52,  3.31s/it]test:7/40 | accuracy 194  0.8660714285714286:  15%|█▌        | 6/40 [00:23<01:52,  3.31s/it]test:7/40 | accuracy 194  0.8660714285714286:  18%|█▊        | 7/40 [00:23<01:47,  3.26s/it]test:8/40 | accuracy 222  0.8671875:  18%|█▊        | 7/40 [00:27<01:47,  3.26s/it]         test:8/40 | accuracy 222  0.8671875:  20%|██        | 8/40 [00:27<01:43,  3.23s/it]test:9/40 | accuracy 249  0.8645833333333334:  20%|██        | 8/40 [00:30<01:43,  3.23s/it]test:9/40 | accuracy 249  0.8645833333333334:  22%|██▎       | 9/40 [00:30<01:40,  3.25s/it]test:10/40 | accuracy 277  0.865625:  22%|██▎       | 9/40 [00:33<01:40,  3.25s/it]         test:10/40 | accuracy 277  0.865625:  25%|██▌       | 10/40 [00:33<01:37,  3.24s/it]test:11/40 | accuracy 304  0.8636363636363636:  25%|██▌       | 10/40 [00:36<01:37,  3.24s/it]test:11/40 | accuracy 304  0.8636363636363636:  28%|██▊       | 11/40 [00:36<01:33,  3.24s/it]test:12/40 | accuracy 333  0.8671875:  28%|██▊       | 11/40 [00:40<01:33,  3.24s/it]         test:12/40 | accuracy 333  0.8671875:  30%|███       | 12/40 [00:40<01:31,  3.25s/it]test:13/40 | accuracy 361  0.8677884615384616:  30%|███       | 12/40 [00:43<01:31,  3.25s/it]test:13/40 | accuracy 361  0.8677884615384616:  32%|███▎      | 13/40 [00:43<01:28,  3.28s/it]test:14/40 | accuracy 386  0.8616071428571429:  32%|███▎      | 13/40 [00:46<01:28,  3.28s/it]test:14/40 | accuracy 386  0.8616071428571429:  35%|███▌      | 14/40 [00:46<01:25,  3.28s/it]test:15/40 | accuracy 409  0.8520833333333333:  35%|███▌      | 14/40 [00:50<01:25,  3.28s/it]test:15/40 | accuracy 409  0.8520833333333333:  38%|███▊      | 15/40 [00:50<01:21,  3.27s/it]test:16/40 | accuracy 434  0.84765625:  38%|███▊      | 15/40 [00:53<01:21,  3.27s/it]        test:16/40 | accuracy 434  0.84765625:  40%|████      | 16/40 [00:53<01:18,  3.26s/it]test:17/40 | accuracy 457  0.8400735294117647:  40%|████      | 16/40 [00:56<01:18,  3.26s/it]test:17/40 | accuracy 457  0.8400735294117647:  42%|████▎     | 17/40 [00:56<01:14,  3.25s/it]test:18/40 | accuracy 485  0.8420138888888888:  42%|████▎     | 17/40 [00:59<01:14,  3.25s/it]test:18/40 | accuracy 485  0.8420138888888888:  45%|████▌     | 18/40 [00:59<01:11,  3.25s/it]test:19/40 | accuracy 510  0.8388157894736842:  45%|████▌     | 18/40 [01:03<01:11,  3.25s/it]test:19/40 | accuracy 510  0.8388157894736842:  48%|████▊     | 19/40 [01:03<01:08,  3.27s/it]test:20/40 | accuracy 533  0.8328125:  48%|████▊     | 19/40 [01:06<01:08,  3.27s/it]         test:20/40 | accuracy 533  0.8328125:  50%|█████     | 20/40 [01:06<01:05,  3.28s/it]test:21/40 | accuracy 555  0.8258928571428571:  50%|█████     | 20/40 [01:09<01:05,  3.28s/it]test:21/40 | accuracy 555  0.8258928571428571:  52%|█████▎    | 21/40 [01:09<01:02,  3.29s/it]test:22/40 | accuracy 584  0.8295454545454546:  52%|█████▎    | 21/40 [01:12<01:02,  3.29s/it]test:22/40 | accuracy 584  0.8295454545454546:  55%|█████▌    | 22/40 [01:12<00:59,  3.29s/it]test:23/40 | accuracy 610  0.8288043478260869:  55%|█████▌    | 22/40 [01:16<00:59,  3.29s/it]test:23/40 | accuracy 610  0.8288043478260869:  57%|█████▊    | 23/40 [01:16<00:56,  3.29s/it]test:24/40 | accuracy 632  0.8229166666666666:  57%|█████▊    | 23/40 [01:19<00:56,  3.29s/it]test:24/40 | accuracy 632  0.8229166666666666:  60%|██████    | 24/40 [01:19<00:52,  3.29s/it]test:25/40 | accuracy 661  0.82625:  60%|██████    | 24/40 [01:22<00:52,  3.29s/it]           test:25/40 | accuracy 661  0.82625:  62%|██████▎   | 25/40 [01:22<00:49,  3.29s/it]test:26/40 | accuracy 685  0.8233173076923077:  62%|██████▎   | 25/40 [01:26<00:49,  3.29s/it]test:26/40 | accuracy 685  0.8233173076923077:  65%|██████▌   | 26/40 [01:26<00:46,  3.34s/it]test:27/40 | accuracy 712  0.8240740740740741:  65%|██████▌   | 26/40 [01:29<00:46,  3.34s/it]test:27/40 | accuracy 712  0.8240740740740741:  68%|██████▊   | 27/40 [01:29<00:43,  3.31s/it]test:28/40 | accuracy 738  0.8236607142857143:  68%|██████▊   | 27/40 [01:32<00:43,  3.31s/it]test:28/40 | accuracy 738  0.8236607142857143:  70%|███████   | 28/40 [01:32<00:39,  3.29s/it]test:29/40 | accuracy 764  0.8232758620689655:  70%|███████   | 28/40 [01:36<00:39,  3.29s/it]test:29/40 | accuracy 764  0.8232758620689655:  72%|███████▎  | 29/40 [01:36<00:36,  3.28s/it]test:30/40 | accuracy 795  0.828125:  72%|███████▎  | 29/40 [01:39<00:36,  3.28s/it]          test:30/40 | accuracy 795  0.828125:  75%|███████▌  | 30/40 [01:39<00:32,  3.27s/it]test:31/40 | accuracy 821  0.8276209677419355:  75%|███████▌  | 30/40 [01:42<00:32,  3.27s/it]test:31/40 | accuracy 821  0.8276209677419355:  78%|███████▊  | 31/40 [01:42<00:29,  3.26s/it]test:32/40 | accuracy 843  0.8232421875:  78%|███████▊  | 31/40 [01:45<00:29,  3.26s/it]      test:32/40 | accuracy 843  0.8232421875:  80%|████████  | 32/40 [01:45<00:25,  3.23s/it]test:33/40 | accuracy 866  0.8200757575757576:  80%|████████  | 32/40 [01:48<00:25,  3.23s/it]test:33/40 | accuracy 866  0.8200757575757576:  82%|████████▎ | 33/40 [01:48<00:22,  3.23s/it]test:34/40 | accuracy 890  0.8180147058823529:  82%|████████▎ | 33/40 [01:52<00:22,  3.23s/it]test:34/40 | accuracy 890  0.8180147058823529:  85%|████████▌ | 34/40 [01:52<00:19,  3.21s/it]test:35/40 | accuracy 916  0.8178571428571428:  85%|████████▌ | 34/40 [01:55<00:19,  3.21s/it]test:35/40 | accuracy 916  0.8178571428571428:  88%|████████▊ | 35/40 [01:55<00:16,  3.25s/it]test:36/40 | accuracy 945  0.8203125:  88%|████████▊ | 35/40 [01:58<00:16,  3.25s/it]         test:36/40 | accuracy 945  0.8203125:  90%|█████████ | 36/40 [01:58<00:13,  3.27s/it]test:37/40 | accuracy 973  0.8217905405405406:  90%|█████████ | 36/40 [02:02<00:13,  3.27s/it]test:37/40 | accuracy 973  0.8217905405405406:  92%|█████████▎| 37/40 [02:02<00:09,  3.28s/it]test:38/40 | accuracy 1002  0.8240131578947368:  92%|█████████▎| 37/40 [02:05<00:09,  3.28s/it]test:38/40 | accuracy 1002  0.8240131578947368:  95%|█████████▌| 38/40 [02:05<00:06,  3.28s/it]test:39/40 | accuracy 1030  0.8253205128205128:  95%|█████████▌| 38/40 [02:08<00:06,  3.28s/it]test:39/40 | accuracy 1030  0.8253205128205128:  98%|█████████▊| 39/40 [02:08<00:03,  3.27s/it]test:40/40 | accuracy 1045  0.824782951854775:  98%|█████████▊| 39/40 [02:10<00:03,  3.27s/it] test:40/40 | accuracy 1045  0.824782951854775: 100%|██████████| 40/40 [02:10<00:00,  2.95s/it]test:40/40 | accuracy 1045  0.824782951854775: 100%|██████████| 40/40 [02:10<00:00,  3.27s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.4/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 4,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.92s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.90s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.91s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.90s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.90s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.89s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.87s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.88s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:18,  1.88s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:18<00:16,  1.87s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:14,  1.87s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.88s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.88s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.87s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.89s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.89s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.87s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:33<00:01,  1.87s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.87s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.4 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors
##### Loading 256 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors #####
  0%|          | 0/37 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/37 | accuracy 24  0.75:   0%|          | 0/37 [00:06<?, ?it/s]test:1/37 | accuracy 24  0.75:   3%|▎         | 1/37 [00:06<03:42,  6.18s/it]test:2/37 | accuracy 45  0.703125:   3%|▎         | 1/37 [00:11<03:42,  6.18s/it]test:2/37 | accuracy 45  0.703125:   5%|▌         | 2/37 [00:11<03:16,  5.62s/it]test:3/37 | accuracy 68  0.7083333333333334:   5%|▌         | 2/37 [00:16<03:16,  5.62s/it]test:3/37 | accuracy 68  0.7083333333333334:   8%|▊         | 3/37 [00:16<03:04,  5.43s/it]test:4/37 | accuracy 92  0.71875:   8%|▊         | 3/37 [00:21<03:04,  5.43s/it]           test:4/37 | accuracy 92  0.71875:  11%|█         | 4/37 [00:21<02:50,  5.17s/it]test:5/37 | accuracy 115  0.71875:  11%|█         | 4/37 [00:25<02:50,  5.17s/it]test:5/37 | accuracy 115  0.71875:  14%|█▎        | 5/37 [00:25<02:37,  4.93s/it]test:6/37 | accuracy 135  0.703125:  14%|█▎        | 5/37 [00:31<02:37,  4.93s/it]test:6/37 | accuracy 135  0.703125:  16%|█▌        | 6/37 [00:31<02:42,  5.23s/it]test:7/37 | accuracy 161  0.71875:  16%|█▌        | 6/37 [00:36<02:42,  5.23s/it] test:7/37 | accuracy 161  0.71875:  19%|█▉        | 7/37 [00:36<02:30,  5.03s/it]test:8/37 | accuracy 184  0.71875:  19%|█▉        | 7/37 [00:43<02:30,  5.03s/it]test:8/37 | accuracy 184  0.71875:  22%|██▏       | 8/37 [00:43<02:48,  5.81s/it]test:9/37 | accuracy 203  0.7048611111111112:  22%|██▏       | 8/37 [00:49<02:48,  5.81s/it]test:9/37 | accuracy 203  0.7048611111111112:  24%|██▍       | 9/37 [00:49<02:45,  5.90s/it]test:10/37 | accuracy 221  0.690625:  24%|██▍       | 9/37 [00:55<02:45,  5.90s/it]         test:10/37 | accuracy 221  0.690625:  27%|██▋       | 10/37 [00:55<02:36,  5.79s/it]test:11/37 | accuracy 240  0.6818181818181818:  27%|██▋       | 10/37 [01:00<02:36,  5.79s/it]test:11/37 | accuracy 240  0.6818181818181818:  30%|██▉       | 11/37 [01:00<02:26,  5.63s/it]test:12/37 | accuracy 265  0.6901041666666666:  30%|██▉       | 11/37 [01:06<02:26,  5.63s/it]test:12/37 | accuracy 265  0.6901041666666666:  32%|███▏      | 12/37 [01:06<02:24,  5.80s/it]test:13/37 | accuracy 287  0.6899038461538461:  32%|███▏      | 12/37 [01:11<02:24,  5.80s/it]test:13/37 | accuracy 287  0.6899038461538461:  35%|███▌      | 13/37 [01:11<02:11,  5.50s/it]test:14/37 | accuracy 309  0.6897321428571429:  35%|███▌      | 13/37 [01:17<02:11,  5.50s/it]test:14/37 | accuracy 309  0.6897321428571429:  38%|███▊      | 14/37 [01:17<02:08,  5.60s/it]test:15/37 | accuracy 338  0.7041666666666667:  38%|███▊      | 14/37 [01:23<02:08,  5.60s/it]test:15/37 | accuracy 338  0.7041666666666667:  41%|████      | 15/37 [01:23<02:02,  5.57s/it]test:16/37 | accuracy 362  0.70703125:  41%|████      | 15/37 [01:28<02:02,  5.57s/it]        test:16/37 | accuracy 362  0.70703125:  43%|████▎     | 16/37 [01:28<01:53,  5.41s/it]test:17/37 | accuracy 386  0.7095588235294118:  43%|████▎     | 16/37 [01:33<01:53,  5.41s/it]test:17/37 | accuracy 386  0.7095588235294118:  46%|████▌     | 17/37 [01:33<01:46,  5.30s/it]test:18/37 | accuracy 413  0.7170138888888888:  46%|████▌     | 17/37 [01:38<01:46,  5.30s/it]test:18/37 | accuracy 413  0.7170138888888888:  49%|████▊     | 18/37 [01:38<01:43,  5.43s/it]test:19/37 | accuracy 439  0.7220394736842105:  49%|████▊     | 18/37 [01:44<01:43,  5.43s/it]test:19/37 | accuracy 439  0.7220394736842105:  51%|█████▏    | 19/37 [01:44<01:40,  5.59s/it]test:20/37 | accuracy 458  0.715625:  51%|█████▏    | 19/37 [01:49<01:40,  5.59s/it]          test:20/37 | accuracy 458  0.715625:  54%|█████▍    | 20/37 [01:49<01:31,  5.36s/it]test:21/37 | accuracy 483  0.71875:  54%|█████▍    | 20/37 [01:54<01:31,  5.36s/it] test:21/37 | accuracy 483  0.71875:  57%|█████▋    | 21/37 [01:54<01:22,  5.17s/it]test:22/37 | accuracy 509  0.7230113636363636:  57%|█████▋    | 21/37 [01:59<01:22,  5.17s/it]test:22/37 | accuracy 509  0.7230113636363636:  59%|█████▉    | 22/37 [01:59<01:17,  5.13s/it]test:23/37 | accuracy 531  0.7214673913043478:  59%|█████▉    | 22/37 [02:04<01:17,  5.13s/it]test:23/37 | accuracy 531  0.7214673913043478:  62%|██████▏   | 23/37 [02:04<01:12,  5.16s/it]test:24/37 | accuracy 551  0.7174479166666666:  62%|██████▏   | 23/37 [02:09<01:12,  5.16s/it]test:24/37 | accuracy 551  0.7174479166666666:  65%|██████▍   | 24/37 [02:09<01:05,  5.04s/it]test:25/37 | accuracy 571  0.71375:  65%|██████▍   | 24/37 [02:14<01:05,  5.04s/it]           test:25/37 | accuracy 571  0.71375:  68%|██████▊   | 25/37 [02:14<01:02,  5.17s/it]test:26/37 | accuracy 591  0.7103365384615384:  68%|██████▊   | 25/37 [02:20<01:02,  5.17s/it]test:26/37 | accuracy 591  0.7103365384615384:  70%|███████   | 26/37 [02:20<00:59,  5.42s/it]test:27/37 | accuracy 612  0.7083333333333334:  70%|███████   | 26/37 [02:27<00:59,  5.42s/it]test:27/37 | accuracy 612  0.7083333333333334:  73%|███████▎  | 27/37 [02:27<00:57,  5.71s/it]test:28/37 | accuracy 631  0.7042410714285714:  73%|███████▎  | 27/37 [02:32<00:57,  5.71s/it]test:28/37 | accuracy 631  0.7042410714285714:  76%|███████▌  | 28/37 [02:32<00:49,  5.52s/it]test:29/37 | accuracy 650  0.7004310344827587:  76%|███████▌  | 28/37 [02:38<00:49,  5.52s/it]test:29/37 | accuracy 650  0.7004310344827587:  78%|███████▊  | 29/37 [02:38<00:45,  5.70s/it]test:30/37 | accuracy 671  0.6989583333333333:  78%|███████▊  | 29/37 [02:43<00:45,  5.70s/it]test:30/37 | accuracy 671  0.6989583333333333:  81%|████████  | 30/37 [02:43<00:38,  5.50s/it]test:31/37 | accuracy 697  0.7026209677419355:  81%|████████  | 30/37 [02:48<00:38,  5.50s/it]test:31/37 | accuracy 697  0.7026209677419355:  84%|████████▍ | 31/37 [02:48<00:31,  5.28s/it]test:32/37 | accuracy 721  0.7041015625:  84%|████████▍ | 31/37 [02:53<00:31,  5.28s/it]      test:32/37 | accuracy 721  0.7041015625:  86%|████████▋ | 32/37 [02:53<00:26,  5.31s/it]test:33/37 | accuracy 744  0.7045454545454546:  86%|████████▋ | 32/37 [02:58<00:26,  5.31s/it]test:33/37 | accuracy 744  0.7045454545454546:  89%|████████▉ | 33/37 [02:58<00:20,  5.20s/it]test:34/37 | accuracy 767  0.7049632352941176:  89%|████████▉ | 33/37 [03:03<00:20,  5.20s/it]test:34/37 | accuracy 767  0.7049632352941176:  92%|█████████▏| 34/37 [03:03<00:15,  5.22s/it]test:35/37 | accuracy 791  0.70625:  92%|█████████▏| 34/37 [03:08<00:15,  5.22s/it]           test:35/37 | accuracy 791  0.70625:  95%|█████████▍| 35/37 [03:08<00:10,  5.14s/it]test:36/37 | accuracy 812  0.7048611111111112:  95%|█████████▍| 35/37 [03:13<00:10,  5.14s/it]test:36/37 | accuracy 812  0.7048611111111112:  97%|█████████▋| 36/37 [03:13<00:04,  4.99s/it]test:37/37 | accuracy 827  0.7056313993174061:  97%|█████████▋| 36/37 [03:16<00:04,  4.99s/it]test:37/37 | accuracy 827  0.7056313993174061: 100%|██████████| 37/37 [03:16<00:00,  4.40s/it]test:37/37 | accuracy 827  0.7056313993174061: 100%|██████████| 37/37 [03:16<00:00,  5.31s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.4/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 4,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.94s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.91s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.90s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.88s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.89s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.89s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.88s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.88s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:16<00:18,  1.88s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:18<00:16,  1.88s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.90s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.89s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.89s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.88s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.88s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.88s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:32<00:03,  1.88s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:33<00:01,  1.88s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.87s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.4 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors
##### Loading 256 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors #####
  0%|          | 0/75 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/75 | accuracy 26  0.8125:   0%|          | 0/75 [00:05<?, ?it/s]test:1/75 | accuracy 26  0.8125:   1%|▏         | 1/75 [00:05<06:53,  5.58s/it]test:2/75 | accuracy 55  0.859375:   1%|▏         | 1/75 [00:10<06:53,  5.58s/it]test:2/75 | accuracy 55  0.859375:   3%|▎         | 2/75 [00:10<06:35,  5.41s/it]test:3/75 | accuracy 81  0.84375:   3%|▎         | 2/75 [00:15<06:35,  5.41s/it] test:3/75 | accuracy 81  0.84375:   4%|▍         | 3/75 [00:15<06:10,  5.15s/it]test:4/75 | accuracy 107  0.8359375:   4%|▍         | 3/75 [00:20<06:10,  5.15s/it]test:4/75 | accuracy 107  0.8359375:   5%|▌         | 4/75 [00:20<05:47,  4.89s/it]test:5/75 | accuracy 136  0.85:   5%|▌         | 4/75 [00:27<05:47,  4.89s/it]     test:5/75 | accuracy 136  0.85:   7%|▋         | 5/75 [00:27<06:36,  5.66s/it]test:6/75 | accuracy 162  0.84375:   7%|▋         | 5/75 [00:32<06:36,  5.66s/it]test:6/75 | accuracy 162  0.84375:   8%|▊         | 6/75 [00:32<06:14,  5.43s/it]test:7/75 | accuracy 190  0.8482142857142857:   8%|▊         | 6/75 [00:37<06:14,  5.43s/it]test:7/75 | accuracy 190  0.8482142857142857:   9%|▉         | 7/75 [00:37<06:00,  5.31s/it]test:8/75 | accuracy 217  0.84765625:   9%|▉         | 7/75 [00:41<06:00,  5.31s/it]        test:8/75 | accuracy 217  0.84765625:  11%|█         | 8/75 [00:41<05:39,  5.06s/it]test:9/75 | accuracy 246  0.8541666666666666:  11%|█         | 8/75 [00:46<05:39,  5.06s/it]test:9/75 | accuracy 246  0.8541666666666666:  12%|█▏        | 9/75 [00:46<05:20,  4.86s/it]test:10/75 | accuracy 271  0.846875:  12%|█▏        | 9/75 [00:51<05:20,  4.86s/it]         test:10/75 | accuracy 271  0.846875:  13%|█▎        | 10/75 [00:51<05:30,  5.09s/it]test:11/75 | accuracy 300  0.8522727272727273:  13%|█▎        | 10/75 [00:56<05:30,  5.09s/it]test:11/75 | accuracy 300  0.8522727272727273:  15%|█▍        | 11/75 [00:56<05:13,  4.90s/it]test:12/75 | accuracy 329  0.8567708333333334:  15%|█▍        | 11/75 [01:01<05:13,  4.90s/it]test:12/75 | accuracy 329  0.8567708333333334:  16%|█▌        | 12/75 [01:01<05:12,  4.96s/it]test:13/75 | accuracy 357  0.8581730769230769:  16%|█▌        | 12/75 [01:06<05:12,  4.96s/it]test:13/75 | accuracy 357  0.8581730769230769:  17%|█▋        | 13/75 [01:06<05:01,  4.87s/it]test:14/75 | accuracy 385  0.859375:  17%|█▋        | 13/75 [01:11<05:01,  4.87s/it]          test:14/75 | accuracy 385  0.859375:  19%|█▊        | 14/75 [01:11<05:12,  5.13s/it]test:15/75 | accuracy 413  0.8604166666666667:  19%|█▊        | 14/75 [01:16<05:12,  5.13s/it]test:15/75 | accuracy 413  0.8604166666666667:  20%|██        | 15/75 [01:16<04:59,  4.99s/it]test:16/75 | accuracy 441  0.861328125:  20%|██        | 15/75 [01:21<04:59,  4.99s/it]       test:16/75 | accuracy 441  0.861328125:  21%|██▏       | 16/75 [01:21<04:51,  4.94s/it]test:17/75 | accuracy 465  0.8547794117647058:  21%|██▏       | 16/75 [01:26<04:51,  4.94s/it]test:17/75 | accuracy 465  0.8547794117647058:  23%|██▎       | 17/75 [01:26<04:58,  5.14s/it]test:18/75 | accuracy 492  0.8541666666666666:  23%|██▎       | 17/75 [01:31<04:58,  5.14s/it]test:18/75 | accuracy 492  0.8541666666666666:  24%|██▍       | 18/75 [01:31<04:44,  5.00s/it]test:19/75 | accuracy 521  0.8569078947368421:  24%|██▍       | 18/75 [01:36<04:44,  5.00s/it]test:19/75 | accuracy 521  0.8569078947368421:  25%|██▌       | 19/75 [01:36<04:39,  5.00s/it]test:20/75 | accuracy 550  0.859375:  25%|██▌       | 19/75 [01:41<04:39,  5.00s/it]          test:20/75 | accuracy 550  0.859375:  27%|██▋       | 20/75 [01:41<04:28,  4.89s/it]test:21/75 | accuracy 575  0.8556547619047619:  27%|██▋       | 20/75 [01:45<04:28,  4.89s/it]test:21/75 | accuracy 575  0.8556547619047619:  28%|██▊       | 21/75 [01:45<04:21,  4.84s/it]test:22/75 | accuracy 599  0.8508522727272727:  28%|██▊       | 21/75 [01:50<04:21,  4.84s/it]test:22/75 | accuracy 599  0.8508522727272727:  29%|██▉       | 22/75 [01:50<04:19,  4.90s/it]test:23/75 | accuracy 626  0.8505434782608695:  29%|██▉       | 22/75 [01:56<04:19,  4.90s/it]test:23/75 | accuracy 626  0.8505434782608695:  31%|███       | 23/75 [01:56<04:27,  5.15s/it]test:24/75 | accuracy 655  0.8528645833333334:  31%|███       | 23/75 [02:01<04:27,  5.15s/it]test:24/75 | accuracy 655  0.8528645833333334:  32%|███▏      | 24/75 [02:01<04:16,  5.03s/it]test:25/75 | accuracy 681  0.85125:  32%|███▏      | 24/75 [02:06<04:16,  5.03s/it]           test:25/75 | accuracy 681  0.85125:  33%|███▎      | 25/75 [02:06<04:13,  5.06s/it]test:26/75 | accuracy 707  0.8497596153846154:  33%|███▎      | 25/75 [02:11<04:13,  5.06s/it]test:26/75 | accuracy 707  0.8497596153846154:  35%|███▍      | 26/75 [02:11<04:06,  5.02s/it]test:27/75 | accuracy 733  0.8483796296296297:  35%|███▍      | 26/75 [02:16<04:06,  5.02s/it]test:27/75 | accuracy 733  0.8483796296296297:  36%|███▌      | 27/75 [02:16<04:00,  5.01s/it]test:28/75 | accuracy 763  0.8515625:  36%|███▌      | 27/75 [02:21<04:00,  5.01s/it]         test:28/75 | accuracy 763  0.8515625:  37%|███▋      | 28/75 [02:21<03:58,  5.07s/it]test:29/75 | accuracy 789  0.8502155172413793:  37%|███▋      | 28/75 [02:26<03:58,  5.07s/it]test:29/75 | accuracy 789  0.8502155172413793:  39%|███▊      | 29/75 [02:26<03:48,  4.96s/it]test:30/75 | accuracy 815  0.8489583333333334:  39%|███▊      | 29/75 [02:31<03:48,  4.96s/it]test:30/75 | accuracy 815  0.8489583333333334:  40%|████      | 30/75 [02:31<03:41,  4.92s/it]test:31/75 | accuracy 843  0.8497983870967742:  40%|████      | 30/75 [02:36<03:41,  4.92s/it]test:31/75 | accuracy 843  0.8497983870967742:  41%|████▏     | 31/75 [02:36<03:40,  5.01s/it]test:32/75 | accuracy 871  0.8505859375:  41%|████▏     | 31/75 [02:42<03:40,  5.01s/it]      test:32/75 | accuracy 871  0.8505859375:  43%|████▎     | 32/75 [02:42<03:54,  5.45s/it]test:33/75 | accuracy 895  0.8475378787878788:  43%|████▎     | 32/75 [02:47<03:54,  5.45s/it]test:33/75 | accuracy 895  0.8475378787878788:  44%|████▍     | 33/75 [02:47<03:42,  5.29s/it]test:34/75 | accuracy 921  0.8465073529411765:  44%|████▍     | 33/75 [02:53<03:42,  5.29s/it]test:34/75 | accuracy 921  0.8465073529411765:  45%|████▌     | 34/75 [02:53<03:40,  5.39s/it]test:35/75 | accuracy 950  0.8482142857142857:  45%|████▌     | 34/75 [02:58<03:40,  5.39s/it]test:35/75 | accuracy 950  0.8482142857142857:  47%|████▋     | 35/75 [02:58<03:25,  5.15s/it]test:36/75 | accuracy 978  0.8489583333333334:  47%|████▋     | 35/75 [03:02<03:25,  5.15s/it]test:36/75 | accuracy 978  0.8489583333333334:  48%|████▊     | 36/75 [03:02<03:14,  4.98s/it]test:37/75 | accuracy 1005  0.8488175675675675:  48%|████▊     | 36/75 [03:07<03:14,  4.98s/it]test:37/75 | accuracy 1005  0.8488175675675675:  49%|████▉     | 37/75 [03:07<03:10,  5.00s/it]test:38/75 | accuracy 1031  0.8478618421052632:  49%|████▉     | 37/75 [03:12<03:10,  5.00s/it]test:38/75 | accuracy 1031  0.8478618421052632:  51%|█████     | 38/75 [03:12<03:01,  4.91s/it]test:39/75 | accuracy 1057  0.8469551282051282:  51%|█████     | 38/75 [03:17<03:01,  4.91s/it]test:39/75 | accuracy 1057  0.8469551282051282:  52%|█████▏    | 39/75 [03:17<02:58,  4.95s/it]test:40/75 | accuracy 1085  0.84765625:  52%|█████▏    | 39/75 [03:23<02:58,  4.95s/it]        test:40/75 | accuracy 1085  0.84765625:  53%|█████▎    | 40/75 [03:23<03:03,  5.24s/it]test:41/75 | accuracy 1114  0.8490853658536586:  53%|█████▎    | 40/75 [03:29<03:03,  5.24s/it]test:41/75 | accuracy 1114  0.8490853658536586:  55%|█████▍    | 41/75 [03:29<03:05,  5.45s/it]test:42/75 | accuracy 1144  0.8511904761904762:  55%|█████▍    | 41/75 [03:33<03:05,  5.45s/it]test:42/75 | accuracy 1144  0.8511904761904762:  56%|█████▌    | 42/75 [03:33<02:50,  5.15s/it]test:43/75 | accuracy 1174  0.8531976744186046:  56%|█████▌    | 42/75 [03:39<02:50,  5.15s/it]test:43/75 | accuracy 1174  0.8531976744186046:  57%|█████▋    | 43/75 [03:39<02:49,  5.30s/it]test:44/75 | accuracy 1199  0.8515625:  57%|█████▋    | 43/75 [03:44<02:49,  5.30s/it]         test:44/75 | accuracy 1199  0.8515625:  59%|█████▊    | 44/75 [03:44<02:43,  5.26s/it]test:45/75 | accuracy 1227  0.8520833333333333:  59%|█████▊    | 44/75 [03:49<02:43,  5.26s/it]test:45/75 | accuracy 1227  0.8520833333333333:  60%|██████    | 45/75 [03:49<02:33,  5.13s/it]test:46/75 | accuracy 1254  0.8519021739130435:  60%|██████    | 45/75 [03:53<02:33,  5.13s/it]test:46/75 | accuracy 1254  0.8519021739130435:  61%|██████▏   | 46/75 [03:53<02:23,  4.96s/it]test:47/75 | accuracy 1281  0.8517287234042553:  61%|██████▏   | 46/75 [03:58<02:23,  4.96s/it]test:47/75 | accuracy 1281  0.8517287234042553:  63%|██████▎   | 47/75 [03:58<02:17,  4.92s/it]test:48/75 | accuracy 1304  0.8489583333333334:  63%|██████▎   | 47/75 [04:03<02:17,  4.92s/it]test:48/75 | accuracy 1304  0.8489583333333334:  64%|██████▍   | 48/75 [04:03<02:13,  4.96s/it]test:49/75 | accuracy 1333  0.8501275510204082:  64%|██████▍   | 48/75 [04:08<02:13,  4.96s/it]test:49/75 | accuracy 1333  0.8501275510204082:  65%|██████▌   | 49/75 [04:08<02:04,  4.80s/it]test:50/75 | accuracy 1364  0.8525:  65%|██████▌   | 49/75 [04:12<02:04,  4.80s/it]            test:50/75 | accuracy 1364  0.8525:  67%|██████▋   | 50/75 [04:12<01:58,  4.76s/it]test:51/75 | accuracy 1391  0.852328431372549:  67%|██████▋   | 50/75 [04:17<01:58,  4.76s/it]test:51/75 | accuracy 1391  0.852328431372549:  68%|██████▊   | 51/75 [04:17<01:55,  4.82s/it]test:52/75 | accuracy 1419  0.8527644230769231:  68%|██████▊   | 51/75 [04:23<01:55,  4.82s/it]test:52/75 | accuracy 1419  0.8527644230769231:  69%|██████▉   | 52/75 [04:23<01:55,  5.01s/it]test:53/75 | accuracy 1448  0.8537735849056604:  69%|██████▉   | 52/75 [04:27<01:55,  5.01s/it]test:53/75 | accuracy 1448  0.8537735849056604:  71%|███████   | 53/75 [04:27<01:47,  4.88s/it]test:54/75 | accuracy 1474  0.8530092592592593:  71%|███████   | 53/75 [04:32<01:47,  4.88s/it]test:54/75 | accuracy 1474  0.8530092592592593:  72%|███████▏  | 54/75 [04:32<01:41,  4.83s/it]test:55/75 | accuracy 1504  0.8545454545454545:  72%|███████▏  | 54/75 [04:37<01:41,  4.83s/it]test:55/75 | accuracy 1504  0.8545454545454545:  73%|███████▎  | 55/75 [04:37<01:39,  4.97s/it]test:56/75 | accuracy 1531  0.8543526785714286:  73%|███████▎  | 55/75 [04:44<01:39,  4.97s/it]test:56/75 | accuracy 1531  0.8543526785714286:  75%|███████▍  | 56/75 [04:44<01:44,  5.48s/it]test:57/75 | accuracy 1558  0.8541666666666666:  75%|███████▍  | 56/75 [04:50<01:44,  5.48s/it]test:57/75 | accuracy 1558  0.8541666666666666:  76%|███████▌  | 57/75 [04:50<01:38,  5.49s/it]test:58/75 | accuracy 1585  0.8539870689655172:  76%|███████▌  | 57/75 [04:54<01:38,  5.49s/it]test:58/75 | accuracy 1585  0.8539870689655172:  77%|███████▋  | 58/75 [04:54<01:25,  5.04s/it]test:59/75 | accuracy 1611  0.8532838983050848:  77%|███████▋  | 58/75 [04:59<01:25,  5.04s/it]test:59/75 | accuracy 1611  0.8532838983050848:  79%|███████▊  | 59/75 [04:59<01:21,  5.12s/it]test:60/75 | accuracy 1637  0.8526041666666667:  79%|███████▊  | 59/75 [05:04<01:21,  5.12s/it]test:60/75 | accuracy 1637  0.8526041666666667:  80%|████████  | 60/75 [05:04<01:15,  5.05s/it]test:61/75 | accuracy 1661  0.850922131147541:  80%|████████  | 60/75 [05:08<01:15,  5.05s/it] test:61/75 | accuracy 1661  0.850922131147541:  81%|████████▏ | 61/75 [05:08<01:08,  4.86s/it]test:62/75 | accuracy 1691  0.8523185483870968:  81%|████████▏ | 61/75 [05:12<01:08,  4.86s/it]test:62/75 | accuracy 1691  0.8523185483870968:  83%|████████▎ | 62/75 [05:12<01:00,  4.68s/it]test:63/75 | accuracy 1719  0.8526785714285714:  83%|████████▎ | 62/75 [05:17<01:00,  4.68s/it]test:63/75 | accuracy 1719  0.8526785714285714:  84%|████████▍ | 63/75 [05:17<00:56,  4.69s/it]test:64/75 | accuracy 1746  0.8525390625:  84%|████████▍ | 63/75 [05:22<00:56,  4.69s/it]      test:64/75 | accuracy 1746  0.8525390625:  85%|████████▌ | 64/75 [05:22<00:52,  4.78s/it]test:65/75 | accuracy 1774  0.8528846153846154:  85%|████████▌ | 64/75 [05:27<00:52,  4.78s/it]test:65/75 | accuracy 1774  0.8528846153846154:  87%|████████▋ | 65/75 [05:27<00:48,  4.85s/it]test:66/75 | accuracy 1803  0.8536931818181818:  87%|████████▋ | 65/75 [05:31<00:48,  4.85s/it]test:66/75 | accuracy 1803  0.8536931818181818:  88%|████████▊ | 66/75 [05:31<00:42,  4.69s/it]test:67/75 | accuracy 1830  0.8535447761194029:  88%|████████▊ | 66/75 [05:36<00:42,  4.69s/it]test:67/75 | accuracy 1830  0.8535447761194029:  89%|████████▉ | 67/75 [05:36<00:36,  4.59s/it]test:68/75 | accuracy 1856  0.8529411764705882:  89%|████████▉ | 67/75 [05:40<00:36,  4.59s/it]test:68/75 | accuracy 1856  0.8529411764705882:  91%|█████████ | 68/75 [05:40<00:31,  4.52s/it]test:69/75 | accuracy 1882  0.8523550724637681:  91%|█████████ | 68/75 [05:45<00:31,  4.52s/it]test:69/75 | accuracy 1882  0.8523550724637681:  92%|█████████▏| 69/75 [05:45<00:27,  4.59s/it]test:70/75 | accuracy 1909  0.8522321428571429:  92%|█████████▏| 69/75 [05:50<00:27,  4.59s/it]test:70/75 | accuracy 1909  0.8522321428571429:  93%|█████████▎| 70/75 [05:50<00:23,  4.65s/it]test:71/75 | accuracy 1933  0.8507922535211268:  93%|█████████▎| 70/75 [05:55<00:23,  4.65s/it]test:71/75 | accuracy 1933  0.8507922535211268:  95%|█████████▍| 71/75 [05:55<00:19,  4.83s/it]test:72/75 | accuracy 1957  0.8493923611111112:  95%|█████████▍| 71/75 [06:00<00:19,  4.83s/it]test:72/75 | accuracy 1957  0.8493923611111112:  96%|█████████▌| 72/75 [06:00<00:14,  4.79s/it]test:73/75 | accuracy 1981  0.8480308219178082:  96%|█████████▌| 72/75 [06:04<00:14,  4.79s/it]test:73/75 | accuracy 1981  0.8480308219178082:  97%|█████████▋| 73/75 [06:04<00:09,  4.72s/it]test:74/75 | accuracy 2011  0.8492398648648649:  97%|█████████▋| 73/75 [06:09<00:09,  4.72s/it]test:74/75 | accuracy 2011  0.8492398648648649:  99%|█████████▊| 74/75 [06:09<00:04,  4.87s/it]test:75/75 | accuracy 2018  0.8493265993265994:  99%|█████████▊| 74/75 [06:11<00:04,  4.87s/it]test:75/75 | accuracy 2018  0.8493265993265994: 100%|██████████| 75/75 [06:11<00:00,  3.85s/it]test:75/75 | accuracy 2018  0.8493265993265994: 100%|██████████| 75/75 [06:11<00:00,  4.95s/it]


test finished
MixtralAdapterConfig {
  "_name_or_path": "checkpoints/Mixtral-8x7B-v0.1.lora_16.4/config.json",
  "architectures": [
    "MixtralAdapterForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "embedded_routing_adapter": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "model_type": "mixtral-adapter",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": true,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "shared_adapter": true,
  "shared_adapter_args": {
    "lora_alpha": 32,
    "lora_dropout": 0.0,
    "r": 16
  },
  "shared_adapter_num": 4,
  "shared_adapter_type": "LoRA",
  "shared_routing_adapter": false,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": false,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:34,  1.93s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.91s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:30,  1.90s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:28,  1.88s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:26,  1.89s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:24,  1.88s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:22,  1.88s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:20,  1.88s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:16<00:18,  1.89s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:18<00:16,  1.89s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:20<00:15,  1.88s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:13,  1.89s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:11,  1.88s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:09,  1.87s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.88s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:30<00:05,  1.88s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:31<00:03,  1.87s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:33<00:01,  1.87s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:35<00:00,  1.87s/it]
Some weights of MixtralAdapterForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
##### Loading checkpoint from checkpoints/Mixtral-8x7B-v0.1.lora_16.4 #####
Restarting from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors
##### Loading 256 parameters #####
##### Successfully loaded parameters from checkpoints/Mixtral-8x7B-v0.1.lora_16.4/model.safetensors #####
  0%|          | 0/16 [00:00<?, ?it/s]/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/hk-project-p0022189/hgf_mxv5488/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
test:1/16 | accuracy 26  0.8125:   0%|          | 0/16 [00:04<?, ?it/s]test:1/16 | accuracy 26  0.8125:   6%|▋         | 1/16 [00:04<01:14,  4.95s/it]test:2/16 | accuracy 54  0.84375:   6%|▋         | 1/16 [00:09<01:14,  4.95s/it]test:2/16 | accuracy 54  0.84375:  12%|█▎        | 2/16 [00:09<01:06,  4.75s/it]test:3/16 | accuracy 81  0.84375:  12%|█▎        | 2/16 [00:13<01:06,  4.75s/it]test:3/16 | accuracy 81  0.84375:  19%|█▉        | 3/16 [00:13<00:54,  4.19s/it]test:4/16 | accuracy 109  0.8515625:  19%|█▉        | 3/16 [00:17<00:54,  4.19s/it]test:4/16 | accuracy 109  0.8515625:  25%|██▌       | 4/16 [00:17<00:50,  4.18s/it]test:5/16 | accuracy 130  0.8125:  25%|██▌       | 4/16 [00:21<00:50,  4.18s/it]   test:5/16 | accuracy 130  0.8125:  31%|███▏      | 5/16 [00:21<00:45,  4.16s/it]test:6/16 | accuracy 157  0.8177083333333334:  31%|███▏      | 5/16 [00:25<00:45,  4.16s/it]test:6/16 | accuracy 157  0.8177083333333334:  38%|███▊      | 6/16 [00:25<00:41,  4.15s/it]test:7/16 | accuracy 184  0.8214285714285714:  38%|███▊      | 6/16 [00:29<00:41,  4.15s/it]test:7/16 | accuracy 184  0.8214285714285714:  44%|████▍     | 7/16 [00:29<00:36,  4.09s/it]test:8/16 | accuracy 210  0.8203125:  44%|████▍     | 7/16 [00:33<00:36,  4.09s/it]         test:8/16 | accuracy 210  0.8203125:  50%|█████     | 8/16 [00:33<00:32,  4.03s/it]test:9/16 | accuracy 237  0.8229166666666666:  50%|█████     | 8/16 [00:37<00:32,  4.03s/it]test:9/16 | accuracy 237  0.8229166666666666:  56%|█████▋    | 9/16 [00:37<00:27,  3.93s/it]test:10/16 | accuracy 262  0.81875:  56%|█████▋    | 9/16 [00:40<00:27,  3.93s/it]          test:10/16 | accuracy 262  0.81875:  62%|██████▎   | 10/16 [00:40<00:23,  3.83s/it]test:11/16 | accuracy 285  0.8096590909090909:  62%|██████▎   | 10/16 [00:44<00:23,  3.83s/it]test:11/16 | accuracy 285  0.8096590909090909:  69%|██████▉   | 11/16 [00:44<00:18,  3.80s/it]test:12/16 | accuracy 309  0.8046875:  69%|██████▉   | 11/16 [00:48<00:18,  3.80s/it]         test:12/16 | accuracy 309  0.8046875:  75%|███████▌  | 12/16 [00:48<00:15,  3.86s/it]test:13/16 | accuracy 337  0.8100961538461539:  75%|███████▌  | 12/16 [00:52<00:15,  3.86s/it]test:13/16 | accuracy 337  0.8100961538461539:  81%|████████▏ | 13/16 [00:52<00:11,  3.80s/it]test:14/16 | accuracy 366  0.8169642857142857:  81%|████████▏ | 13/16 [00:56<00:11,  3.80s/it]test:14/16 | accuracy 366  0.8169642857142857:  88%|████████▊ | 14/16 [00:56<00:07,  3.88s/it]test:15/16 | accuracy 394  0.8208333333333333:  88%|████████▊ | 14/16 [00:59<00:07,  3.88s/it]test:15/16 | accuracy 394  0.8208333333333333:  94%|█████████▍| 15/16 [00:59<00:03,  3.85s/it]test:16/16 | accuracy 411  0.822:  94%|█████████▍| 15/16 [01:02<00:03,  3.85s/it]             test:16/16 | accuracy 411  0.822: 100%|██████████| 16/16 [01:02<00:00,  3.58s/it]test:16/16 | accuracy 411  0.822: 100%|██████████| 16/16 [01:02<00:00,  3.93s/it]


test finished

============================= JOB FEEDBACK =============================

Job ID: 2629215
Cluster: hk
User/Group: hgf_mxv5488/hk-project-p0022189
Account: hk-project-p0022189
State: COMPLETED (exit code 0)
Partition: accelerated
Nodes: 1
Cores per node: 152
Nodelist: hkn0531
CPU Utilized: 01:15:27
CPU Efficiency: 0.67% of 7-20:51:36 core-walltime
Job Wall-clock time: 01:14:33
Starttime: Sun Sep  8 08:29:00 2024
Endtime: Sun Sep  8 09:43:33 2024
Memory Utilized: 7.54 GB
Memory Efficiency: 0.00% of 0.00 MB
Energy Consumed: 3670582 Joule / 1019.60611111111 Watthours
Average node power draw: 820.608540129667 Watt
